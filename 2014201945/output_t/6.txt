                  using linear programming bayesian exploration                                      markov decision processes                                 pablo samuel castro doina precup                                             mcgill university                                        school science                                       pcastrdprecupcsmcgillca                        abstract                          based heuristics help agent data                                                        trying protect “danger” thrun       key problem reinforcement learning ﬁnd   meuleau bourgine   recently algo      ing good balance need explore rithms proposed carry guarantees terms      environment need gain rewards ex   number samples necessary agent      ploiting existing knowledge research    tain optimal performance kearns singh       devoted topic pro  brafman tennenholtz  strehl littman       posed methods aimed simply ensuring   strehl et al       samples gathered estimate    paper explore bayesian approach explo                                      value function contrast bellman kal     ration initial idea bellman  sug                    aba  proposed constructing representation  gested keeping information agent’s current state      states original paired knowledge addition model learned      knowledge current model    agent method bayesoptimal point view      knowledge possible markov models     decision making complexity grows exponentially      environment represented maintained ex horizon considered makes inapplicable ex      plicitly unfortunately approach intractable cept case bandit problems gives rise      bandit problems gives rise gittins indices recently body work bayesian rein      gittins indices optimal exploration method forcement learning developed method approximating      paper explore ideas making method bayesoptimal procedure case general mdps      computationally tractable maintain model dearden et al  duff  wang et al      environment markov decision process  paper pursue similar idea important      sample ﬁnitelength trajectories inﬁnite differences propose use linear programming      tree using ideas based sparse sampling  order approximate value function during proce      ing values nodes sparse subtree dure second use sampling approximation based      expressed optimization problem value function expand horizon decision making      solve using linear programming il   process      lustrate approach domains com    paper organized follows section  present      pare exploration algorithms        necessary background section  present lin                                                        ear programming approach problem section     introduction                                       present empirical results three domains  key problem reinforcement learning posed  need explore environment sufﬁciently order dis  background  cover sources reward time exploit ﬁnite markov decision process mdp tuple  ing knowledge agent taking ac s  rwheres ﬁnite state space ﬁnite  tions yield high return popular techniques action space  s×a×s →deﬁnes transition prob  current literature eggreedy boltzmann explo abilities  s×a→is reward function bounded  ration aimed efﬁcient exploration instead rmax agent state ∈sand performs action  ensure action choices randomized enables ∈a · distribution possible states  agent try actions states approach rs expected immediate reward determinis  guarantees convergence reinforcement learning algo tic stationary policy π  s→ais function determines  rithms efﬁcient point view action depending current state  number samples gathered cause agent goals reinforcement learning ﬁnd policy  peatedly try harmful actions extensive line research maximizes expected discounted return received given                                                          ∞    t−  devoted directed exploration methods γ rtwherert reward received time step                                                    ijcai                                                    γ ∈   discount factor value state πs parameters states change assuming  given expected discounted return received ex ﬁxed number rewards ob  ecuting policy π starting state               served each hyperstate hmdp a×s×r                                                      successors each successor hyperstate uniquely                            ∞                 π               t−                    indexed hmdp inﬁnite tree note                 seπ      γ   rt                                                                     hmdp takes account possible transitions                                                                                   fully known mdp  note upper bound value policy express bellman optimality equations                                                        hmdp  state rmax − γ                                                                                                                                                                                                                                                 ¯        ¯                    ¯                                                         rmax ri  γ   p¯ij mv dij mdij    hyperstate mdps                                              a∈a                                                                                                                 paper consider mdps transition prob  abilities rewards unknown situation solving set equations exactly yield optimal  agent maintains estimate mdp model based policy bellman kalaba  martin  prior knowledge observations transition clear number states hmdp inﬁnite  probabilities convenient use distribution exact solution intractable clear num  closed updates performed observing state tran ber hyperstates increases exponentially depth  sition mdps ﬁnite state action sets es tree exact solution ﬁnitehorizon subset  timate transition probabilities maintained tree limited shallow depths focus  matrix size × × sasshowninmartin    paper present algorithm computing em   matrix beta distribution closed updates pirically good learning policy ﬁnite subset tree  natural choice distribution case theoretical guarantees left future work                  note   mij  current indexing parameters focus comparing performance algorithm                      distribution mij simply number observed wellknown exploration techniques  transitions state state action atheex                                                          related work  pected transition probabilities based model given      pa  ¯pa    ma    ma                      optimal learning seen increase recent     ij    ij     ij    ik letting matrix                                                      years recent work spurred kearns       denote additional number observed transi                                                ij                                             singh  authors introduce  algo  tions state state action ainmartin                                     p                  rithm guaranteed converge optimal learning  shown posterior distribution parameters policy polynomial time drawbacks algo  m      matrix beta distribution                         da                         rithm difﬁculty implementation intuition  follows denote ij new distribution ob does necessarily scale large state spaces  tained through operation observing                                                  practical modelbased algorithm similar guarantees  transition state state action                                                                                                   given brafman tennenholtz   advanced    estimated reward received performing action                                              modelbased algorithms pac guarantees based real  state estimated r¯i  p¯ij r˜ij  time dynamic programming recently proposed         r˜ij average reward obtained going state strehl littman  strehl et al thesemeth  state action rewards summarized ods explore assigning reward exploratory actions            ¯  matrix                                        drawback kearns singh  strehl    matrix beta distribution viewed summary et al  strategies myopic   information agent accumulated far consider long term effects actions  information original mdp states ac total reward  tions form new hyperstatemdp hmdp ac  using hyperstates model exploration overcomes  tions hmdp original mdp myopic behavior concept hyperstates ﬁrst  states hmdp consist pairings states introduced bellman kalaba  refers  possible information states hyperstate r¯ model adaptive control process math  tains mdp state counts summarizing agent’s ematically rich paper presents algorithmic approach  experience far estimates expected idea duff  various heuristic methods  rewards r¯ counts deﬁne distribution mdp presented approximating exact solution adaptive  models consistent data observed far      control process produce empirically good results    given time step action taken orig general theoretical guarantees  inal mdp state precisely transition observed wang et al  propose sampling inﬁnite hy  state given hyperstate r¯ pertree produce small manageable tree solve  consider taking possible actions ∈aifsuch exactly mdps path sampling  action results state new hyperstate gives estimates values different actions             ¯  dij mdij updated innforma use thompson sampling expand tree locally  tion model described reﬂect new transition promising actions authors estimate unsampled regions  note update affects parameters state effectively ‘copying’ sampled node                                                    ijcai                                                    current planning horizon finally sparse sampling kearns et corresponding mdp state value com  al  used decide action optimal correc puted actionvalue function mdp state used  tion procedure used desired horizon instead note unsampled regions constitute sub  reached empirical results demonstrate algorithm tree hyperstate mdp unsampled subtrees  performs comparison algorithms estimated setting value hyperstate root  sparse sampling technique enables algorithm subtree value correspondinng  able handle inﬁnite state action spaces    mdp state method effectively choose                                                        variables constraints want use lin    solving hyperstate mdp                         ear program                                                          sampled hyper tree built solve using  paper approach similar wang et al linear programming order gather samples   key differences actionvalue esti choose action greedily based values computed  mates maintained stateaction pairs origi linear program enter unsampled region  nal mdp addition hyperstate values actionvalues action choice greedy respect value esti  updated obtained samples using standard mates attached original mdp states decide  learning sutton barto  construct new hyper tree compute new values  easy estimate good different actions second  compute value hyperstates using linear pro             levels maxsamples numsteps maxepochs  gramming linear programming lp technique algorithm  explore                            long time understood el  initialize matrix counts uniform distribu                                                          tion  egant theory puterman   furthermore recent work      epochs ≤ maxepochs                                                                             approximate linear programming farias roy                                        levels                                                           construct hypertree depth    using   hauskrecht kveton   suggests ap  maxsamples  proach used work continuous states using lin            sampled trajectories  ear function approximation hopeful tech  solve lp sample hypertree  niques eventually used generalize approach  choose action greedily based hyperstate values  continuous mdps limit   observe transition update hyper parameter ob                                                              served transition  discrete mdps                                                    numsteps    manner similar schweitzer seidmann                optimality equations hyperstate mdp  choose action greedily based current hy  mulated linear program follows                       perstate value current estimate state values                                                             observe transition update hyper parameters              minimize        r¯                      based observed transition                                                         end                                                           epochs ← epochs  numsteps                                               end                                                                           ¯    ¯                      ¯  r− ri  γ  p¯ij mv dij mdij ≥  algorithm  presents outline approach                                                      maxepochs   parameter deﬁnes total samples                                                        gathered environment levels parameter  states ∈s actions ∈aand information                                                        scribes depth expand sampled hyper  states refer formulation exact lp                                                        tree maxsamples  parameter controls tra  however depth  number hyperstates                                                      jectories sampled width tree   × consider possible                                                        parameters control large each linear program  ward levels this means  depth linear program                                                      numsteps parameter deﬁnes steps  × variables constraints pro                                                        environment taken lp solution recomputed  hibitive                                                        allows trade precision computation    kearns et al  authors overcome similar                                                        time obviously recompute lp  obstacle constructing sparse sample tree apply                                                        sample generate samples based action  similar idea construct sampling tree incrementally                                                        truly believed best wait longer start  precisely sample hyperstate mdp using                                                        selecting actions based imperfect estimates values  current estimates action values decide                                                        underlying mdp speeds compu  each node use corresponding estimate                                                        tation signiﬁcantly  dynamics sample node trajec  tory each trajectory completed desired horizon  continue constructing trajectories way maxi  empirical results  mum desired number samples reached       tested algorithm three different problems    encounter hyperstate value com domains results averaged  independent runs  puted using lp use lpbased estimate compared approach qlearning agent α   limit approaches true optimal value usingangreedy approach varying values                                                     ijcai                                                    figure  twostate threeaction mdp dynamics left comparison algorithm center effect parameters  right    algorithm described wang et al left panel action  solid action  dashed action  experimented myopic agent acting based  solid grey leftmost state corresponds winning  immediate reward agent chooses middle state corresponds winning machine   actions randomly algorithms did far worse rightmost state corresponds winning machine   omitted clarity cost associated gambling  algorithm experimented different parameter settings comparison algorithms use parameters  explained primal lp levels numsteps maxsamples   constraints variables dual solved instead problem wang et al ’s algorithm able handle    considered different possible uses  samples algorithm outperformed  model constructed algorithm ﬁrst ver figure    sion desired number epochs completed samples algorithm does best  perform dynamic programming using acquired model learning algorithm  determine value function original mdp  second setting just use values determined lp  value estimates obviously approach faster grid world  extra computation required im problem gridworld reward   precise                                              right state                                                         actions north south west east  probability  small mdp                                             remaining state agent tries  ﬁrst problem twostate threeaction mdp wall deterministically stay state  scribed left panel figure  intuitively best pol parameter values used levels numsteps   icy perform action  state  reached maxsamples   action  state                    larger problem ﬁrst    center graph compares algorithm parameters algorithms ‘stuck’ low reward algorithm  levels numsteps   maxsamples        able ﬁnd good policy ﬁgure  clear  learning using three different values     using dynamic programming obtain state value estimates  approach wang et al algorithms plot advantageous task  average return episode note implementation interesting ﬁrst problems  algorithm described wang et al  able ing values returned lp estimate state val  handle  samples performed                                                   ues yields better returns initially probably  comparison level qlearning    ‘lookahead’ nature hmdp gridworld  outperformed algorithms graph problem performance better using dp  allow algorithm  samples construct solution estimate state values behavior  hypertree performs better rest interestingly probably size hypertree created state  case using values lp gives better action space bigger samples needed ob  formance doing step dynamic programming tain state value estimates closer true values                                                        noted dp solved ex  bandit problem                                        actly cases performance using hmdps  second problem similar bandit problem superior algorithms note ex  slot machines bandits three actions problem perimented boltzmann exploration algorithms omitted  modeled using three states three actions dynam plots clarity cases performance  ics given ﬁgure  dynamics shown similar qlearning algorithms                                                    ijcai                                                       figure  bandit problem dynamics left comparison different algorithms center parameter inﬂuence right                 figure  comparison algorithms left inﬂuence parameters right gridworld task    running time                                             conclusions future work  results demonstrate approach advantage aim paper demonstrate performance  compared methods terms accuracy bayesian learning using sparse sampling linear program  terms computation time using hyperstates obvi ming observed general using hyperstates does  ously slower qlearning table  plots average lead greater average returns exploring unknown  running time episode solving dp estimate environment used simpler exploration techniques  state values versus using values returned lp stateoftheart techniques algorithm pre  different domains running time wang et sented wang et al  drawback technique  al ’s algorithm worth observing difference leave certain areas environment unex  performancerunning time depending state values plored desired behavior  estimated solving dp using values returned sider physical agent robot placed task  lp algorithm considerably slower exploring new environment want agent  solving dp faster wang et al ’s avoid areas cause harm mechanical parts  algorithm using values returned lp solution people explore sufﬁciently  gridworld problem solving try reach states high reward situation like  hmdp slower wang et al ’s algorithm justify increase computation furthermore  savings using wang et al ’s algorithm method sparse sampling allow hypertree  signiﬁcant compared superior performance greater depth agent states  demonstrated ﬁgure                               myopic methods able ability value ac                                                        tions immediate longterm effect makes                       hmdp              wang           bayesian learning good candidate exploratory agents                using dp    dp                         step obtain theoretical guarantees   small mdp                algorithm like implement algorithms     bandit                 presented strehl et al  possibly duff    grid world                         empirically compare algorithm latest                                                        advances bayesian decision theory inter           table  comparison running times         esting try algorithm problems continuous                                                        state andor action spaces placing gaussian sampled                                                    ijcai                                                    
