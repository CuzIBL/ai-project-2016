                 direct code access selforganizing neural networks                                    reinforcement learning                                               ahhwee tan                                     nanyang technological university                      school engineering intelligent systems centre                                    nanyang avenue singapore                                             asahtanntuedusg                        abstract                          function state space action policy mlp bp                                                        designed online incremental learning      tdfalcon selforganizing neural network     typically require iterative learning process addi      incorporates temporal difference td meth  tion issue instability sense learning      ods reinforcement learning despite advan new patterns erode previously learned knowledge      tages fast stable learning tdfalcon consequently resultant reinforcement learning systems      relies iterative process evaluate each avail able learn operate real time      able action decision cycle remove deﬁ      ciency paper presents direct code access pro instead learning value functions action policies      cedure tdfalcon conducts instanta       selforganizing neural networks selforganizing      neous searches cognitive nodes match map som typically used representation      current states time provide max generalization continuous state action spaces smith      imal reward values comparative experiments    state action clusters used en      show tdfalcon direct code access       tries qvalue table implemented separately using lo      produces comparable performance origi   calized representation som advantage stable      nal tdfalcon improving signiﬁcantly     learning compared backpropagation networks      computation efﬁciency network complexity     som remains iterative learning requiring                                                        rounds learn compressed representation                                                        state action patterns addition mentioned smith    introduction                                        som expected scale badly dimensions  reinforcement learning sutton barto  inter state actions spaces signiﬁcantly higher  action based paradigm autonomous agent learns dimension map applications som  adjust behaviour according feedback en limited low dimensional state action spaces  vironment classical solutions reinforcement learning recent approach reinforcement learning builds  problem generally involve learning follow adaptive resonance theory art carpenter gross  ing mappings ﬁrst linking given state desired ac berg  class selforganizing neural networks  tion action policy second associating pair state distinct characteristics som ueda et al  action utility value value function using temporal  use art models learn clusters state ac  difference methods sarsa rummery niran tion patterns clusters turns used compressed  jan  qlearning watkins dayan states actions qlearning module ninomiya   problem original formulation mappings couples supervised art temporal differ  learned each possible state each ence reinforcement learning module hybrid architecture  possible pair state action causes scalability states actions reinforcement mod  sue continuous andor large state action spaces ule exported supervised art    neural networks reinforcement learning learning systems operate independently redundancy  intertwining relationship kaelbling et al  particu representation unfortunately leads instability unnec  lar multilayer feedforward neural networks known essarily long processing time action selection  multilayer perceptron mlp used extensively learning value functions through generalization art  reinforcement learning applications ack input pattern ﬁeld multiple pattern channels tan  ley littman  sutton  recent thread  presents neural architecture called falcon fusion  research approximate dynamic programming adp  architecture learning cognition navigation  si et al  mlp gradient descent backpropagation learns multichannel mappings simultaneously multi  bp learning algorithms commonly used learn ap modal input patterns involving states actions rewards  proximation value function state action online incremental manner handling problems  spaces value policy andor approximation action delayed evaluative feedback reward signal variant                                                    ijcai                                                    falcon known tdfalcon   tan xiao  aaam denote action vector ai ∈    learns value functions stateaction space estimated indicates possible action iletr r¯ denote  through temporal difference td algorithms compared ward vector ∈   reward signal value  artbased systems described ueda et al r¯ complement risgivenbyr¯ − comple  ninomiya tdfalcon presents truly integrated solution ment coding serves normalize magnitude input  sense implementation separate vectors effective art systems pre  inforcement learning module qvalue table         venting code proliferation problem input values    tdfalcon provides promising approach   falcon assumed bounded    action selection procedure contains inherent limitation malization necessary original values  speciﬁcally tdfalcon selects action weighting range                                                                              ck           ck  consequence performing each possible action activity vectors let denote activity vector                                                                                    given state action selection process lety denote activity vector                                                                             ck  inefﬁcient large number actions numerat weight vectors let wj denote weight vector associ                                                                                 ing step assumes ﬁnite set actions rendering ated jth node learning input patterns                                                            ck                          applicable continuous action space view  initially contains  ﬁciency paper presents direct code access procedure committed node weight vectors contain ’s  tdfalcon perform instantaneous searches uncommitted node selected learn association  cognitive nodes match current states committed  time provide highest reward values parameters falcon’s dynamics determined  sides algorithm natural efﬁcient choice parameters αck    learning rate                                                                  βck ∈       tdfalcon operate continuous state  parameters                       contribution pa                                                                 ck                                 ck  action spaces comparative experiments based rameters γ ∈    γ   mineﬁeld navigation task show tdfalcon direct vigilance parameters ρck ∈     code access produces comparable performance orig code activation bottomup propagation process ﬁrst  inal tdfalcon improving vastly terms takes place activities known choice function                                                                                          computation efﬁciency network complexity  values cognitive nodes ﬁeld computed    rest paper organized follows complete speciﬁcally given activity vectors xc xc xc                                                                        cs                           ness section  presents summary falcon architec input ﬁelds  respectively each                                                                                  ture associated learning prediction algorithms node choice function tj computed follows  section  presents new tdfalcon algorithm                                                                              direct code access procedure section  introduces                   xck ∧ wck                                                                         γck           ﬁeld navigation simulation task presents experimental                 αck  wck             results ﬁnal section concludes provides brief dis                        cussion future work                               fuzzy operation ∧ deﬁned p∧ qi ≡                                                        minpiqi norm  deﬁned p≡ pi    falcon dynamics                                    vectors essence choice function tj com                                                        putes similarity activity vectors respec  falcon employs channel architecture figure  com                                                                          tive weight vectors node respect norm  prising cognitive ﬁeld three input ﬁelds                                                     individual weight vectors  sensory ﬁeld representing current states action                                                  code competition code competition process follows  ﬁeld representing actions reward ﬁeld                                                                der node highest choice function value  representing reinforcement values generic network dy identiﬁed winner indexed  namics falcon based fuzzy art operations car                                                                   maxt           penter et al  described                                   node                                                                                                                                                                      category choice node yj andyj                                                           indicates winnertakeall strategy                                                        template matching code used learning                                                        template matching process checks weight templates                                                        code sufﬁciently close respective activity pat                                                        terns speciﬁcally resonance occurs each channel kthe                                                                       ck                                                        match function mj chosen code meets vigilance                                                        criterion                                                                             xck ∧ wck                                                                      mck           ≥  ρck                                                                             xck                                                                            match function computes similarity activity            figure  falcon architecture          weight vectors respect norm activity                                                        vectors choice match functions work  input vectors let sssn denote state vec operatively achieve stable coding maximize code com  tor si ∈   indicates sensory input ileta  pression                                                    ijcai                                                    resonance occurs learning ensues deﬁned  action selection policy  vigilance constraints violated mismatch simplest action selection policy pick action                                                 set occurs value choice function highest value predicted tdfalcon network  set  duration input presentation key requirement autonomous agents ex  match tracking process beginning each input pre plore environment especially important                               ρc  sentation vigilance parameter equals baseline vig agent function situations immediate evaluative       ρ¯c                        ρc  ilance   mismatch reset occurs increased feedback agent keeps selecting optimal action                                      mc  slightly larger match function  search believes able explore discover better                              process selects  node revised vig alternative actions fundamental tradeoff  ilance criterion resonance achieved search tween exploitation sticking best actions believed  test process guaranteed end falcon ﬁnd exploration trying seemingly inferior  committed node satisﬁes vigilance criterion acti familiar actions  vate uncommitted node deﬁnitely satisfy greedy policy selects action highest                                      criterion initial weight values        value probability  −  takes random ac  template learning node selected each chan tion probability  p´erezuribe  constant                     wck  nel  weight vector modiﬁed following  value agent explore environment  learning rule                                        ﬁxed level randomness practice beneﬁ                                                                             wcknew −  βckwckold  βckxck ∧ wckold   cial higher value encourage exploration                                               paths initial stage lower  value optimize  learning rule adjusts weight values fuzzy performance exploiting familiar paths later stage                                                               original values respective weight val decay greedy policy adopted gradually reduce                                                                 ues rationale learn encoding common value time rate decay typically inversely  tribute values input vectors weight vectors proportional complexity environment  uncommitted node learning rates βck typically complex environment larger state action spaces  set  committed nodes βck remain  fast longer time explore                   learning slow learning noisy environment  direct code access  uncommitted node selecting learning  comes committed new uncommitted node added exploiting mode agent best knowledge       ﬁeld falcon expands network architecture selects action maximal reward value given cur  dynamically response input patterns        rent situation state through direct code access procedure                                                        tdfalcon searches cognitive node matches                                                        current state maximal reward value  tdfalcon                                            direct code access activity vectors xc xcandxc  tdfalcon incorporates temporal difference td meth initialized xc  xc andxc    ods estimate learn value functions actionstate pairs tdfalcon performs code activation code compe  qs indicates goodness learning tition according equations   select cognitive  certain action given state value functions node following lemma shows  used action selection mechanism known cognitive nodes encoding current state tdfalcon  policy select action maximal payoff given activity vectors enable selection  original tdfalcon algorithm proposed tan xiao cognitive node maximum reward value   selects action maximal qvalue state  enumerating evaluating each available action direct access principle during direct code access given                                                                         xc   xc      xc   presenting corresponding state action vectors activity vectors                                                                       falcon tdfalcon presented paper      tdfalcon code activation competition pro                                                                                      places action enumeration step direct code access cess select cognitive node weight vectors                                                        wc              wc  procedure shown table  given current state closest representing maximal reward  tdfalcon ﬁrst decides exploration exploita value exists  tion following action selection policy exploration proof contradiction suppose tdfalcon selects                                                                                       random action picked exploitation tdfalcon  node    exists  node                                                                       wc                     wc  searches optimal action through direct code access pro weight vector similar                                                                    wc                                wc  cedure receiving feedback environment af weight vector encodes higher reward value                                                            wc                    wc  ter performing action td formula used compute similar wederivethat  new estimate value performing chosen ac        xc ∧ wc     xc ∧ wc                                                                                            tion current state new value used        αc  wc     αc  wc           teaching signal tdfalcon learn association                               current state chosen action estimated                                                                                        likewise wk encodes higher reward wj wehave  value details action selection policy direct                                                                    xc ∧ wc     xc ∧ wc  code access procedure temporal difference equation                                                                                                                           elaborated                                             α   wk       α    wj                                                     ijcai                                                                                table  td−falcon algorithm direct code access        initialize tdfalcon network      sense environment formulate state representation      following action selection policy ﬁrst make choice exploration exploitation        exploring random action        exploiting identify action maximal qsa value presenting state vector action        vector reward vector tdfalcon      perform action observe state s receive reward environment      estimate revised value function qs following temporal difference formula Δqs aαtderr      present corresponding state action reward qvalue vectors aandr tdfalcon learning      update current state ss’      repeat step  terminal state                                                      equation  conditions imply tk tj   means node selected tdfalcon  instead node contradiction  end proof                               selecting winning node chosen node                                                    performs readout weight vector action ﬁeld                 cnew    cold                              ∧ wj              action ai chosen highest activation  value                  cnew                xi maxxi          node     learning value function  typical temporal difference equation iterative estima  tion value functions qsa given                                                               figure  mineﬁeld navigation simulator                  Δqs aαt derr                  α ∈   learning parameter tderr  function current qvalue predicted tdfalcon   mineﬁeld navigation task  qvalue newly computed td formula    tdfalcon    employs   bounded  qlearning rule  objective given task teach autonomous ve  temporal error term computed        hicle av navigate through mineﬁeld randomly se                                                        lected target position speciﬁed time frame hitting            Δqs aαt derr   −       each trial av starts randomly chosen                                 tderr   γmaxa qs − qs aofwhichr   position ﬁeld repeats cycles sense act  immediate reward value γ ∈   discount pa learn trial ends av reaches target success                         rameter maxa qs denotes maximum estimated hits failure exceeds  senseactlearn cycles  value state s important note time target mines remain stationary during                                        values involved estimating maxa qs computed trial results reported paper based   falcon network separate  mineﬁeld containing  mines illustrated figure   reinforcement learning qlearning update rule av coarse sensory capability   applied states agent traverses value iter degree forward view based ﬁve sonar sensors each                                                                                                    ation value function qs expected converge direction  sonar signal measured                                                                                                  γmaxa qs time incorporating scal distance obstacle bound  ing term  − adjustment values ary mineﬁeld direction input attributes  selfscaling increased sensory state vector include bearing target  learning rule provides smooth normalization current position each step av choose  values reward value constrained   ﬁve possible actions left  guarantee values remain bounded diagonally left straight ahead diagonally right                                         right end trial reward  given                                                    ijcai                                                    av reaches target reward  given  av hits delayed reward scheme im  mediate reward given each step trial reward   given runs time    learning mineﬁeld task use tdfalcon  network containing  nodes sensory ﬁeld represent  ing complementcoded sonar signals  target bear  ing values  nodes action ﬁeld  nodes  reward ﬁeld representing complementcoded function  value tdfalcon direct code access employed set  parameter values obtained through empirical experiments  choice parameters αc αc αc   learning rate βck     fast learning                               tribution parameters γ  γ   γ      baseline vigi  lance parameters ρ¯c  ρ¯c  marginal  level match requirement state action spaces  ρ¯c  stricter match criterion reward values  temporal difference learning learning rate α ﬁxed   discount factor γ set  initial figure  average normalized steps taken td  value set  action selection policy  initial falcon using action enumeration ae direct code ac  ized  decayed rate  dropped cess da compared bp reinforcement learner                                                          senting  sonar signal values  possible target bearings                                                         selectable actions output layer consisted                                                        node representing value performing action                                                        particular state key issue using bp network                                                        termination number hidden nodes experimented                                                        varying number nodes empirically obtained                                                        best results  nodes enable fair comparison                                                        bp learner use action selection module                                                        based decay greedy policy                                                          figure  summarizes performance tdfalcon                                                        direct code access falconda original td                                                        falcon action enumeration falconae                                                        bp reinforcement learner terms success rates averaged                                                        trial intervals  trials  sets experi                                                        ments observed success rates falcon                                                        da falconae increased steadily right                                                        ginning  trials systems achieve                                                         percent success rates backpropagation bp based  figure  success rates tdfalcon using action inforcement learner hand required longer  enumeration ae direct code access da compared exploration phase fact bp learner managed  bp reinforcement learner               reach  success rates  trials lower                                                         decay rate  ﬁrst  trials    performance falcon perspective fur success rates remained   ther conducted experiments evaluate performance evaluate av traverses starting po  alternative reinforcement learning using standard sition target deﬁne measure called normalized                                                                    step    step       step  qlearning rule multilayer perceptron network trained step given sd   number  gradient descent based backpropagation algorithm sensemovelearn cycles taken reach target sd  function approximator chosen backpropaga shortest distance starting target positions  tion bp algorithm reference comparison gra normalized step  means taken opti  dient descent far widely used universal mal shortest path target depicted figure   function approximation techniques applied  trials falconae falconda able  contexts including qlearning sun et al  reach target optimal close optimal paths  adaptive critic werbos  bp learner employed great majority cases bp learner expected pro  standard threelayer feedforward architecture learn duced unsatisfactory performance terms path optimality  value function learning rate  momentum considering network complexity falconda demon  term  input layer consisted  nodes repre strated great improvement falconae creating                                                    ijcai                                                    
