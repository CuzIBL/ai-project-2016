               hierarchical multichannel hidden semi markov models                      ∗                                 pradeep natarajan      ramakant nevatia                               institute robotics intelligent systems                                     university southern california                                       los angeles ca                                         pnataraj nevatiauscedu                        abstract                          limb joints high level semantic concepts                                                        gestures need recognized handle      interesting human actions involve multiple   large variations duration styles activi      interacting agents typical durations ties performed different people      inherent hierarchical organiza person different times uncertainty inherent      tion activities order model sensory observations probabilistic reasoning offers natu      introduce new family hidden markov models    ral approach probabilistic models      hmms provide compositional state represen proposed years various communities activity      tations space time recursive recognition hidden markov models hmms ex      hierarchical structure inference higher levels tensions far widely used ones      abstraction particular focus possi offer advantages clear bayesian semantics efﬁcient      ble layer structures  hierarchicalsemi par learning inferencing algorithms natural way      allel hidden markov model hspahmm       troduce domain knowledge model structure bui et      hierarchical parallel hidden semimarkov model    al  presented extension hierarchical hidden      hpahsmm lower layer hspahmm        markov model hhmm inferring activities different      sists multiple hmms each agent   levels abstraction hongeng nevatia  used      layer consists single hsmm hpahsmm      hidden semimarkov model hsmm modeling activity      hand multiple hsmms lower durations duong et al  presented switch      layer markov chain layer     ing hidden semimarkov model shsmm exploit      present efﬁcient learning decoding algorithms hierarchical structure typical durations ac      models demonstrate ﬁrst tivity recognition vogler metaxas  introduced      synthetic time series data application parallel hidden markov model pahmm recognizing      sign language recognition                    complex handed gestures asl brand et al                                                         introduced coupled hidden markov model chmm    introduction                                       applied recognize tai chi gestures multiagent                                                        activities  goal develop methods inferring activities given believe real applications need models  observations visual sensory data activities combine features models intro  natural hierarchical structure combinations duce new class models hierarchical multi  simpler activities form higher level complex activities channel hidden semimarkov models particular focus  general multiple agents involved par model structures  hierarchical semi parallel hid  allel actions temporal spatial relations den markov model hspahmm hierarchical paral  important inference higher level activities present lel hidden semimarkov model hpahsmm present efﬁ  mathematical formalism recognizing multiagent cient decoding learning algorithms validate  activities advantage inherent hi utility models testing simulated data  erarchical organization typical durations models real task continuous sign language recognition  wide range applications surveillance rest paper organized follows   assistive technologies like sign language recognition section deﬁne parameters hspahmm  telligent environments                               hpahsmm formally sections   present    key challenge faced researchers activity recog efﬁcient decoding learning algorithms finally  nition bridge gap observations section  present experimental results    ∗this research partially funded advanced research  development activity government contract mda                                                     ijcai                                                      model deﬁnition parameters                      hpahsmm contains multiple hsmms lower layer                                         λ              single markov chain upper layer  begin deﬁning standard hmm model tuple following set parameters  π set possible states                                                         hpahsmm                             set observation symbols state transition probabil λlower qloweroloweralowerblowerdlowerπlower  ity matrix aij  qt  jqt  observation hpahsmm                                                              λupper     qupperoupperaupperbupperπupper  probability distribution bjkp ot  kqt  π  initial state distribution straightforward generalize figure  illustrates various hmm extensions dis  model continuous like gaussian output models    extended hierarchical hidden markov  model hhmm including hierarchy hidden states  hhmm    formally speciﬁed tuples λ   qdodadbdπd  ∈  indicates hierar  chy index    traditional hmms ﬁrst order markov assumption  implies duration probability state decays expo  nentially hidden semimarkov models hsmm  proposed alleviate problem introducing explicit  state duration models hsmm model speci  ﬁed tuple λ π contains  set parameters form di  probability  state having duration    basic hmm single variable represents state  instant  ing activities multiple interacting processes sev  eral multichannel hmms proposed model  extensions basically generalize hmm state                                      ssc  collection state variables     figure   structure ahmm   bhsmm    cpahmm  general form extensions represented dpahsmm ehspahmm fhpahsmm  λ qc oc ac bc πc  qc oc  possible states observations channel respec cussed key difference hspahmm   tively πc represents initial probability channel c’s hpahsmm hspahmm models duration  states ac contains transition probabilities com entire lowlevel pahmm toplevel hsmm state                                                hpahsmm   models duration each state each  posite states qt  qtqt   qt   tains observation probabilities composite states lowlevel hsmm hspahmm requires fewer parame                                                  ters hpahsmm richer structure modeling real  ot   ot qt   qt  form learning  inferencing algorithms exponential events  sult poor performance overﬁtting large number  parameters learn various multichannel extensions  decoding algorithm  typically introduce simplifying assumptions help fac  torizing transition observation probabilities parallel section use following notations  let  hidden markov models pahmm factor hmm mul number channels lower layer number frames  tiple independent chains allow factorizing ac observations each channel number states                                                                                    bc  coupled hidden markov models chmm factor each channel lower layer number lower  hmm multiple chains current state chain level hmms toplevel hmm state  depends previous state chains      lower level hmm state possi    hierarchical multichannel hidden semimarkov mod ble transition lower level hmms  els propose try combine characteris applications like sign language recognition each word  tics single model structure general form modeled multichannel hmmhsmm each transition  described set parameters form words lexicon distinct each  λ qc ocac bcdcπc      ∈                          hi hmms transition hmms giving total  erarchy index number channels level  transition states toplevel hmm total  parameters interpretations similar each ∗   channel higher level formed combination          states  channels lower level duration models  each level optional channels each level  hspahmm decoding  hierarchy maybe factorized using methods dis decoding algorithm hspahmm works travers  cussed pahmm chmm seen λ                                                  ing toplevel hsmm each time segment ﬁnding  presents synthesis λ  λ λ  hspahmm  lay maximum likelihood path through lowlevel pahmm  ers multiple hmms lower layer hsmm  upper layer following set parameters     assumed lower level hmms         λhspahmm  qc     oc   ac   bc    πc     number states simplicity notation straightforward          lower         lower lower lower  lower lower  extend algorithms presented cases lowlevel hmms   hspahmm  λupper    qupperoupperaupperbupperdupperπupper varying number states                                                    ijcai                                                    corresponding upper hsmm’s state traversing algorithm  hspahmm decoding                                           toplevel hsmm states takes ow    time    toplevel hsmm ww states                                         each lower pahmm takes ocn  time giving  use following indices states  total complexity ow       toplevel states lowlevel pahmms correspond  duce complexity duration models toplevel ing  hsmm   assumed uniform normal case  toplevel state  ww  transition state                                                            →                ∈ −   deﬁne parameters Σ each state      transition                                                            beamttop states time  level hsmm need consider state dura            ∈ mi − ΣimiΣi                  − Σm  Σ                           state ’s duration  tions range              values      δi                                       Σ                                                         probability maxlikelihood path state time     obtained lower upper thresholds  tml  probability transitioning state state                                                              case uniform distributions mean vari  ototm  probability observing otot state  ance normal distributions case need  check Σ durations each instant each lowlevel  ddm  probability spending duration state  pahmm   observations average traversing  maxp athm tt  function calculate probability  level hsmm takes ow Σ each lower level    maxlikelihood path through lowlevel pahmm state  pahmm takes ocnm    giving total inference complex  time vogler metaxas  details  ity ocww  tmΣ small val                                                             ues inference complexity prohibitively large  ← jth     beami  varies           state                                                             lwthen  storing states upperhsmm each        ∗    ∗                                           ow                                            stant each state transfer states       mm − Σm mmΣm   transition states transfer destination states                                                                                               δin   ←    δi ∗ ml ∗  nm ∗                                                                         states corresponding lowerlevel pahmms trans         oioinm                                                                          fer transition states overall inference complexity    δin paths time                       ocwkn     tmΣ figure  illustrates decod           add beami   ing algorithm algorithm  presents pseudocode           end                                                                 end                                                               end                                                                                                                           ←  remainderlw                                                                mm − Σm mmΣm                                                                                                                                            δin ← δi ∗ nm ∗ maxp athm                                                                                                                                      δin paths time                                                                    add beami                                                                  end                                                               end                                                             end                                                           end                                                         end                                                         return δ maximum probability state beamt              figure  decoding hspahmm                                                                                   hpahsmm decoding                                 takes ocw      duration models                                                        mal uniform need consider state durations  decoding hpahsmms traverse toplevel range m−σ σ values σ deﬁned  markov chain each state corresponding low each state pahsmm time complexity  level pahsmm procedure simpliﬁed string ocww  ntmσ storing  ing lowlevel pahsmms transition states compound pahsmm each instant  states large compound pahsmm traversing reduce complexity ockww  ntmσ  lowlevel pahsmms states each figure  illustrates decoding algorithm algorithm              transition states level compound presents pseudocode hpahsmm  pahsmm   ∗  states note transition  states duration output models  embedded viterbi learning  each channel compound pahsmm hsmm taking  ow   nt   time entire decoding algorithm applications like sign language recognition event                                                        recognition training samples typically contain sequence    murphy  points hsmm inference complexity wordsevents segmented cases  reduced ow  precalculating output string individual wordevent hmms  probabilities states intervals beam estimate parameters combined hmm using tra  search store small subset states each time ditional baumwelch algorithm individual events  interval procedure actually increases run time case segmented automatically during reestimation pro                                                    ijcai                                                                                                          algorithm  hpahsmm decoding                                                          beamcttop states time channel                                                          state i’s duration ∈ mi − σimiσi                                                          mint minmi − σi maxt maxmiσi                                                             ci                                                          δt  probability maxlikelihood path state channel                                                            time                                                                                                                      pc ml  probability transitioning state state                                                            channel                                                                                                                      ototc  probability observing otot state                                                            channel                                                          ddc  probability spending duration state                                                            channel              figure  decoding hpahsmm                  beamc set using initialization probabilities states ∀                                                           cess constrained hmm structures like leftright   mint maxt  hmm each state transition   state higher reestimation procedure simpliﬁed  ∗   calculating viterbi path through hmm each     eration reestimating parameters histogram anal    ← nth state beaml −                                                                        lm    lp            ysis state occupancies applications      δij  δi− ∗ pc mp ∗ ∗                                                                         primarily interested leftright hmms adopted           oioij                                                                         lm  viterbilike embedded training algorithm hspahmm         δij paths time   hpahsmm algorithm    presents pseudocode             add beaml   learning algorithm each iteration corre    end  sponding decoding algorithm reestimate param    end  eters using simple histogram analysis state occupancies  end                                                             end                                                           end                                                         end    experiments                                                                                                           return Σc δ maximum probability state beamct  evaluate learning decoding hspahmm    hpahsmm   conducted experiments syn  thetic data real data sign language asl  recognition task cases compare results sequences producing total  sequences ran  pahmm duration models beam search domly chose set training sequences each word  states run time results ghz pentium  windows occurred  times training set used rest  platform gb ram running java programs         test sequence training set contained  se                                                        quences test set contained  sequences data    benchmarks synthetic data                     generators discarded following models  experiment used discrete event simulator gen trained  hpahsmm randomly initialized output  erate synthetic observation sequences each event models leftright lowlevel pahsmms lowlevel dura  channelsagents each channel tion models parameters σ  section  set accurately  states time state transitions restricted left using corresponding simulator parameters beam  toright each state transition state size set manually  hspahmm randomly ini  states dimensional gaussian observation models tialized output models leftright lowlevel pahmms  means uniformly distributed  level duration models means set summing  variances set each state gaussian means corresponding lowlevel pahsmms  duration models means range  vari simulator set parameters Σ section  ances set dparam built toplevel event  manually  pahmm output models initialized  transition graph uniform interevent transition probabil randomly decoding performed viterbi beam search  ities continuous observation sequences generated each model trained reestimating output models  random walks through event transition graph using embedded viterbi learning described section   corresponding lowlevel event models random noise til slope loglikelihood curve fell   added event sequences  training iterations completed ran  ginning end observation sequence figure  illus learned models test sequences obtained accu  trates procedure foraeventtoplevel transition graph racy measures using metric − − −  seen setup corresponds hpahsmm nnumber events test set dno deletion errors  model structure observation sequences generated sno substitution errors ino insertion errors  ing setup described using event toplevel tran accuracy complexity decod  sition graph each sequence  individual events ing algorithms depend manually set parameters kΣ  each event occurred  times entire set hspahmm hpahsmm ﬁrst investigated                                                    ijcai                                                    algorithm  embedded viterbi learning    numtrain  number training samples          pc ji  probability transitioning state state     channel          pc oij  probability observing symbol oi state     channel          pc di  probability spending duration state chan     nel    numt rain     jointhmm ← string hmms wordsevents form       ing training sequence     repeat       maxpath ← state sequence maximum probability path         through jointhmm obtained using decoding algorithm                 ni ← times channel state maxpath                nij ← times channel transitions state         state maxpath           coj                                                  figure  structure event simulator      ni   ← times oj observed state channel         maxpath           cd      ni  ←  times state channel spends duration         maxpath      reestimate parameters using following equations                        pc ji ← nij ni                  coi      pc oij ← nj nj                  cd      pc di ← ni ni    convergence    split hmms update corresponding wordevent hmms   end    effects varied parameters ran  iter figure  variation hspahmm speedframessec fps  ations traintest setup described hspahmm accuracy asigma beam size bbeam size  hpahsmm each parameter value figures  show sigma  variations    seen increasing Σ hspahmm pro                                                        ties hands through complex sequence states si  duces signiﬁcant drop frame rate does affect                                                        multaneously each sign distinct durations hi  accuracy hand increasing beam sizek pro                                                        erarchical structure phoneme word sentence level  duces signiﬁcant increase accuracy cost slower                                                          experimented set  test sentences  speed hpahsmm increasing beam size does                                                        larger dataset used vogler metaxas provided  improve accuracy based observations ran set                                                         drvogler sequences collected using   tests comparing hspahmm Σ                  tm                                                        motionstar      frames second vocabu  hpahsmm   pahmm table  summarizes                                                        lary limited words each sentence  words long  average accuracies speeds hspahmm pro                                                        total  signs input contains loca                                                        tion hands each time instant calculate    model         accuracy                    speed    hpahsmm                                        instantaneous velocities used observation                            vector each time instant    hspahmm           pahmm                 model each  word   channel pahmm                                                           hspahmm pahsmm      hpahsmm based        table  model accuracy speedfps       movementholdmh  model  liddell johnson                                                         breaks each sign sequence ”moves”                                                        ”holds” during ”move” aspect hand  duces huge jump performance compared plain changed during ”hold” aspects held constant  pahmm   affecting speed hspahmm’s  mh model identiﬁes aspects hand  accuracy lower hpahsmm  times faster ﬁguration like location chest chin distance body  serves good mean hpahsmm    hand shape kind movement straight curved round  pahmm                                                deﬁnitions encode signs various                                                        words terms constituent phonemes example    application continuous sign language          word ”i” righthanded signer start dis       recognition                                      tance chest index ﬁnger closed  sign language recognition useful pro end chest encoded mh model  vides good domain test hierarchical multiagent activi hpchmstrt owardhch pch indi                                                    ijcai                                                    
