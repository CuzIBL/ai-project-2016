       heuristic selection actions multiagent reinforcement learning                      ∗          reinaldo bianchi             carlos ribeiro                anna costa         fei university center     technological institute aeronautics      escola polit´ecnica   electrical engineering department     science division          university s˜ao paulo    s˜ao bernardo campo brazil     s˜ao jos´e dos campos brazil         s˜ao paulo brazil           rbianchifeiedubr              carloscompitabr               annarealipoliuspbr                        abstract                          stateaction space worsening rl algorithms                                                        formance applied multiagent problems despite      work presents new algorithm called heuris mrl algorithms proposed success      tically accelerated minimaxq hammq al   fully applied simple problems minimax      lows use heuristics speed  littman  friendorfoe qlearning littman      known multiagent reinforcement learning algo      nash qlearning hu wellman                                              rithm minimaxq heuristic function      alternative way increasing convergence rate      ﬂuences choice actions characterises rl algorithm use heuristic functions selecting      hammq algorithm function associated      actions order guide exploration stateaction      preference policy indicates cer space useful way use heuristics speed      tain action taken instead   inforcement learning algorithms proposed recently      set empirical evaluations conducted bianchi et al  successfully used      proposed algorithm simpliﬁed simulator   policy learning simulated robotic scenarios      robot soccer domain experimental results                                                          paper investigates use heuristic selection ac      show simple heuristics enhances sig                                                        tions concurrent onpolicy learning multiagent      niﬁcantly performance multiagent rein                                                        mains proposes new algorithm incorporates heuris      forcement learning algorithm                                                        tics minimaxq algorithm heuristically acceler                                                        ated minimaxq hammq set empirical evaluation    introduction                                       hammq carried simpliﬁed simulator                                                        robot soccer domain littman  based results  reinforcement learning rl techniques attractive simulations possible show using  context multiagent systems reasons frequently simple heuristic functions performance learning al  cited attractiveness rl algorithms guar gorithm improved  antee convergence equilibrium limit szepesvari                                                          paper organised follows section  brieﬂy reviews  littman  based sound theoretical founda                                                        mrl approach describes minimaxq algorithm  tions provide modelfree learning adequate sub                                                        section  presents approaches speed mrl  optimal control strategies easy                                                        section  shows learning rate improved  use applied solve wide variety control                                                        using heuristics select actions performed during  planning problems analytical model                                                        learning process modiﬁed formulation minimax  sampling model available apriori                                                        algorithm section  describes robotic soccer domain    rl learning carried line through trialand used experiments presents experiments performed  error interactions agent environment receiv shows results obtained finally section  provides  ing reward penalty each interaction multia conclusions  gent reward received agent depends  behaviour agents multiagent reinforcement  learning mrl algorithm needs address nonstationary  multiagent reinforcement learning  scenarios environment agents  represented unfortunately convergence rl markov games mgs – known stochastic games  algorithm achieved extensive exploration sgs – extension markov decision processes  stateaction space time consuming mdps uses elements game theory allows  existence multiple agents increases size modelling systems multiple agents compete                                                        accomplish tasks    ∗     support fapesp proc  cnpq proc formally mg deﬁned littman    capesgrices proc  acknowl  edged                                                  •s ﬁnite set environment states                                                    ijcai                                                       •aa                                 collection sets possible ac initialise qˆts                           tions each agent                              repeat    •t  s×a   × ×ak  → Πs state transition func visit state      tion depends current state actions select action using  − greedy rule eq       each agent                                       execute observe opponent’s action    •r    s×a    × ×a   →                              receive reinforcement rs                          set reward functions                    s      specifying reward each agent receives    observe state                                                            update values qˆs according    solving mg consists computing policy π  s×  ××a                                                   qˆts ← qˆts           maximizes reward received agent                                                                                                           αrs oγvts  − qˆts  time                                                        paper considers wellstudied specialization mgs ←   players called agent oppo stop criterion reached  nent having opposite goals specialization called zero  sum markov game zsmg allows deﬁnition            table  minimaxq algorithm  reward function learning agent tries maximize  opponent tries minimize                                                        case agent knows advance action taken    player zsmg  littman  deﬁned                                       π  s×a×o  quintuple s  rwhere                     opponent policy deterministic                                                         equation  simpliﬁed    •s ﬁnite set environment states                                                                      smaxmin   qs            •a ﬁnite set actions agent perform                   a∈a o∈o    •o         ﬁnite set actions opponent perform case optimal policy π∗   ≡                                                                       ∗    •t   s×a×o→Πs state transition function    arg maxa mino  possible action choice      Πs probability distribution set rule used standard  − greedy      states s deﬁnes probability transition                       s                                           arg max min qˆs ≤      state state time  learn                                                                                πs                                           ing agent executes action opponent performs             arandom                    action                                                                 •r  s×a×o→ reward function speciﬁes      random value uniform probability                                                                ≤ ≤      reward received agent executes action     parameter deﬁnes explo                                                                                                           opponent performs action state rationexploitation tradeoff greater value                                                        smaller probability random choice arandom    choice optimal policy zsmg trivial random action selected possible actions state  performance agent depends crucially nondeterministic action policies general formu  choice opponent solution problem lation minimaxq deﬁned littman                                     obtained minimax algorithm russell norvig  banerjee et al           evaluates policy agent regarding possi finally minimaxq algorithm extended  ble actions opponent choosing policy cover domains mgs applied  maximizes payoff                             robotic soccer littman  bowling veloso     solve zsmg littman  proposed use economy tesauro   similar strategy minimax choosing action problems minimaxq algorithm  learning algorithm minimaxq algorithm table  agent iteratively estimates learning early stages  minimaxq works essentially way qlearning                                                    basically random exploration update value  does actionvalue function action state stateaction pair time each interaction  opponent takes action given                                                       environment larger environment longer                                                 qs ors oγ     trialanderror exploration takes approximate function                                                                                  s∈s                            alleviate problems techniques described                                                        section proposed  value state computed using linear pro  gramming strang  equation                                                          approaches speed multiagent            max   min    qs oπa            reinforcement learning                  π∈Πa o∈o                            a∈a                         minimaxsarsa algorithm  banerjee et al   agent’s policy probability distribution ac modiﬁcation minimaxq admits action  tions π ∈ Πaandπa probability taking action chosen randomly according predeﬁned probability sep  opponent’s action                    arating choice actions taken update    mg players actions consecutive values a chosen according greedy policy  turns called alternating markov game amg minimaxsarsa equivalent minimaxq                                                    ijcai                                                     a selected randomly according predeﬁned probabil variable used weight inﬂuence heuristic usually  ity distribution minimaxsarsa algorithm achieve   better performance mimimaxq banerjee et al  general rule value hts used hammq    work proposed minimaxqλ algorithm  higher variation qˆs val  combining eligibility traces minimaxq eligibility ues ∈s ∈oinsuchawaythatitcan  traces proposed initially tdλ algorithm sutton inﬂuence choice actions low   used speed learning process track possible order minimize error deﬁned  ing visited states adding portion reward received     each state visited episode instead      max qˆs − qˆs oη  πh                                                              updating stateaction pair each iteration pairs                                                                         eligibilities different zero updated allowing                                                                                                          wards carried stateaction pairs banerjee η                               πh  et al  proposed minimaxsarsaλ combi  small real value usually     nation algorithms efﬁcient action suggested heuristic policy  combines strengths heuristic function used choice    different approach speed learning process action taken proposed algorithm different  use each experience effective way through tempo original minimaxq way exploration carried                                                        rl algorithm operation modiﬁed  ral spatial action generalization minimaxqs algo              rithm proposed ribeiro et al  accomplishes spa dates function minimaxq  tial generalization combining minimaxq algorithm proposal allows theoretical conclusions ob  spatial spreading actionvalue function tained minimaxq remain valid hammq  receiving reinforcement actionvalue pairs theorem  consider hammq agent learning deter  involved experience updated ministic zsmg ﬁnite sets states actions bounded  coding knowledge domain similarities spread rewards ∃c ∈∀s rs discount factor  ing function allows single experience single γ  ≤ γ values used heuristic  loop algorithm update single cost value function bounded ∀s hmin ≤ hs ≤ hmax  consequence taking action state st spread agent qˆ values converge q∗ probability  pairs real experience time actually uniformly states ∈s provided each state  s strt                                  action pair visited inﬁnitely obeys minimaxq                                                        inﬁnite visitation condition    combining heuristics multiagent                                                        proof hammq update value function approxi     reinforcement learning hammq                  mation does depend explicitly value heuristic     algorithm                                          function littman szepesvary  presented list  heuristically accelerated minimax hammq algo conditions convergence minimaxq  rithm deﬁned way solving zsmg making condition hammq jeopardy  explicit use heuristic function  s×a×o→to    depends action choice necessity visiting each  inﬂuence choice actions during learning process pair stateaction inﬁnitely equation  considers                                                                           hs deﬁnes heuristic indicates desirability exploration strategy – greedy regardless fact  performing action agent state op value function inﬂuenced heuristic function vis  ponent executes action                             itation condition guaranteed algorithm converges                                                        qed    heuristic function associated preference pol  icy indicates certain action taken instead condition each stateaction pair visited  said heuristic function inﬁnite number times considered valid practice  deﬁnes “heuristic policy” tentative policy used – way minimaxq – using  accelerate learning process heuristic function strategies  derived directly prior knowledge domain • using boltzmann exploration strategy kaelbling et  clues suggested learning process used al   during selection action performed                                                          • intercalating steps algorithm makes alternate  agent action choice rule deﬁnes action                                                            use heuristic function exploration steps  executed agent state action  choice rule used hammq modiﬁcation standard • using heuristic function during period time   − greedy rule includes heuristic function     shorter total learning time                                                                                                   use heuristic function hammq explores          arg max min  qˆs oξhts ≤  πs                                             important characteristic rl algorithms free          arandom                            choice training actions consequence                                                     suitable heuristic function speeds learning process   s×a×o →is heuristic function sub result delay learning convergence  script indicates nonstationary ξ real does prevent converging optimal value                                                    ijcai                                                     initialise qˆts hts  repeat     visit state     select action using modiﬁed −greedy rule                                                                   equation      execute observe opponent’s action                               receive reinforcement rs     observe state s     update values hts     update values qˆs according       qˆts ← qˆts                                                           αrs oγvts  − qˆts figure  environment proposed littman      ← s                                            picture shows initial position agents  stop criterion reached               table  hammq algorithm      complete hammq algorithm presented table   worth noticing fundamental difference  tween hammq minimaxq algorithm action  choice rule existence step updating func  tion hts      robotic soccer using hammq  playing robotic soccer game task team multi  ple fastmoving robots dynamic environment figure  heuristic policy used environment  main great relevance artiﬁcial intelligence ﬁgure  arrows indicate actions performed  possesses characteristics com  plex real problems examples problems robotic  automation systems seen group robots heuristic policy used deﬁned using simple rule  assembly task space missions multiple robots holding ball opponent’s goal ﬁgure  shows  tambe  mention                  heuristic policy player ﬁgure  note    paper experiments carried using sim heuristic policy does account opponents po  ple robotic soccer domain introduced littman  sition leaving task deviate opponent  modelled zsmg agents domain learning process values associated heuristic  twoplayersaandbcompeteinaxgridpresentedin         function deﬁned based heuristic policy presented  ﬁgure  each cell occupied players ﬁgure  using equation   action turn action allowed parameters used experiments  indicate direction agent’s – north south east algorithms minimaxq hammq learning                                                                          α                         west – agent                   rate initiated  decay      ball players represented each executed action exploration exploitation rate                                                                                 γ       circle agent ﬁgure  player ex  discount factor  parameters  ecutes action ﬁnish cell occupied identical used littman  value                                                        η  opponent looses ball stays cell set  reinforcement used  reach  action taken agent leads board agent ing goal  having goal scored op  stands                                         ponent values table randomly initiated                                                          ≤  ≤    player ball gets opponent’s goal        experiments programmed  ends team scores point beginning executed amd kii mhz mb  each game agents positioned initial position ram linux platform  depicted ﬁgure  possession ball randomly thirty training sessions run each algorithm  determined player holds ball making each session consisting  matches  games game  ﬁrst implementation moves alternated ﬁnishes goal scored agents  agents                               moves completed    solve problem algorithms used      figure  shows learning curves average  train    •                                                   ing sessions algorithms agent learns      minimaxq described section                 play opponent moving randomly presents    • hammq proposed section                      average goal balance scored learning agent each                                                    ijcai                                                                                                                                               minimax−q                                               module                                      hammq                                                  limit                                                                                         limit                                                                                                                                                                                                   goals                                                                                                                                                                                                                                                                                                                                     matches                                               matches    figure   average goal balance minimaxq     figure  results student’s test minimaxq  hammq    algorithms random opponent hammq algorithms training random oppo  littman’s robotic soccer                             nent                                                                                               minimax−q                                               module                                     hammq                                                  limit                                                                                           limit                                                                                                                                                                                                     goals                                                                                                                                     −       −                                                                                                                             matches                                               matches    figure  average goal balance minimaxq  figure  results student’s test minimax  hammq algorithms agent using minimaxq hammq algorithms training agent using  littman’s robotic soccer                             minimaxq    match possible verify minimaxq worse  formance hammq initial learning phase                                                        hammq better minimaxq th  matches proceed performance algorithms                                                        match results comparable level  similar expected                                                        conﬁdence greater  figure  presents similar    figure  presents learning curves average  train sults learning agent playing opponent using  ing sessions algorithms learning playing minimaxq  learning opponent using minimaxq case  clearly seen hammq better beginning finally table  shows cumulative goal balance ta  learning process th match perfor ble  presents number games won end   mance agents similar converge matches average  training sessions sum  equilibrium                                          matches won agents different total    student’s t–test spiegel  used verify hy number matches ended draw  pothesis use heuristics speeds learning pro stands table  greater number  cess figure  shows test learning minimax goals scored hammq beginning learning  hammq algorithms random opponent  process algorithm wins matches  using data shown ﬁgure  ﬁgure possible random opponent minimaxq learning opponent                                                    ijcai                                                     
