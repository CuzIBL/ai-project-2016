               multiplegoal reinforcement learning modular sarsao                              nathan sprague                                        dana ballard                    science department                         science department                        university rochester                             university rochester                          rochester ny                                  rochester ny                        spraguecs rochester edu                            danacs rochester edu                               abstract                               used fairly distribute control mod•                                                                 ules approach independently explored        present new algorithm gmsarsao find•        humphrys  karlsson  attractive        ing approximate solutions multiplegoal rein•          simplicity shown good empirical performance        forcement learning problems modeled           number domains        composite markov decision processes according                                                                     paper highlight previously unrecognized        formulation different subgoals modeled                                                                  problem existing modular qlearning algorithms exist•       mdps coupled requirement                                                                  ing algorithms learn component policies highly        share actions existing reinforcement learning                                                                  suboptimal context composite task        algorithms address similar problem formulations                                                                  account fact component mod•       finding optimal policies component                                                                  ules forced share control show fix        mdps merging policy                                                                  problem replacing qlearning closely re•       composite task problem methods                                                                  lated sarsao learning rule resulting algorithm shows        policies optimized separately                                                                  improved performance large sample problem        perform merged        composite solution instead searching opti•       mal policies component mdps isolation          problem formalized        approach finds good policies context                                                                  underlying formalism reinforcement learning        composite task                                                                  algorithms markov decision process mdp denoted                                                                  described tuple  state      keywords reinforcement learning                                                                  space action space ts transition                                                                  function indicates probability arriving state     introduction                                                action taken state reward function rs   traditional reinforcement learning algorithms success•     denotes expected onestep payoff taking action   fully solve small singlegoal tasks main challenge    state goal reinforcement learning algorithms   area reinforcement learning scaling larger discover optimal policy rs maps states actions   complex problems scaling problem takes num•        maximize discounted long term reward   ber forms problem large        consider problem discovering joint pol•  state space problem best described set hi•  icy set mdps use   erarchically organized goals subgoals problem   subscripts distinguish mdps mdps each   requires learning agent address tasks distinct state space share common action space   form scaling paper concerned required execute action each time step      naive approach learning solve composite tasks  model intended map case single agent   create state space includes information simultaneously faced set different goals   relevant each subtask agent learn      component mdps implicitly define larger com•  joint space receiving reward subgoals   posite mdp formally goal optimal policy   accomplished problem approach    composite mdp optimal composite policy de•  suffers curse dimensionality additional state  fined policy maximizes summed discounted reward   dimensions added each new subtask size    component mdps   joint state space grows exponentially                           state space composite mdp cross prod•     promising approach sort multiplegoal     uct state spaces component mdps —   problem use known qlearning algorithm       £  sn composite reward function defined   train learning module handle each subgoals     rs  case component   internal qvalues different learning modules    mdps independent composite transition function       poster papers                                                                                                        written case   component mdps independent exact   composite transition function depend particu•  lar dependencies models      theory reason composite mdp   solved directly using traditional qlearning algo•  rithm generally practical   size composite state space grow exponentially   number component mdps    modular qlearning   humphrys karlsson humphrys  karlsson    independently developed similar approaches problem   multiplegoal reinforcement learning idea   separate learning module created each component mdp   agent takes actions environment each module   trained standard qlearning update rule        figure  results training run gmsarsao                                                                 three qlearning based algorithms food gathering task                                                                                                                           training divided trials lasting  time steps data   immediate reward learning rate      points generated suspending training  tri•  parameter discount factor applied future re•     als computing mean performance  trials with•  wards single goal reinforcement learning problems  exploration each algorithm uses egrecdy exploration   qvalues used rank order actions given   policy linearly reduced   during half   state key observation qvalues trials algorithms use fixed learning rate    used multiplegoal problems indicate degree   discount factor    preference modules different actions   possible ways values used select     problem modular qlearning   compromise action execute different approaches                                                                 qlearning attractive qualities basis   referred action selection mechanisms                                                                 multiplegoal reinforcement learning chief     karlssons suggestion calls greatest mass                                                                 fact offpolicy learning method means   generate overall qvalue simple sum qvalues                                                                 qlearning single mdp guaranteed converge   individual modules qsa —                                                                 optimal solution regardless policy followed during   action maximum summed value chosen                                                                 training long each stateaction pair visited infinitely   execute refer approach gmq greatest                                                                 limit fact makes easy prove conver•  mass qlearning                                                                 gence results composite reinforcement learning algo•    humphrys considers greatest mass approach raises                                                                 rithms introduced particular easy   objection action highest sum                                                                 each module guaranteed converge optimal policy   particularly good modules result                                                                 value function mdp action selec•  module able reach goal explores                                                                 tion mechanisms generate policy deterministically   winnertakeall alternatives constrain chosen action                                                                 component value functions composite policy guar•  optimal module given state                                                                 anteed converge guarantee concerning   each modules promotes action value                                                                 quality composite solution   wlsl module largest value allowed   execute preferred action                                 unfortunately offpolicy character qlearning                                                                 limitation difficulty onestep     simplest method generating wvalues                                                                 value updates each module computed as•  refer topq set                                                               sumption future actions chosen optimally   giving control module highest qvalue                                                                 mdp assumption valid action selec•  current state method suffers draw•                                                                tion mechanisms described future actions repre•  module highest qvalue                                                                 sent compromise policy different modules   preference action chosen module                                                                 share control means computed qvalues   stands lose great deal action selected                                                                 converge actual expected return composite   method exhibits reasonable performance                                                                 policy instead max equation  results qvalues   strongly dependent structure reward functions                                                                 positive bias     better alternative referred negotiated wlearning   grant control module stands lose   long term reward selected module     modular sarsao   discovered examining qvalues current state   possible solution problem positive bias   refer humphrys  detailed description   replace qlearning onpolicy learning algorithm   algorithm                                                    particular explore use sarsao rummery                                                                                                         poster papers  niranjan  singh sutton  sutton     based algorithms section  task    update rule sarsao                                  algorithms gmsarsao exhibits best performance                                                                  conclusion    update rule virtually identical qlearning presented method learning approximately opti•   max qvalues right re•   mal policies certain class composite markov decision   placed qvalue state action pair actu• processes empirical results demonstrate approach    ally observed step case single mdps   performs better number existing algorithms future    sarsao proved converge optimal policy    work focus proving convergence results algo•   long exploration rate asymptotically decayed to• rithm longer version paper including discussion    ward zero according appropriate schedule singh et al  related work available sprague ballard           key observation purposes sarsao acknowledgments    onpolicy method does suffer problem   positive bias updates based actions  material based work supported grant   actually taken best possible action department education grant number   expect sarsao based modules discover qvalues    pa grant national institutes health   closer true expected return composite policy grant number prr grant na•     action selection mechanisms section        tional science foundation grant number el   recast use sarsao qlearning train opinions findings conclusions recommendations   modules focus method greatest       expressed material authors   mass refer resulting algorithm gmsarsao      necessarily reflect views mentioned institu•  recall goal maximize summed reward   tions   component mdps assuming trust•  worthy utility estimates each modules makes    references   sense choose action highest summed util•   humphrys  humphrys action selection methods   ity modules definition action  using reinforcement learning animals animats   lead greatest summed long term reward       proceedings fourth international conference   reasoning did hold qlearning utility      simulation adaptive behavior pages  cam•  estimates inaccurate composite policy             bridge ma       far convergence proof gm                                                                 karlsson  karlsson learning solve multiple   sarsao algorithm refer associated technical report                                                                     goals phd thesis university rochester    sprague ballard  discussion possible   convergence characteristics                                   rummery niranjan  rummery ni•                                                                    ranjan online qlearning using connectionist systems    examples                                                        technical report cuedfnfengtr  cambridge                                                                     university engineering department    figure  demonstrates performance gmsarsao                                                                  singh cohn  singh cohn dy•  sample composite task task adapted singh                                                                     namically merge markov decision processes advances   cohn  goal agent task gather                                                                     neural information processing systems volume    stationary food items avoiding predator   grid                                                                        three food items present times      agent moves possible directions  singh sutton  singh sutton reinforce•  each time step random probability          ment learning replacing eligibility traces machine    agent contacts food items receives  learning     reward  item randomly moved new posi•   singh et al  singh jaakkola littman   tion agent receives reward  time step  szepesvari convergence results singlestep   avoids predator predator moves deterministically      onpolicy reinforcementlearning algorithms machine   position agent time step           learning       positions food items positions                                                                  sprague ballard  sprague ballard   agent predator result     million distinct                                                                    multiplegoal reinforcement learning modular   states large monolithic tabular learning al•                                                                    sarsao technical report  university rochester   gorithm practical task good candidate                                                                     science department    modular reinforcement learning algorithm   decomposed small mdps mdp describes          sutton  sutton generalization reinforcement   agents interaction predator three mdps         learning successful examples using sparse coarse cod•  interaction food items each       ing advances neural information processing sys•    component mdps    states figure  shows           tems volume     performance gmsarsao three qlearning       poster papers                                                                                                        
