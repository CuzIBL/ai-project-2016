            optimal nonmyopic value information graphical models –                           efﬁcient algorithms theoretical limits                          andreas krause                               carlos guestrin                  carnegie mellon university                    carnegie mellon university                           abstract                          guarantees paper present efﬁcient algorithms                                                         guarantee optimal nonmyopic value information      realworld decision making tasks require                                                         chain graphical models hidden markov models      choose expensive observations sen                                                         hmms address settings subset selection      sor network example important select sub                                                         optimal subset observations obtained openloop      set sensors expected provide strongest                                                         fashion conditional plans closedloop plan      duction uncertainty general practice use                                                         observation strategy depends actual value ob      heuristicguided procedures selecting observations                                                         served variables cf fig knowledge      paper present ﬁrst efﬁcient optimal algorithms                                                         ﬁrst optimal efﬁcient algorithms tasks      selecting observations class graphical models                                                         class graphical models settings address      containing hidden markov models hmms provide                                                         ﬁltering smoothing versions filtering important      results selecting optimal subset observa                                                         online decision making decisions utilize      tions obtaining optimal conditional observation                                                         observations past smoothing arises example      plan problems present algorithms ﬁl                                                         structured classiﬁcation tasks temporal      tering case observations past                                                         dimension data observations taken      taken account smoothing case ob                                                         account evaluate algorithms empirically three      servations utilized furthermore prove surpris                                                         realworld datasets show wellsuited      ing result graphical models tasks designs                                                         interactive classiﬁcation sequential data      efﬁcient algorithm chain graphs hmms                                                           problems graphical models probabilistic      procedure generalized polytrees prove                                                         ference probable explanation solved      value information problem nppphard                                                         efﬁciently chainstructured graphs solved efﬁ      discrete polytrees follows results                                                         ciently polytrees prove problem maximizing      computing conditional entropies widely                    pp      used measure value information pcomplete value information np hard discrete polytree      problem polytrees finally demonstrate effec graphical models giving complexity theoretic classiﬁcation                                                                                                pp      tiveness approach realworld datasets core artiﬁcial intelligence problem np hard prob                                                         lems believed signiﬁcantly harder npcomplete                                                         pcomplete problems commonly arising    introduction                                        text graphical models special case prove  probabilistic reasoning choose sev computing conditional entropies pcomplete  eral possible expensive observations central case discrete polytrees surprising result  issue decide variables observe order ef measure uncertainty frequently used practice  fectively increase expected utility medical expert sys  tem  example multiple tests available each  test different cost systems important  optimization criteria  decide tests perform order certain order maximize value information objective func  patient’s condition minimum cost occasionally tions depend probability distributions variables  cost testing exceed value information let      xn set discrete random variables  possible outcome                                  consider class local reward functions    following running example motivates research ﬁned marginal probability distributions variables  empirically evaluated section  consider temperature class computational advantage local rewards  monitoring task wireless temperature sensors dis evaluated using probabilistic inference techniques  tributed building task certain total reward sum local rewards  temperature distribution whilst minimizing energy let subset xj   denotes  expenditure critically constrained resource     marginal distribution variable xj conditioned observa    researchers suggested use myopic    tions classiﬁcation purposes appropriate                                                                                     max  greedy approaches select observations     consider maxmarginals xj  xj     fortunately heuristic does provide performance maxx  xj  xj   xj set value                                                               tmorn high                                                                    yes   probability probable assignment                                                           high high  random variables conditioned observations local noon  eve  reward rj functional probability distribution example conditional plan decomposition reward   max     xj write                                       figure  example decomposing reward idea                                                         set random variables      xn forms                                                                chain graphical model chain xi conditionally inde                                                                             pendent xk given xj   assume  abbreviation indicate expected local rewards joint distribution speciﬁed prior  expectation taken assignments observa conditional probability distributions xi  xi time  tions important measures value information include series model temperature measured sensor                                                         example formulated chain graphical model    • entropy set rjp xj   −hxj                                                           chain graphical models originating time series            xj log xj  objective opti        xj                                            additional speciﬁc properties online decision      mization problem minimize sum resid                                                         making observations past present time steps      ual entropies choose reward function run                                                         taken account observations      ning example measure uncertainty tem                                                         future general referred ﬁltering prob      perature distribution                                                         lem setting notation  refer    • maximum expected utility concept local reward                                                                                      distribution conditional observations prior      functions includes concept utility nodes                                                             including time structured classiﬁcation problems dis      ﬂuence diagrams  × dom →  utility                                                   cussed section  general observations      function mapping action ∈ outcome ∈                                                       chain taken account situation usually      dom  reward maximum expected utility                                                       referred smoothing problem provide algo      principle states actions selected max                                                       rithms ﬁltering smoothing      imize euja      ouja      certain xj economically key insight allows ef      choose action deﬁne local reward ﬁcient optimization chains consider set observations                                                       ⊆ variable observed xj ∈      function rp xj   maxa euja     • margin  consider margin conﬁ  local reward simply rxj   rxj  xj                  max                      max   ∗     consider xj ∈ let oj subset contain      dence rjp    xj       op   xj            max               ∗              max          ing closest ancestor smoothing problem      o−p     ¯xj   argmaxxj xj                             max                          closest descendant xj conditional inde      x¯  argmax    ∗   xj  describes                     xj                              pendence property graphical model implies given      margin likely outcome clos                                                         oj xj independent rest observed variables      est runner reward function useful                                                         xj   xj  oj  follows      structured classiﬁcation purposes shown sec                                                          rxj   rxj  oj  examples demonstrate generality notion lo observations imply expected reward  cal reward generalize algorithms set observations decomposes chain simplicity  measure total entropy margin notation add independent dummy variables  probable explanation runner details omitted xn   β  rn  cn  βn    space limitations                                                         let   xi      xim  il  il      want capture constraint observations im    using notation total reward ro   expensive mean each observation xj                                                           rjxj  smoothing case given  associated positive penalty cj effectively decreases  reward example interested trading                                                                                                                                iv−  accuracy sensing energy expenditure alternatively                     possible deﬁne budget selecting observa riv xiv  xiv  − civ  rjxj  xiv  xiv    tions each associated integer cost βj                 jiv   want select observations sum cost  budget costs decrease reward                                                         ﬁltering settings simply replace rjxj  xiv  xiv   running example sensors powered solar   figure illustrates decomposition  power regain certain energy day   iv  allows certain sensing formulation consider hidden markov model unrolled time  optimization problems allows penalties budgets steps partitioned hidden variables  simplify notation write           xn emission variables     yn                                          xj ∈o                                                        hmms yi observed variables xi form chain  βo         βj extend β sets           xj ∈o                                         applications discussed sec                                                         tion  observe hidden variables    decomposing rewards                                 asking expert addition observing emission vari  following sections   present efﬁcient algo ables cases problem selecting expert labels  rithms problems optimizing value information belongs class chain graphical models addressed  class chain graphical models                   paper  subset selection                                                          input budget rewards rj costs βj penalties cj  subset selection problem want ﬁnd output optimal selection observation times  formative subset variables observe advance begin  observations running example  ≤  ≤   compute lab  deploying sensors identify time points    expected provide informative sensor readings   ≤  ≤    according model                                          sel−  lab    deﬁne objective function subsets          −  selj                                                                  −cj   rjxj  xj  laj  ljbk − βj                                 lo        −                    labk  maxajb selj                                                               Λ    argmax      selj                                                                 ab           ajb                                                                end  subset selection problem ﬁnd optimal subset  end                                                                    ∅                 o∗    argmax   lo                       o⊆sβo≤b                            repeat                                                                 Λabk  maximizing sum expected local rewards minus        ≥   ∪ xj  − βj  penalties subject constraint total cost  −  exceed budget                                    end    solve optimization problem using dynamic pro  gramming algorithm chain broken subchains   algorithm  optimal subset selection  using insight sec  consider subchain vari  able xa xb deﬁne labk represent expected subset problem openloop order ob  total reward subchain xa     xb xa xb servations irrelevant need consider split points  smoothing case observed budget level ﬁrst subchain receives zero budget  formally                                        pseudo code implementation given alg                          xb−                            consider different costs β simply choose βj    labk      max          rj xj  ∪ xa −  variables compute labn alg  uses quanti           o⊂x                    b− ja                              Λ               βo≤k                                    ties ab recover optimal subset tracing maximal                                                         values occurring dynamic programming equations  ﬁltering version                          ing induction proof obtain                         xb−                            theorem  dynamic programming algorithm described                                                                                                          labk     max           rj xj  o∪xa xb−co  computes optimal subset budget            o⊂x                                                                                                   b− ja                                             βo≤k                                     evaluations expected local rewards                                                           variables continuous algorithm ap  smoothing version  note                                                                             plicable integrations inferences necessary  max         lo eq  computing      oβo≤b                                           computing expected rewards performed efﬁciently  values labk compute maximum expected total  reward entire chain                             conditional plan    compute labk using dynamic programming  base case simply                                   conditional plan problem want compute opti                                                         mal querying policy sequentially observe variable pay                         b−                                                       penalty depending observed values select               lab      rjxj  xa                query long budget sufﬁces objective                       ja                             ﬁnd plan highest expected reward  ﬁltering                                      each possible sequence observations budget                                                         exceeded ﬁltering select observations                       b−                                                       future smoothing case observation             lab      rjxj   xa xb             chain running example ﬁlter                      ja                              ing algorithm appropriate sensors                                                         sequentially follow conditional plan deciding  smoothing recursion labk cases  choose spend budget reaching informative times sense based previous observations  base case break chain subchains se fig shows example conditional plan                                                                                                    lecting optimal observation        formal deﬁnition objective function given                                                       recursively base case considers exhausted budget    labk  maxlab   max    −cj                         jajbβ ≤k                                                                                                   jo       rjxj    −              rjxj   xj  laj  ljbk − βj                     xj ∈s  ﬁrst recursion consider op recursion jo  represents maximum expected  timal split budget subchains reward conditional plan chain  hasbeen observed budget limited          theorem   algorithm smoothing presented                                                         computes optimal conditional plan d·b·  non   jo    maxjo   max                                                                                           xj ∈o                  evaluations local rewards maximum domain                                                       size random variables       ﬁltering case            · jo   − β                                                                                    budget used optimal plan computed                                                                                                                                                ing · ·     ·     evaluations  optimal plan reward j∅                   respectively    propose dynamic programming algorithm obtain faster computation budget case obtained  ing optimal conditional plan similar subset observing require maximum computa  algorithm presented sec  utilize decompo tion distributes budget subchains  sition rewards described section  difference  observation selection budget allocation input budget rewards rj costs βj penalties cj                                                                                       π   σ  pend actual values observations          output optimal conditional plan ab ab                                                          begin    consider subchains xa     xb base case  deals zero budget setting                         ≤a  b≤   xa ∈ dom xa xb ∈ dom xb                                                             compute jabxa xb                         xb−                                              jabxa     rj xj  xa  xa                                                                ≤ab≤n xa   ∈dom  xa xb ∈dom xb                       ja                                                                   sel−  jab   ﬁltering                                                                                                                   selj  −cj  rjxj  xj                     xb−                                                                     xj ∈ dom xj       jabxa xb   rj xj  xa  xa xb  xb                                                                       ≤ ≤ − βj                     ja                                             bdl   smoothing      recursion deﬁnes  jabxa                 jajxa xj  jjbxj xb − − βj  jabxa xb smoothing expected reward            selj  selj  xj  xa xb · maxl bdj  problem restricted subchain xa     xb condi       σj xj  argmaxl bdj  tioned values xa xb smoothing      end  budget limited compute quantity iterate  jabk  maxajb selj  through possible split points                                                                     πabxa xb  argmaxajb selj  observe notable difference ﬁltering         ∈ dom        smoothing case smoothing consider                      πabk  possible splits budget resulting      σabxa xb xj  σπabk xj  subchains observation time require end                                                             end  make additional earlier observation                end   jabxa xb  maxjabxa xb  max −cj         algorithm  computation optimal conditional plan                               ajb                                                          input budget observations xa  xa xb  xb σ π            xj  xj  xa  xa xb  xbrj xj  xj  begin          xj                                                              πabxa xb           max  jaj xa xj   jjbxj  xb − − βj  ≥          ≤l≤k−βj                                                                observe xj  xj  looking time possible ﬁltering case  σabxa xb xj  recursion simpliﬁes                                    recurse                                                                   recurse  − − βj         jabxa  maxjabxa  max −cj                                 ajbβj ≤k                  end                                                        end                 xj  xj  xa  xarj xj  xj                                                            algorithm  observation selection using conditional plan              xj             jaj xa   jjbxj  − βj          theoretical limits  optimal reward obtained jn∅  j∅ problems solved efﬁciently discrete chain  alg  presents pseudo code implementation smooth graphical models efﬁciently solved discrete  ing version – ﬁltering case straightforward modiﬁca polytrees examples include probabilistic inference  tion plan compactly encoded quantities πab probable explanation mpe surprisingly prove  determines variable query σab optimization problems discussed paper gen  determines allocation budget considering ex eralization possible unless  np proofs  ponential number possible sequences observations section stated appendix  remarkable optimal plan represented order solve optimization problems  ing polynomial space alg  indicates computed likely evaluate objective function expected  plan executed procedure recursive requiring local rewards ﬁrst result states problem  parameters   xa      xb    tractable discrete polytrees  initial induction obtain                             optimal conditional plan  mean margin optimal subset                                             mean margin greedy heuristic                                                                                                                                                        mean score                                                                                                                                                              mean margin                                                mean accuracy                                                  optimal subset                                                                                                          mean accuracy           percent  improvement optimal subset            greedy heuristic                       greedy heuristic                                                                                                                                                                  number observations          number observations         number observations        temperature data improvement cpg island data set effect increas partofspeech tagging data set ef        uniform spacing heuristic ing number observations margin fect increasing number observa                                       classiﬁcation accuracy     tions margin score                                          figure  experimental results    theorem   computation expected local rewards  minutes discretized  bins  degrees  discrete polytrees pcomplete                    kelvin avoid overﬁtting used pseudo counts α      negative result specialized conditional en learning model using parameter sharing learned  tropy frequently used reward function char sets transition probabilities      acterize residual uncertainty value information prob  pm  pm   pm  pm   combining data  lems                                                  three adjacent sensors got  sample time series                                                           goal task select  time points  corollary  computation conditional entropy dis during day during sensor readings infor  crete polytrees pcomplete                        mative experiment designed compare perfor    evaluating local rewards pcomplete mance optimal algorithms greedy heuristic  suspected subset selection problem uniform spacing heuristic distributed observations  hard show npppcomplete complexity uniformly day fig shows relative improve  class containing problems believed signiﬁcantly ment optimal algorithms greedy heuristic  harder np complete problems result pro uniform spacing heuristic performance measured  vides complexity theoretic classiﬁcation value informa decrease expected entropy zero observations  tion core ai problem                               baseline seen half                                                         possible observations optimal algorithms decreased  theorem  subset selection npppcomplete dis                                                         expected uncertainty percent heuristics  crete polytrees                                                         improvement gained optimal plan subset    running example implies generalized selection algorithms appears drastic large  problem optimally selecting sensors network number observations half possible observations  correlated sensors likely computationally intractable allowed furthermore large number observations  resorting heuristics corollary extends hard optimal subset subset selected greedy heuris  ness subset selection hardness conditional plans tic identical  corollary  computing conditional plans nppphard  discrete polytrees                             cpgisland detection                                                         studied bioinformatics problem ﬁnding cpg    experiments                                         islands dna sequences cpg islands regions                                                         genome high concentration cytosineguanine se  section evaluate proposed methods quence areas believed mainly located  real world data sets special focus comparison promoters genes frequently expressed  optimal methods greedy heuristic heuris cell experiment considered gene loci  tic methods selecting observations algo hsk af al gen  rithms used interactive structured classiﬁcation bank annotation listed three cpg islands each    temperature time series                           ran algorithm  base window beginning                                                         end each island using transition emission prob  ﬁrst data set consists temperature time series collected abilities  hidden markov model used                                                      sensor network deployed intel research berkeley  sum margins reward function  described running example data continuously goal experiment locate beginning  collected  days linear interpolation used case ending cpg islands precisely asking experts  missing samples temperature measured certain bases belong cpg region     contains problems counting number satisfying fig shows mean classiﬁcation accuracy mean  assignments boolean formula                      margin scores increasing number observations    nppp   natural ai planning problems  complete results indicate expected margin scores  problem emajsat  ﬁnd assignment similar optimal algorithm greedy heuristic  ﬁrst variables cnf formula formula satisﬁed mean classiﬁcation performance optimal algorithm  majority assignments remaining variables better performance greedy heuristic
