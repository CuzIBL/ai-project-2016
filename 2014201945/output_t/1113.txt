                               occam’s razor just got sharper                                    saher esmeir     shaul markovitch        science department technion—israel institute technology haifa  israel                                    esaher shaulmcstechnionacil                          abstract                          accuracy accepting ﬁrst interpretation domingos                                                        questioned second      occam’s razor principle given hy  webb   presented cx extension      potheses consistent observed data sim uses similarity considerations specialize consistent      pler preferred machine learn leaves webb reported empirical evaluation shows      ing algorithms follow principle search cx slight advantage domains ar      small hypothesis version space  gued results discredit occam’s thesis      principle subject heated debate   murphy pazzani  reported set experiments      theoretical empirical arguments possible consistent trees produced      earlier empirical studies lacked suf accuracy tested ﬁndings inconclusive      ﬁcient coverage resolve debate work cases larger trees average better accu      provide convincing empirical evidence oc  racy recommend using occam’s principle      cam’s razor context decision tree induc additional information concept available      tion applying variety sophisticated sam major limitation work exhaustive enumeration      pling techniques methodology samples ver version space approach applicable      sion space realworld domains tests  domains features      correlation size tree ac work present alternative approach      curacy show smaller tree likely forms statistical testing occam’s thesis sample      accurate correlation sta version space approach allows use high      tistically signiﬁcant domains        dimensional domains complex concepts problem                                                        random sampling version space rarity    introduction                                       small trees sample use addition  occam’s razor attributed thcentury english logi random sampling biased sampling methods based mod                                                                                       cian william ockham principle given hy ern anytime induction algorithms esmeir markovitch                                                              potheses consistent observed data simpler   methods produce samples higher  preferred principle basis concentrations small trees  induction algorithms search small hypothesis major contribution work provide convinc  version space mitchell  studies ing empirical evidence occam’s razor context  tempted justify occam’s razor theoretical empiri classiﬁcation trees furthermore various sampling tech  cal arguments blumer et al  quinlan rivest  niques applied help better understand space  fayyad irani  number recent works sistent decision trees topdown induction methods  questioned utility occam’s razor provided theoret explore note empirical demonstration util  ical experimental evidence            ity occam’s principle does pretend provide philo    schaffer  proved learning bias outper sophical proof occam’s thesis  form bias space possible learning tasks  looks like theoretical evidence occam’s razor  occam’s empirical principle  rao et al  argued applicability context machine learning widely accepted  result realworld problems questioning valid terpretation occam’s razor given consistent  ity basic assumption uniform distribution hypotheses simpler likely lower er  possible learning tasks                              ror rate fayyad irani  formally deﬁned    domingos  argued disagreement notion decision trees ﬁxed   utility occam’s razor stems different inter   likely lower error rate  pretations given ﬁrst simplicity goal pr t pt  wherep t prob  second simplicity leads better ability error rate greater                                                     ijcai                                                           procedure tdidtea                            subset consistent trees—the trees obtainable           ∅                                      topdown induction denoted tdidt ae             return leafnil                           tdidt scheme set examples partitioned sub          ∃c ∀e ∈ classec             sets testing value attribute each subset             return leafc                             used recursively build subtree recursion stops          ←  chooseattributea                   examples class label figure  formal           ← domaina                                izes basic procedure topdown induction          foreach vi ∈                                 tdidt  ae  strict subset dtaewe             ei ←e  ∈  aevi                     claim trees dtae tdidt ae             si ← tdidteia−a                       interesting purpose model learning          return nodea visii          types trees dtae − tdidt ae                                                           tree containing subtree leaves marked        whena  numeric cutting point chosen   class obviously subtree        ﬁltered calling sid         replaced single node marked class                                                         tree internal node associated ex  figure  topdown induction decision trees stands             training set stands set attributes  amples   subtree rooted node                                                            supported training examples                                                            teresting induction    fayyad irani  provided theoretical support  favoring smaller trees showed set note including trees unjustly distort  assumptions given trees consistent results ﬁrst case larger trees logically  observed data likely lower error rate equivalent included arbitrarily weakening nega  fewer leaves berkman sandholm  tive correlation second case extra branches  questioned set assumptions argued supported training example leaves  opposite conclusion drawn    labeled randomly lowering accuracy arbi    main challenge face work empirically trarily strengthening negative correlation  test validity occam’s razor decision tree induction restrict deﬁnition occam’s empirical prin  deﬁne occam’s empirical principle   ciple consistent hypotheses experiments                                                        examine applicability pruned tdidt ae trees                         deﬁnition  let train test training testing allows draw conclusions noisy datasets  set respectively let set hypotheses consistent  etrain say satisﬁes occam’s empirical principle  sampling techniques  respect etrain etest hh drawn                                                        goal sample tdidt ae space order  ×                                                     test occam’s empirical principle ﬁrst proposed sam                                     acc hetest ≥ acc hetest h≤h ≥    pling technique uses tdidt random selection                                                        splitting attribute cutting point attribute nu  size hypothesis  ≤ acch ≤  meric refer method random tree gen  accuracy test set                 erator rtg  observe bias                                                        spect generalization quality rtg does uniformly sam    sampling version space                         ple tdidt  ae example concept                                                        attributes aaa probability construct  given learning problem like produce ing smallest tree single split higher  possible trees consistent observations test constructing speciﬁc large tree later show  size accuracy correlated ex nonuniform sampling affect validity  haustive enumeration practical real conclusions  world domains follows propose sam problem random sampling rarity small  pling alternative deﬁne population trees induction methods likely  space decision trees sample centrate small trees theoretically correlation   different sampling techniques each focuses statistically signiﬁcant sampling tdidt  different sampled space                  space sampling subspace consisting small                                                        trees test hypothesis need sample small    deﬁning version space                        trees sample obtained repeatedly  given set attributes hypothesis class deal voking rtg keeping smaller trees  set decision trees denoted dtalet number rtg invocations needed obatin reason  set examples occam’s razor applica able number small trees prohibitively high al  ble hypotheses explain observations ternative use id repeated invocations idhow  limit discussion version space—the set trees result similar trees vary differ  consistent denoted dtae furthermore ent tiebreaking decisions esmeir markovitch   decision tree learners build tree topdown focus troduced sid stochastic version id designed                                                    ijcai                                                          procedure sidchooseattributeea             improve increase figure  lists procedure         foreach ∈                                  attribute selection applied lsid goal            ← gainea                         sample small trees obtain small         ∃a entropyea               est tree use lsidr  sampler observe            a∗ ← choose attribute random        lsid  stochastic nature need                ∈  entropyea              randomize decisions                    a∗ ← choose attribute random        experimental results                                              each attribute  probability    tested occam’s empirical principle stated deﬁnition                selecting proportional                a∗                                        datasets  chosen arbitrarily         return                                         uci repository blake merz                                                          artiﬁcial datasets represent hard concepts xor             figure  attribute selection sid       additional irrelevant attributes bit multiplexer each                                                        dataset partitioned  subsets used cre      procedure lsidchooseattributeear                                                      ate  learning problems each problem consisted sub                                                   set serving testing set union remaining           return idchooseattributee             training set fold cross validation sampled        foreach ∈                                                        version space tdidt ae each training set          foreach vi ∈ domaina              ←e  ∈                        ing three methods described section  tested                                                 correlation size tree number leaves             mini ←∞                                                      accuracy associated testing set size sam             repeat  times                              ple thousand rtg sid thousand               ←  sideia−a               min  ←      min                     lsid higher costs ﬁrst present dis                   i  min                          cuss results consistent trees address          total ←    domaina min                                                problem pruned inconsistent trees        return totala minimal                                                          consistent decision trees           figure  attribute selection lsid       figure  plots sizefrequency curves trees obtained                                                        each sampling method three datasets nursery glass  sample version space semirandomly bias multiplexer fold  each three  smaller trees sid instead choosing attribute methods focuses different subspace tdidt ae  maximizes information gain choose splitting biased sampling methods producing samples consist  tribute semirandomly likelihood attribute ing smaller trees cases bellshaped curve  chosen proportional information gain indicating distribution close normal recall  attributes decrease entropy zero rtg does uniformly sample tdidt ae speciﬁc  picked randomly attribute selection pro small tree better chance built speciﬁc  cedure sid listed figure                  large tree histograms indicate fre    hard learning tasks parity concepts quency small trees sample similar large  id’s greedy heuristic fails correctly estimate useful trees symmetry explained fact  ness attributes mislead learner produce large trees small trees verify  relatively large trees cases sid theory compared distribution tree size rtg sam  produce signiﬁcantly smaller trees probability ple trees reported murphy pazzani  low decreases number attributes  mux dataset sizefrequency curves  creases overcome problem use sampling space sampled space similar  technique based recently introduced lsid occam’s empirical principle states nega  algorithm anytime induction decision trees esmeir tive correlation size tree accuracy  markovitch  lsid adopts general tdidt test signiﬁcance correlation used non  scheme invests time resources making better parametric spearman correlation test each samples  split decisions candidate split lsid attempts spearman’s coefﬁcient ρ measures monotonic association  estimate size resulting subtree split variables making assumptions  place favors smallest expected size frequency distribution paired sample  ρ                                                                                  estimation based biased sample space trees deﬁned  −  dinn − wheredi differ  rooted evaluated attribute sample obtained using ence statistical rank xi yi special  sid lsid parameterized sample size correction formula presence ties  greater sample larger resulting estimate table  lists summarizing statistics rtg sid  expected accurate lsid expected lsid samplers validity occam’s empirical prin      sid ensures attributes gain zero posi statistics computed using project package  tive probability selected                      development core team                                                     ijcai                                                                                                                                                    rtg                rtg                 rtg                                  sid                               sid                                                  sid                                              lsid          lsid                lsid                                                                                                                                                                                                                                                 frequency               frequency                frequency                                                                                                                                                                                                                                                                                                                               tree size               tree size                tree size             figure  frequency curves nursery left glass middle multiplexer left datasets                                   rtg          √             sid        √            lsid        √          dataset     accsize         ρ         accsize         ρ         accsize        ρ            breastw   ± ±      ±  ±       ±  ±             bupa       ± ±   ±        ±    ±         ±             car        ± ±     ±  ±      ± ±            cleveland  ± ±    ±        ±    ±         ±             corral     ± ±       ±  ±       ±  ±     na          glass      ± ±      ±   ±       ±  ±             hungerian  ± ±      ±   ±        ±  ±             iris       ±  ±      ±   ±        ±  ±              monks    ± ±     ±  ±     ±  ±  na   na          monks    ± ±      ±  ±       ± ±              monks    ± ±     ±   ±      ±  ±    na          mux     ± ±     ±  ±      ± ±            nursery    ± ±   ± ±     ± ±            scale      ± ±       ±  ±        ± ±              splice     ± ±   ± ±     ± ±            tictac    ± ±     ±  ±      ± ±            voting     ±  ±     ±   ±        ±  ±             wine       ± ±     ±   ±        ±  ±              xor      ± ±     ± ±      ±  ±            zoo        ±  ±      ±   ±        ±  ±   na    table  testing occam’s empirical principle using different sampling methods produce consistent trees each method  report accuracy tree size spearman’s correlation coefﬁcient ρ averaged  partitions report  number times  negative correlation statistically signiﬁcant     ciple tested spearman’s method listed rightmost pothesis rejected higher previous  column each sampling method each dataset samplers possible reason phenomenon  count times  folds null hypoth lsid samples trees tighter sizerange  esis states variables correlated increased probability ﬁnding trees   rejected α signiﬁcance level  size accuracy  greater  alternative hypothesis correlation negative indicated frequency curves different sam    results indicate random sampling rtg pling methods cover different portions space  used occam’s empirical principle measured spear times overlap interesting question conclu  man’s test valid problems scale sions hold analyze samples note  results sid sampling method indicate statistically valid merge samples rtg sidand  focusing smaller trees simplicity beneﬁcial lsid different distributions  bias accuracy scale dataset measured correlation statistics combined samples  strong inverse correlation size accuracy results similar strong neg  numbers indicate correlation weaker rtg ative correlation size tree accuracy  case signiﬁcant domains         illustrate correlation size tree    lsid samples focus small trees accuracy grouped trees single run bins  cases lsid reach smallest tree possible according size calculated average accuracy  negative correlation signiﬁcant each bin did each sampling methods  domains number cases null hy bins  observations discarded figure                                                         plots results nursery glass multiplexer    note cases signiﬁcance tests com datasets error bars represent conﬁdence intervals  puted large number ties indicated na table α                                                     ijcai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      accuracy                accuracy             accuracy                                                                                                                                                                                                                                                                                                                                                                   tree size               tree size                tree size                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      accuracy                accuracy             accuracy                                                                                                                                                                                                                                                                                                                                                         tree size               tree size                tree size                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       accuracy                accuracy                 accuracy                                                                                                                                                                                                                                                                                                                                                      tree size               tree size                tree size  figure  correlation size accuracy using rtg leftmost sid middle lsid rightmost upper  graphs represent results nursery dataset graphs middle row stand glass dataset lower  graphs stand multiplexer dataset      graphs show strong correlation size  conclusions  accuracy conﬁrming occam’s empirical principle occam’s razor states given consistent hypotheses  nursery multiplexer datasets correlation simpler preferred principle  strong  samplers glass correlation weaker subject heated debate theoretical empirical ar  trees small graphs represent guments work provided  each size range bin indicate positive sup convincing empirical evidence validity occam’s  port showed occam’s principle result size principle respect decision trees state occam’s em  bias sampling methods                         pirical principle welldeﬁned learning prob                                                        lem consisting training set testing set show ex    pruned decision trees                            perimentally principle valid known learn  formally occam’s empirical principle deﬁned section ing problems note study purely empirical   applicable consistent hypotheses decision does attempt reach indisputable conclusion  tree learners necessarily produce consistent occam’s razor epistemological concept  models output pruned tree attempt avoid testing methodology uses various sampling techniques  overﬁtting data usually phases ﬁrst sample version space applies spearman’s correla  tree grown topdown pruned second tion test measure monotonic association  set experiments examine taking simplicity size tree accuracy experiments conﬁrm oc  bias ﬁrst stage beneﬁcial tree later cam’s empirical principle show negative correla  pruned measure correlation tion size accuracy strong  size unpruned trees accuracy pruning exceptions conclude general simpler trees    table  summarizes results statistics likely accurate observe results  measured single change accuracy measured contradict reported murphy pazzani  applying errorbased pruning quinlan asinthe complement claim smaller tree al  case consistent trees examining overall correlation ways accurate show domains smaller  tween size tree accuracy indicates trees likely accurate  datasets inverse correlation statistically signiﬁcant view results strong empirical evidence    table  gives percentages pruned leaves utility occam’s razor decision tree induction im  trees produced rtg aggressively pruned portant note datasets used study nec  percentage pruned leaves lsid relatively low essarily represent possible learning problems  stronger support decisions datasets frequently used machine learning  leaves smaller consistent trees               search considered typical tasks induction algorithms                                                    ijcai                                                     
