          image modeling using tree structured conditional random fields              pranjal awasthi                aakanksha gagrani                balaraman ravindran        ibm india research lab∗                dept cse                      dept cse               new delhi                         iit madras                       iit madras          prawasthinibmcom            aksgagcseiitmernetin           ravicsiitmernetin                          abstract                          bottomup cues refer statistical properties individual                                                        pixels group pixels derived outputs various      paper present discriminative framework ﬁlters topdown cues refer contextual features present      based conditional random ﬁelds stochas    image information represents interactions      tic modeling images hierarchical fashion exist labels set ythesetopdown cues      main advantage proposed framework   need captured different levels scale different      ability incorporate rich set interactions tasks described kumar hebert  bottomup      image sites achieve inducing cues color texture features image      hierarchy hidden variables given label sufﬁcient ambiguous discriminating pixels      ﬁeld proposed tree like structure model similar spectral signatures reference figure  taken      eliminates need huge parameter space corel image database observed rhino      time permits use exact ef land sea sky pixels closely related terms      ﬁcient inference procedures based belief prop spectral properties expanded context win      agation demonstrate generality ap dow resolve ambiguity obviates improba      proach applying important   ble conﬁgurations like occurrence boundary      vision tasks image labeling object snow water pixels enlarging context win      tection model parameters trained using   dow captures expected global relationships  polar      contrastive divergence algorithm report bear surrounded snow  sky image      performance real world images compare   water fails capture      existing approaches                     likely make large number errors difﬁcult                                                        test images work try model relationships    introduction                                       proposing discriminative hierarchical framework  probabilistic modeling techniques used effectively  vision related tasks image seg  mentation classiﬁcation object detection tasks  typically require processing information available different  levels granularity work propose probabilistic  framework incorporate image information various  scales meaningful tractable model framework  like existing image modeling approaches built  random ﬁeld theory random ﬁeld theory models  uncertain information encoding terms set  random variables underlying graph structure gthe  graph structure represents dependence relationship  variables interested problem inferenc  ing stated given values taken  subset ∈s estimate probable values figure  contextual features multiple scales play vital  subset ∈s conventionally deﬁne variables role discrimination use local feature fails  set observations variables set labels conclusive answer    information relevant constructing accurate  model image broadly classiﬁed cate  related work  gories widely known bottomup cues topdown cues                                                        past geman geman   proposed    ∗part work author iit madras markov random field mrf based models joint mod                                                    ijcai                                                    eling variables generative nature advantages offered hierarchical models discrimi  mrfs restricts underlying graph structure simple native approaches single framework rest pa  dimensional lattice capture local rela organized follows formally present frame  tionships neighboring pixels  ×  work section  section discuss graph   ×  window computational complexity train structure form probability distribution train  ing inference procedures mrfs rules pos ing inference tcrf discussed sections   sibility using larger window size multiscale random  respectively present results model  fields msrfs bouman shapiro  try remove different tasks section  possible extensions  limitation capturing interactions labels framework discussed section  ﬁnally conclude  hierarchical framework similar approach proposed section   feng et al  using tree structured bayesian net  works tsbns approaches ability capture  tree structured conditional random field  long range label relationships inspite advantage  fered hierarchical nature methods suffer let observations corresponding labels  limitations generative models presenting framework ﬁrst state deﬁnition                                                                                                  • strong independence assumptions observa conditional random fields given lafferty et al                                                                  tions                                               • introduction large number parameters model  crf deﬁnition      ing joint probability    •                                                   let  ve    graph indexed      need large training data                     vertices gthenx  conditional random    recently introduction conditional random ﬁeld conditioned random variables yi ∈  fields crfs use discriminative models classiﬁ obey markov property respect graph  cation tasks popular lafferty et al  crfs pyi  xyji jpyi  xyji∼ jherei ∼ implies  offer lot advantages generative approaches di yi yj neighbors  rectly modeling conditional probability  xsince applying hammersleyclifford theorem li   assumption underlying structure conditional probability py  factors product  observations model able incorporate rich set potential functions real valued functions deﬁned  nonindependent overlapping features observations cliques graph  context images various authors successfully ap                                                                                    plied crfs classiﬁcation tasks reported signiﬁ                                                                                     py         ψcyc            improvement mrf based generative models                     et al  kumar hebert                                          ∈    discriminative random fields drfs closely equation set cliques graph  semble crfs applied task detecting man yc set variables belong clique  structures real world images kumar hebert normalization factor   model outperforms traditional mrfs  strong capture long range correlations  tcrf graph structure  labels rigid lattice based structure allows ×  pairwise interactions et al  propose mul consider   neighborhood pixels shown figure   tiscale conditional random fields mcrfs capa way model association labels  ble modeling interactions labels multiple pixels introduce weight vector edge  scales work consists combination three differ represents compatibility labels  ent classiﬁers operating local regional global contexts nodes figure alternative introduce  respectively suffers main drawbacks hidden variable connected  nodes    •                                                   value variable takes induces probability      including additional classiﬁers operating different distribution labels nodes connected fig      scales mcrf framework introduces large ure      number model parameters    • model assumes conditional independence hidden      variables given label ﬁeld    number generative hierarchical models  proposed similar work involving discriminative frame  works limited kumar hebert  paper  propose tree structured conditional random field tcrf  modeling images results demonstrate tcrf  able capture long range label relationships robust  applied different classiﬁcation tasks pri figure  different ways modeling interactions  mary contribution work method combines neighboring pixels                                                    ijcai                                                      dividing image regions size × matrix weights size l×f estimated  introducing hidden variable each gives layer training data transformation possibly non  hidden variables given label ﬁeld conﬁgu linear applied observations approach  ration each label node associated hidden variable Γiyi xlogp yi  xwherep yi  prob  layer main advantage following ap ability estimate output separately trained local classi  proach long range correlations nonneighboring ﬁer et al   pixels easily modeled associations hid                                                        edge potential  den variables layer attractive feature                              tb  resulting graph structure induced tree al deﬁned edges tree let φ matrix                                                        weights size l×lwhich represents compatibility  lows inference carried time linear number                                        th  pixels felzenszwalb huttenlocher  motivated values hidden variable level  introduce multiple layers hidden variables neighbor level  quadtree structure                                                           letl denote number levels tree  given label ﬁeld each layer hidden variables tries                                          capture label relationships different level scale af root node level  image sites level                                                        denote set hidden variables present level tthen  fecting values hidden variables layer                  ∈          ∈  work value equal edge  l−                                                                                           ht  l−by  quadtree structure model shown figure  edge potential deﬁned exp φ ina                                                        similar manner edge potential node hi level                                                                th                                                        neighbor hj level represented                                                              tb                                                        exphi φ  hj                                                          overall joint probability distribution product                                                        potentials deﬁned written                                                                                                                                                                                                                               ∝  exp    yi wfix                                                                                           ∈                                                                                i                                                                                              l−b                                                                                          hi φ    yj                                                                                 yj ∈y hi∈hl−                                                                                      ∈e     figure  tree structured conditional random ﬁeld                           yj hi                                                                               l−                                                                                                       tb    formally let set label nodes set                                   hi φ  hj  observations set hidden variables                    hi∈ht hj ∈ht  tcrf following conditions hold                                  hihj ∈e    exists connected acyclic graph ve      vertices onetoone correspondence  parameter estimation      variables ∪ number edges                                                           −                                        given set training images  yn  xn                                                  want estimate parameters Θ  Φ standard    node set    be partitioned subsets way maximize conditional loglikelihood      vl vi   vi ∩ vj  ∅ training data equivalent minimizing      vertices vl correspond variables kullbackleibler kl divergence model distri                                                        bution   yx  Θ  empirical distribution yx    degv    ∀v ∈ vl                                                                                                                               deﬁned training set approach requires    similar crfs conditional probability  calculating expectations model distribution  tcrf factors product potential functions expectations approximated using markov chain monte  form functions            carlo mcmc methods pointed  hinton                                                         methods suffer main drawbacks    potential functions                                                           markov chain takes long time reach equilib  underlying graph tcrf tree set rium distribution  cliques corresponds nodes edges graph  deﬁne kinds potentials cliques        estimated gradients tend noisy                                                            high variance  local potential                                                          contrastive divergence cd approximate learning  intended represent inﬂuence observations method overcomes problem minimizing dif  label nodes yforeveryyi ∈ function takes                                                ferent objective function hinton  work use  form expΓi  Γi  deﬁned     cd learning estimating parameters                       wf                 Γi                         contrastive divergence learning    let number classes number let model distribution empirical dis  image statistics each block yi vector length tribution label variables cd learning tries minimize                                                    ijcai                                                    contrastive divergence deﬁned               feature extraction                                                      visual contents image color texture spatial                      −                                                                        layout vital discriminatory features considering real                                                      qy                  world images extracting color information cal           y∈y qylog kullback                                                       culate ﬁrst three color moments image patch  leibler divergence   distribution ciel∗a∗b∗ color space  deﬁned step reconstruction training data incorporate entropy region account  vectors hinton  weight wij           randomness uniform image patch signiﬁcantly lower                                                        entropy value compared nonuniform patch entropy                               ∂d                    Δwij  −η                        calculated shown equation                               ∂wij                                                                                                                      −      ∗                                                                                                 logp    η learning rate let step                       construction training set weight update equations  speciﬁc model                              contains histogram counts texture extracted                                                      using discrete wavelet transform mallat  spatial                                          Δwuv      η    yi fi −    yi fi    layout accounted including coordinates                   ∈y          ∈y                 image site accounts total  statistics                                    l−b                                        corresponding image region                                         Δφuv         η              hi  hi yj                            ∈                    yj ∈y hi hl−                         object detection                         yj hi∈e                                                     apply model task detecting manmade struc                                                    tures set natural images corel image database                   −             ep                                       hi                            ×                                                       each image   pixels divide each image                      ∈   hi∈hl−                     yj                                             ×                          yj hi∈e                    blocks size   each block labeled struc                                                     tured unstructured blocks correspond leaf          tb                                                                             Δφuv      η              hihj  hi hj   nodes tcrf order compare performance                    hi∈ht hj ∈ht                      drfsweusethesamesetof     training  testing                         hihj ∈e                                                     images used kumar hebert wealsoim                                                    plement simple logistic regression lr based classiﬁer                   −             ep                                        hi hj         order demonstrate performance improvement obtained                     hi∈ht hj ∈ht                          hihj ∈e                    capturing label relationships shown table  tcrf                                                        achieves signiﬁcant improvement lr based classi    inference                                          ﬁer considers local image statistics account                                                        figure  shows results obtained model  problem inference ﬁnd optimal label ﬁeld   ∗    argmaxyp   yx using max  imum posterior marginals mpm criterion minimizes table  classiﬁcation accuracy detecting manmade  expected number incorrectly labeled sites according structures calculated test set containing  images                                             ∗  mpm criterion optimal label pixel yi given                                                                     model  classiﬁcation             ∗                     ∀  ∈            yi  argmaxyi yi  yi                                   accuracy     tree structure model probability value        lr                                                                                           yix exactly calculated using belief propaga              drf      tion pearl                                                    tcrf         experiments results                                                          image labeling  order evaluate performance model ap demonstrate performance model label  plied different classiﬁcation tasks object ing real world images took set  images consisting  detection image labeling main aim demon wildlife scenes corel database dataset  strate ability tcrf capturing label relationships tains total  class labels rhinohippo polar bear vege  compared approach existing tech tation sky water snow ground data set  niques qualitatively quantitatively predicted high variability images each image  ×  pix  label each block assigned pixels contained els divide each image blocks size × total  order compare performance  images chosen random training remaining  models like mcrf operate directly pixel  level use common set local features described score reported kumar hebert  ob                                                  tained using different region size                                                    ijcai                                                     images used testing compare results references  logistic classiﬁer implementation bouman shapiro  charles bouman  mcrf  et al  observed table  michael shapiro multiscale random ﬁeld model  tcrf signiﬁcantly improves logistic classiﬁer bayesian image segmentation ieee transactions  modeling label interactions model outper image processing –   forms mcrf given test set qualitative results  test images shown figure         felzenszwalb huttenlocher  pedro felzen                                                           szwalb daniel huttenlocher efﬁcient belief                                                           propagation early vision cvpr   pages  table  classiﬁcation accuracy task image label –   ing corel data set total  testing images                              ×                                  feng et al  xiaojuan feng christopher   present each size                              williams stephen felderhof combining belief                                                           networks neural networks scene segmentation                 model   classiﬁcation                     ieee trans pattern anal mach intell –                         accuracy                                        lr                                geman geman  stuart geman donald ge                 mcrf                                  man stochastic relaxation gibbs distributions                 tcrf                                 bayesian restoration images ieee transactions pat                                                           tern analysis machine intelligence pami–                                                                discussion                                         et al  xuming richard zemel   framework proposed paper falls cat miguel a´ carreiraperpi˜n´an multiscale conditional ran  egory multiscale models successfully  dom ﬁelds image labeling cvpr  pages –  applied various authors bouman shapiro        feng et al  novelty work extend                                                        et al  xuming richard zemel deba  earlier hierarchical approaches incorporate dis                                                           jyoti ray learning incorporating topdown cues  criminative framework based conditional random ﬁelds                                                           image segmentation iccv   immediate extensions possible basic  model presented work concerns hinton  geoffrey hinton training products ex  “blocky” nature labelings obtained reason perts minimizing contrastive divergence neural com  ﬁxed region size used experiments putation –   behavior typical tree based models like tsbns kumar hebert  sanjiv kumar martial  lution problem directly model image pixels hebert discriminative ﬁelds modeling spatial  leaves tcrf require increasing dependencies natural images nips mit press  number levels tree number model   parameters increase linearly number levels                                                                              training inference times increase exponentially kumar hebert  sanjiv kumar martial  number levels recently et al reported good hebert hierarchical ﬁeld framework uniﬁed  results applying model superpixelized repre contextbased classiﬁcation iccv pages –  sentation image et al  approach ieee society   duces number image sites time does lafferty et al  john lafferty andrew mccallum  deﬁne rigid region sizes currently exploring fernando pereira conditional random ﬁelds  possibility incorporating superpixelization tcrf probabilistic models segmenting labeling sequence  framework                                               data icml pages – morgan kaufmann                                                         li  li markov random field modeling im    conclusions                                           age analysis comp sci workbench springer   presented tcrf novel hierarchical model based mallat  mallat wavelets vision proceedings  crfs incorporating contextual features lev ieee –   els granularity discriminative nature crfs com  bined tree like structure gives tcrfs advan pearl  pearl probabilistic reasoning intelligent  tages proposed multiscale models tcrfs yield systems networks plausible inference morgan kauf  general framework applied variety image mann   classiﬁcation tasks empirically demonstrated  tcrf outperforms existing approaches tasks  future wish explore training methods  handle larger number parameters need study  framework adapted image classiﬁ  cation tasks                                                    ijcai                                                    
