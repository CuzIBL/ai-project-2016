              value observation monitoring dynamic systems               eyal evendar                   sham kakade                    yishay mansour∗   information science    toyota technological institute     school science       university pennsylvania            chicago il usa                 tel aviv university      philadelphia pa  usa               shamtticorg                tel aviv  israel        evendarseasupennedu                                               mansourcstauacil                        abstract                           kanazawa  —  share markov                                                        sumption naturally like provide conditions      consider fundamental problem monitor   monitoring possible modelling errors      ing tracking belief state dynamic sys present conditions dynam      tem model approximately correct ics transition states      initial belief state unknown observability true model      general setting model transition dynamics usually depends application      slightly misspeciﬁed monitoring observations depend user      sequently planning impossible errors   able obtain better observations adding sensors      accumulate time provide new      just accurate sensors paper main      characterization value observationwhich  quantifying observations useful      allows bound error accumulation        effects monitoring problem      value observation parameter gov   deﬁne proposed measure illus      erns information observation pro    trative example hmm value information      vides instance partially observable mdps vary parametric manner consider hmm       pomdp mdp     state observation reveals true state prob      unobservable markov decision process param   ability  −  probability  gives random state      eter  new parameter characterizes thought having noisy sensor intuitively      spectrum mdps unobservable mdps       parameter  varies zero state monitoring      pending information conveyed  harder      observations                                   introduce parameter characterizes infor                                                        mative observations helping disambiguate                                                        underlying hidden state coin parameter    introduction                                       value observation value observation criterion tries  real world applications require estimation quantify different belief states different                                                                                               known state given past observations goal main observation distributions formally  distance  tain track belief state distribution states tween belief states related observation dis  applications ﬁrst step tributions maintained multiplicative factor  challenging tasks learning planning value observation parameter  dynamics perfectly known approx paper use update rule variant  imate model available model initial state bayesian update perform bayesian update given  perfectly known state monitoring reduces bayesian inaccurate model add noise par  inference modelling error tran ticular mix resulting belief state uniform dis  sition model slightly incorrect belief states tribution adding noise crucial algorithm en  approximate model diverge true bayesian sures beliefs incorrectly overly conﬁdent  lief state implications divergence preventing belief state adapting fast  dire                                                 new informative information    popular dynamic models monitoring   main results show model approxi  unknown state hidden markov model hmm ra  mate modiﬁed bayesian updates guarantee  biner  juang  extensions partially observ true belief state belief state diverge — assum  able markov decision process puterman  kalman fil ing value observation negligible speciﬁ  ters kalman  dynamic bayesian networks dean cally show initial state approximately ac                                                        curate expected kldivergence belief    ∗supported grant isf bsf     state true belief state remains small show                                                    ijcai                                                    uninformative initial state arbitrary algorithm subsection  provides main monitoring  initial belief state converge belief state theorem subsection  proves theorem section   expected kldivergence true belief state small show extend results dynamic bayesian  remain finally extend networks  results setting considered boyen  koller   goal compactly represent belief state  preliminaries  precision rate convergence depends value                                                        hidden markov model hmm tuple ob  observation                                                        set states     natural setting inaccurate model                                                        transition probability form state state ob  underlying environment precisely markovian ex                                                        observations set observation distribution  ample transition model slightly inﬂu                                                        state belief state distribution states  enced extrinsic random variables given                                                        bi probability state sithe  extrinsic variables true transition model environ                                                        transition probabilities belief states deﬁned accord  ment slightly different model each time step                                                        ing hmm  transition observation probability using  case like model environment                                                        bayesian update  markovian cost introducing error                                                          belief state b· probability observing  fact transition model entirely markovian                                                        oobwhere  results apply setting encouraging                 sult cases markovian assumption           oob    oosbs  abstraction environment precise                   scription  related work work closely related observing observation belief state b·  boyen koller  considered monitor dated belief state  ing hidden markov model setting environ                  oosbs                                                                       uo bs  ment exactly known agent wants com                        oob  pact factored representation belief state                                                                 exactly factored form main assumption uo deﬁned observation update operator  environment mixing rapidly error contract deﬁne transition update operator  geometric factor each time apply transition                                                                                                             matrix operator contrast interested monitoring      bs     sbs   approximate environment model                      s  work theirs assume form contraction                                                          denote bt belief state time time   beliefs tend closer truth bayesian                                                         discuss case initial belief state  updates — through assumption value                                                         known case unknown observing  observation through assumption tran                                                        observation ot ∈ ob inductive computation belief  sition matrix main advantage method                                                        state time  applications improve quality observa                                                                               ob   tions adding better sensors mix                   ot  ing assumption used boyan koller alter  able furthermore ﬁnal section explicitly consider ﬁrst update belief state observation                                                                                             assumption setting show belief state date operator according observation andthenbythe  compactly maintained model approx transition update operator straightforward consider                                                                                        imate additional error accumulates maintain different update order distribution                                                                                          ing compact factored representation                states conditioned observing                                                                                      particle filtering doucet  different monitoring initial belief state   approach estimates current belief state  making clever sampling limit observes  approximate monitoring  true belief state major drawback method interested monitoring belief state case  case large variance requires samples model inaccurate  combination methods considered correct initial belief state let assume  ng et al                                     algorithm access transition matrix p observa                                                building work boyen  koller  tion distribution o error respect true                                 trajectory tree kearns et al   mcallester singh models algorithm’s goal accurately estimate   provides approximate planning algorithm similar                             ˆb  extensions using algorithm possible       belief state time  denote                                                          notational simplicity deﬁne eo∼b  eo∼o ·b     outline outline paper follows sec                                                                                                     p clear context deﬁne  tion  provide notation deﬁnitions section                main section paper deals monitoring t wheno o clear                                                                                            ob  composed subsections subsection  describes context deﬁne uo uo uo uo                                                     ijcai                                                      main behavior                  belief state update                                                                                        present belief state update naive approach                     kl   ˆb                                                  just use approximate transition matrix pˆ ap                                                        proximate observation distribution oˆ problem  expectation taken respect observation se                                                        approach approximate belief state place neg  quences ot−  drawn according true model                                                     ligible probability possible state mistake         ˆb                          belief states time  respect irreversible  observation sequences                            consider following update operator t˜ each states    order quantify accuracy state monitoring ∈  assume accuracy conditions approxi                                                                 t˜s−  u tˆsu unis  mate model kldistance natural error measure   uni  assumptions make accuracy   uniform distribution intuitively update                                                                u˜  model later reﬂected quality monitoring operator mixes uniform distribution weight                                                        u  keeps probability state  assumption  accuracy  given hmm   model    bounded away zero unfortunately mixture  ob oant o accurate model hmm     uniform distribution additional source inaccuracy  ob states ∈        belief state analysis                                                        account                                       klp  ·sp ·s ≤ t                  belief state update follows              kl  ·s o ·s ≤                                       ˆb    t˜uˆ ˆb                                                                        ot                                                                        ˆbt previous belief state    deﬁne value observation parameter  deﬁnition  given observation distribution oletm  monitoring belief state  matrix entry oosthevalue subsection present main theorem relates               γ                       mx             accuracy belief state main parameters  observation  deﬁned infxx                                                quality approximate model value observation                                                        weight uniform distribution    note value observation γ                                                        theorem  time let ˆbt belief state updated ac  belief states                                                        cording equation  bt true belief state zt      b −   ≥o  ·b  − ·b   ≥ γb −                                               klbtˆbt  γ value observation                                                                                            ﬁrst inequality follows simple algebra                zt ≤ zt   − αzt     parameter γ plays critical rule analysis                     √              γ                                                          t   u       γ  o α                         γ            b −     o ·b  −                   log                log  extreme                                                              u  ·b                                                                 ˆ                   note deﬁnition similar def furthermore b − b ≤ α times  inition dobrushin coefﬁcient supb p  · −                                                                                                log √   ·                                                                            u        widely used ﬁltering literature           zt ≤                moral  consider examples                               α      γ       γ                   let   consider  having support state initial belief states ˆb  δthere                            b −                                                         state case     exists time τδ ≥  ≥ τδ      o ·b  − ·b                                                    fore             implies                              √                                                                                   log u  different observations states holds   zt ≤       δ           δ  states implies given observation          α         γ  uniquely recover state illustrate value observation following corollary completely speciﬁes algo  characterization pomdp terminology γ rithm providing choice u  weight uniform  fully observable mdp observation appear distribution                                                                                                      ˆ                positive probability states extreme corollary  assume b − b ≤ α u   unobservable mdp value γ t                                                                         log  times   o·b − o·b belief states                                                                                                             √                                                                           log t    recall example introduction state      zt ≤           t  γ o  observation reveals true state probability −            γ                                                                                 probability gives random state straight proof the choice wehave                   γ    −                                    √               √               √  forward show   approaches                    γ    ≤        γ     value observation approaches                                                     show having value γ bounded away                                                                             log        zero sufﬁcient ensure guarantee monitor       log   log         ≤  log     ing quality improves γ increases         u         t          t  paper assume γ                             completes proof                                                                           ijcai                                                                                                                          analysis                                       note zt positive supermartingale                                                                                                    z  start presenting propositions useful proving converges probability  random variable                                                                    z                        theorem proved later ﬁrst provides bound expectation cannot larger α                                                                           error accumulation                             larger α expectation timestep                                                                                                      z ≥  proposition  error accumulation belief states strictly expected value deﬁnition        ˆ                              ˆ         ˆ     zt regardless initial knowledge belief  bt bt updates bt  tuot bt bt  t˜uot bt                                                    state monitoring accurate results error                                                                                                              α                                                        ˆ                 ˆ   eot klbt   bt    ≤  klbtbtu  log  o                                                    error accumulation analysis                                                                    −klo·bto·ˆbt       subsection present series lemmas prove                                                        proposition  lemmas bound difference    proposition lower bounds term propo updates approximate true model  sition  term depends value obser start proving lemma  provided begin  vation enables ensure belief states ning subsection  diverge                                                                                           proposition  value observation let γ value lemma  belief states    observation belief states ≥ μ                                                                               kltb   tb  ≤ klb t  sthen                                                                                                                                                                                                                                     proof let deﬁne joint distributions ps                                 γklbb                                              kl   ·b o ·b  ≥                            bs ps spˆs bs                                                                     log μ              proof speciﬁcally denote random ’ﬁrst’ state                                  √                     s random ’next’ state chain rule                              −γ   o  o                                                        relative entropy write    using propositions prove main theo                                                                                                       rem theorem                                       klppklbbes∼bklt        ·st·s                                         u˜    proof theorem  fact mixes            ≤   klbbt  uniform distribution μ  u  combining  propositions   recalling deﬁnition we line follows assumption                                                           let ss ss denote distributions  obtain                                                                                                                 ﬁrst state given state respectively             kl   ˆb      ≤                                                               ot      t−                      chain rule conditional probabilities                                                                                                                                            klbtˆbt − α klbtˆbt                 klppkltbtab                                                                                                                                                                                es∼tbklpss pss     taking expectation respect ot−                                                                     ≥   kl  tb tb                                                                                                                                                                                   line follows positivity relative         zt    ≤   zt   − αe  klbtˆbt                                                      entropy putting results leads claim                                                                       ≤   zt   − αz                                                         lemma bounds effect mixing uniform  line follows convexity      distribution                                                                                                                                            lemma   belief states           klbtˆbt  ≥   klbtˆbt                                                             kltbtb˜  ≤    − u kltbtbˆ u log  proves ﬁrst claim theorem    proceed case the initial belief state proof convexity                      ˆ                good sense b − b ≤ α thenwehave                                                             kltb   tb˜  ≤   − u kltb tbˆ                                       − αz                                                 α  function t                                kl   tb uni ·               −   α                      ≤                                               derivative   which positive  α                                                                           ≤    − u kltb tbˆ   zt   mapped   theneveryzt ≤                                                                α               α                  α                                      mapped smaller value zt remain                     log              α     conclude subtle case unknown initial belief line uses fact relative entropy  state deﬁne following random variable        tween distribution uniform bounded                                                      log                                                                                               t         α                     combining lemmas obtain following                zt                                α                   lemma transition model                                                    ijcai                                                    lemma   belief states           proof proposition  using deﬁnitions updates                                                        previous lemmas                                                                                      kltbtb˜  ≤ klbbt  u log                                                                 kl   ˆb                                                             ot∼bt         dealing transition model left deal                                                                                                                ˆ  observation model provide analog lemma        eot∼bt kltuot  btt˜uot bt  regards observation model                                                                                                                                        ˆ                                                              ≤  eot∼bt kluot  btuot bt  t  u log  lemma   belief states                                                                                                                                                          ≤  klbtˆbto − klo·bto·ˆbt              kl   u        o∼o ·b                                                                      t  u log                                                   ≤ klbbo  −  klo·bo·b      ﬁrst inequality lemma  second                                                        lemma  completes proof proposition     proof let ﬁx observation owehave                                                                                           kl  u                             value observation proposition  analysis                   log                                   uobs             following technical lemma useful proof                                                                                                    os  ob  relates  norm kl divergence                                                                   log                 lemma   assume ˆbs μfor μ                                     oosbsoob                                                                                                                                                   ob                                  −                       kl  bˆb ≤b − ˆb                             log   log                                      log μ                                          oob                                                               oos             proof let set states greater ˆb                          uob slog                                                       sbs ≥ ˆbsso                                     oos                                                                                                                                                  kl  bˆb              ≤           line uses fact ob o ob           log            log                                                                          ˆbs            ˆbs  constants respect                                                     s∈a                                                                                                 let expectations ﬁrst term                          bs             bs                                                                        bs − ˆbs log     ˆbslog                                                                                     ˆbs            ˆbs                                                                 s∈a                     s∈a                                                                       o∼b       log                                                                         bs                                 ≤   log     bs − ˆbs                                                                     μ                                                                         s∈a                            oos                                                                          ob                                                            − ˆb                        ob   logb                                                                                                                    ˆbslog                                                                                       ˆb                                                                   s∈a                                 os         kl                                                                  log                                                                                                   ≤            − ˆb      − ˆb                                                                 log μ                                                                                           s∈a             s∈a  step uses fact oos simi                                                                                 b − ˆb  larly term straightforward show           log μ                                                                                                          os                      used concavity log function                                                          o∼b       log                                                                                                          s∈abs−ˆbs  b−ˆb  claim follows                           oos                                                                                                                         μ                                                         os     os         using fact                   ob                             ready complete proof make                         ob    log                                   oos         use pinsker’s inequality relates kl divergence                                                                                                                                                           norm states distributions            es∼b  klo·so·s                                                                                                                                                                klpq ≥  p − q               second term                                                                                                                 proof proposition  pinsker’s inequality                   ob                                                    sumption       eo∼b  − log          klo·b  o·b                                              o ob                                                                          √                                                                                                                                                  o·s − o·s ≤ klo·so·s ≤  o  directly deﬁnition relative entropy lemma states ∈ using triangle inequality  follows assumption                                                                                                                     ready prove proposition           o·b−o·ˆb ≤o·b−o·ˆbo·ˆb−o·ˆb                                                     ijcai                                                    
