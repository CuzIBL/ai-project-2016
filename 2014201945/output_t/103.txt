                 inductive learning sequential data scan               wei fan haixun wang philip yu                              shawhwa lo                       ibm tjwatson research                   statistics department columbia university                         hawthorne ny                                  new york ny                   weifanhaixunpsyuusibmcom                           slostatcolumbiaedu                               abstract                               dataset empirically shown                                                                  higher accuracy single classifier        recent research scalable inductive learn•                                                                   based averaging ensemble propose statistically       ing large dataset decision tree construc•                                                                 based multiple model inductive learning algorithm scans        tion particular focuses eliminating memory                                                                  dataset previous research fan et al        constraints reducing number sequen•                                                                 bl averaging ensemble shown        tial data scans stateoftheart decision                                                                  efficient accurate bagging metalearning        tree construction algorithms require multiple                                                                  paper apply hoeffding inequality estimate        scans data set use sophisticated control                                                                  probability partial complete models equal        mechanisms data structures discuss                                                                  accuracy probability higher threshold        general inductive learning framework scans                                                                  algorithm stops model construction returns cur•       dataset exactly propose exten•                                                                 rent model resulting sequential scan        sion based hoeffdings inequality scans                                                                  dataset objective completely different hulten        dataset frameworks appli•                                                                 domingos  determining change        cable wide range inductive learners                                                                  shape decision tree unlike previous research hulten                                                                  domingos  gehrke et  algorithm    introduction                                                 limited decision tree applicable wide range                                                                  inductive learners applied decision tree   recent research scalable inductive learning                                                                  learning accuracy higher single decision tree   large dataset focuses eliminating memoryconstraints                                                                  advantage ensemble reduces asymp•  reducing number sequential data scans total                                                                  totic complexity algorithm scanning data   number times training file accessed sec•                                                                 important distinction interested reduc•  ondary storage particularly decision tree construction                                                                  ing sequential scan secondary storage data   stateoftheart decision tree algorithms sprint shafer                                                                  held entirely memory base learning algorithm   et al  rainforest igehrke et al  later                                                                  allowed scan data memory multiple times   boat gchrke et  scan data   multiple times employ sophisticated mechanisms   implementation recent work ihulten domingos       scan    applies hoeffding inequality decision tree learning   streaming data node reconstructed iff sta• strawman algorithm scans data   tistically necessary decision tree set exactly propose extension scans   research reducing number data scans   data set strawman algorithm based   inductive learners focus paper propose    probabilistic modeling   general approach wide range inductive learning algo•  rithms scan dataset secondary    probabilistic modeling   storage approach applicable decision trees suppose probability instance   learners rule naive bayes learners class addition benefit matrix      ensemble classifiers studied general ap•   records benefit received predicting example class   proach scalable learning previously proposed meta           instance class  traditional accuracy  learning chan  reduces number data scans      based problems    empirical studies shown accuracy     costsensitive application credit card fraud detec•  multiple model lower respective sin•  tion assume overhead investigate fraud    gle model bagging breiman  boosting ifreund        transaction bfraud fraud —   schapire  scalable methods scan               fraud fraud  using benefit   dataset multiple times proposed method scans      matrix probability expected benefit received pre      learning                                                                                                               dieting instance class                                                                   trains sy       expected benefit —  data  training set validation set partition                                                                             number confidence                                                                   result  multiple model size    based optimal decision policy best decision la•  bel highest expected benefit                          begin                                                                    partition disjoint subsets equal size                     max  argma       assuming true label accuracy            train si   decision tree test data set                             test sv               accuracy                                                                            test ck sv   traditional accuracybased problems normal•               compute hoeffding error   ized dividing costsensitive problems usu•                confidencesatisfied true   ally represented measure benefits dollar               costsensitive problems use to•  tal benefits mean accuracy                                                     highest                                                                              second highest    straw man algorithm                                                  strawman algorithm based averaging ensemble ifan                    confidencesatisfied « false   et al assume data set partitioned                break   disjoint subsets equal size base level model                       end   trained each given example each classi•                  end   fier outputs individual expected benefit based probability          confidence satisfied                                                                              return                                                                                                                                                                                                              end   averaged expected benefit base classifiers         end                                                                      return                                                                      end                                                                                                                                    algorithm  data scan   predict class label highest expected re•  turn eq          optimal decision  finite population size tv adjusted error     obvious advantage strawman algorithm   scans dataset exactly compared scans                                                             metalearning multiple scans bagging boosting   previous research fan et al accuracy   strawman algorithm significantly higher range expected benefit class label   metalearning bagging fan et al explains  index data predefined   statistical reason averaging ensemble   base models constructed hoeffding error   likely higher accuracy single classifier trained computed using eq data example assume   dataset                                                highest expected benefit                                                                secondhighest hoeffding errors    scan                                                                                confidence prediction   lessthanonescan algorithm returns current ensem•  complete multiple model current multiple model   ble number classifiers accuracy             base models trained   current ensemble complete ensemble     algorithm summarized algorithm    high confidence random variable range   observed mean observations             scan validation set   assumption distribution hoeffd  example satisfies confidence classifiers   ings inequality states probabilit  error computed utility check satisfaction     true mean                                classifiers computed ensem•                                                               ble classifiers likely accurate model                                                             practice read example                                                                validation set memory time read new                                                                                                                learning instance validation set current set classi• estimate donation employed multiple   fiers satisfy confidence test addition linear regression method   predictions example given time guaran•       second data set credit card fraud detection prob•  tees algorithm scans validation dataset lem assuming overhead  dispute   nearly memory requirement                                 investigate fraud transaction fol•  training efficiency                                           lowing benefit matrix   extra overhead hoeffdingbased scan   algorithm cost base classifiers predict   validation set calculate statistics   main memory discussed predict       costsensitive problem total benefit sum   example validation set given time as• recovered frauds minus investigation costs data set   sume classifiers end size sampled year period contained total   validation set total number predictions approx• transaction records use data month test                                                                 data  examples data previous months train•  imately average calculation mean                                                                 ing data  examples   standard deviation incrementally need   just example anytime                  data set adult data set uc repository   calculate follows                                         costsensitive studies artificially associate benefit                                                                  class label benefit  class label                                                              summarized                                                              average number arithmetic operation approximately   use natural split training test sets re•                                                                sults easily replicated training set contains      problem proposed algorithm solves    entries test set contains  records   training set large io cost data  experimental setup   scan major overhead io cost bottleneck                                                                 selected three learning algorithms decision tree   extra cost prediction statistical analysis mini•                                                                learner rule builder ripper naive bayes learner   mum                                                                 chosen wide range partitions    experiment                                                    validation dataset sv                                                                 complete training set reported accuracy results run   empirical evaluation compare accuracy                                                                 test dataset   complete multiple model scan   scan accuracy single model trained    experimental results   data set evaluate data scan   tables   compare results single classi•  accuracy scan algorithm compared   fier trained complete dataset   scan models additionally generate dataset scan algorithm scan algorithm   biased distribution study results use original natural order dataset later   scan algorithm                                               section  use biased distribution each data set    datasets                                                  study treated traditional costsensitive prob•                                                                lem scan algorithm run confidence   wellknown donation data set ap•                                                                    peared kddcup competition suppose cost   requesting charitable donation individual   accuracy comparison    best estimate donate baseline traditional accuracy total benefits sin•  yx benefit matrix                               gle model shown columns single                                                                 tables   results baseline                                                                 scan scan algorithms achieve                                                                 scan scan algorithm each reported   costsensitive problem total benefit total   result average different multiple models rang  received charity minus cost mailing     ing   tables   results shown   data divided training set test  columns accuracy benefit compare   set training set consists  records respective results tables   multiple model   known person donation       significantly beat accuracy single model   donation test set contains  records    similar results significant increase ac  similar donation information published un•  curacy total benefits credit card data set   til kdd competition used standard train• total benefits increased approximately     ingtest set splits compare previous results fea•  accuracy increased approximately   ture subsets based kdd winning submission        kddcup donation data set total       learning                                                                                                                                                                                                                                 ripper                                                        ripper       table  comparison single model scan ensemble   table  comparison single model scan ensemble   scan ensemble accuracybased problems       scan ensemble costsensitive problems     benefit increased       batch method fact examples satis•  nb                                                           fied previously learned classifiers high probability     study trends accuracy number     necessarily satisfied base classifiers   partitions increases figure  plot accuracy empirical studies shown differ•  total benefits credit card data sets total benefits ence validation set handled doesnt significantly   donation data set increasing number partitions influence final model accuracy   base learner study sec   clearly credit card data set multiple model  biased distribution   consistently significantly improve accuracy                                                                 data biased distribution   total benefits single model   accuracy                                                                 scan algorithm need scan data uniform distri•   total benefits choices                                                                 bution produce accurate model   donation data set multiple model boosts total benefits                                                                 datascan accuracy uniform    nonetheless increases                                                                 distribution created trap using credit card   accuracy total benefits show slow decreasing trend                                                                 dataset sorted training data increasing transac•  expected extremely large results                                                                 tion detailed results shown table   eventually fall baseline                                                                 accuracy total benefits table nearly     important observation accuracy to•  identical results natural distribution reported   tal benefit scan algorithm close tables   datascan   scan algorithm results nearly identical scan algorithm  compared approxi•  data scan                                                     mately  natural distribution shown table   tables   show data scanned    datascan  confidence satis•  scan algorithm ranges   fied scan continue compute model     adult dataset     total benefits lower distribution biased   data scanned training set smallest variations base classifiers prediction wider re•  requires data partitions compute accurate model   quires data compute accurate model   scans data ripper nb be•     scan algorithm performing correct way   cause generate completely unpruned tree   wide variations different models              training efficiency     table  compare differences accuracy      recorded training time batch mode sin•  training data validation set read gle model training time scan al•  completely classifier batch sequentially gorithm scan algorithm plus time   newly computed base classifiers seq     classify validation set multiple times statistical es•  discussed section  empirical studies timation computed serial improvement   batch mode usually scans approximately          ratio scan scan algo•  training data models computed methods    rithm faster training single model figure    nearly identical accuracy extra training data plot results credit card dataset using                                                                                                                 learning  figure  plots accuracy total benefits credit card data sets plot total benefits donation data set respect    using base learner                                                               accuracy   data scan                            figure  serial improvement credit card dataset using                         batch        seq   batch      seq               scan scan             donation                                                                               adult                                                         ripper                                                                                                              numbar parthona                                    nb                                                                                   performance different classifier biased distribution      table  comparison accuracy datascan us•                performance different data scanned    ing batch memory sequential method                biased distribution    accuracybased problems                                                                            table  performance scan biased dis•                                                                           tribution    training data fit main memory machine    single classifier algorithm reduces number    data scan shafer et al  gehrke et al                   sprint reads attribute lists node breaks    hulten domingos  training time             sets attribute lists child nodes total    result shown figure  scan           file read •  file writes later rainforest gehrke    scan algorithm significantly faster single        et ai  improves performance sprint pro•   classifier scan algorithm faster         jecting attribute list each unique attribute value    scan algorithm                                                 avcset complete avcsets attributes called                                                                            avcgroup avcgroup held main mem•   related work                                                           ory selection predictor attribute efficiently                                                                            construct initial avcgroup incurs cost   scale decision tree learning sprint shafer et ai                sprint construct initial attribute lists plus scan    generates multiple sorted attribute files decision tree          sorted lists project avcsets    constructed scanning splitting attribute lists         split happens rainforest access data file   eliminates need large main memory each at•                reconstruct avcgroup child nodes exact num•   tribute list sorted data file  attributes           ber read write based variations rainforest    examples total cost produce sorted lists               chosen best scenario avcgroup ev•     logn external sort used avoid need            ery node tree fit memory rfread version    main memory each attribute list three columnsa unique             scan data each level tree   record number attribute value class label to•          write files condition met rain•   tal size attribute lists approximately three times size       forest solves problem multiple read write   original data set split takes places node           recently boat gehrke et ai  constructs coarse        learning                                                                                                                                
