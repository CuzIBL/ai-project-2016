                    generalized additive bayesian network classiﬁers                 jianguo li†‡    changshui zhang‡      tao wang   †  yimin zhang†                                †intel china research center beijing china                          ‡department automation tsinghua university china                 jianguoli taowang yiminzhangintelcom zcsmailtsinghuaeducn                          abstract                          called kdependence bayesian network kdb friedman et                                                        al  proposed tree augmented naive bayes tan      bayesian network classiﬁers bnc received   structure learning algorithm learns maximum span      considerable attention machine learning ﬁeld  ning tree mst attributes tan kdb      special structure bncs proposed    treestructure graph algorithm learns general      demonstrate promise performance     bn classiﬁcation purpose cooper herskovits       recent researches show structure learning                                                          key diﬀerences bncs structure      bns lead nonnegligible posterior prob                                                        learning algorithms structure learning task ﬁnding      lem structures sim                                                        graph structure best characterizes true den      ilar posterior scores paper propose                                                        sity given data criteria bayesian scoring      generalized additive bayesian network classiﬁers                                                        function minimal description length mdl conditional      transfers structure learning problem                                                        independence test cheng et al  proposed      generalized additive models gam learning prob                                                        purpose inevitable encounter      lem ﬁrst generate series simple bns                                                        situation candidate graph structures close      framework gam adopt                                                        score value nonnegligible posterior sense      gradientbased algorithm learn combin                                                        problem pointed presented theoretic      ing parameters construct power                                                        analysis friedman koller  candidate      ful classiﬁer large suite benchmark data                                                        bns approximations true joint distribution      sets proposed approach outperforms tra                                                        natural consider aggregating yield      ditional bncs naive bayes tan                                                        accurate distribution estimation works      achieves comparable better performance com                                                        manner example thiesson et      parison boosted bayesian network classiﬁers                                                        al  proposed mixture dag jing et al                                                         proposed boosted bayesian network classiﬁers    introduction                                         paper new solution proposed aggregate  bayesian networks bn known probabilistic graph didate bns series simple bns framework  ical models graphically represent joint probability distri generalized additive models hastie tibshirani   bution set random variables exploit adopt gradientbased algorithm learn combining  ditional independence variables parameters construct powerful learning ma  compact manner generally bn associated di chine experiments large suite benchmark data sets                                                                       ﬀ  rected acyclic graph dag nodes correspond demonstrate ectiveness proposed approach  variables domain edges correspond direct rest paper organized follows section  probabilistic dependencies pearl   brieﬂy introduce typical bncs point    bayesian network classiﬁers bnc characterize nonnegligible problem structure learning section   ditional distribution class variables given attributes propose generalized additive bayesian network classi  predict class label highest conditional prob ﬁers evaluate eﬀectiveness proposed approach  ability bncs successfully applied ar extensive experiments conducted section  finally  eas naive bayesian nb langley et al  sim concluding remarks given section   plest bn consider dependence each  feature xi class variable ignores  bayesian network classiﬁers  pendence diﬀerent features nb perform  data sets violate independence assump bayesian network directed acyclic graph en  tion bncs proposed overcome nb’s   codes joint probability distribution set random                                                                               limitation sahami  proposed general framework variables  ···  xd  denote parent nodes xi  limited dependence feature variables paxi joint distribution pbx represented                                                    ijcai                                                     factors network structures follows       mst experiments show tan signiﬁcantly outper                                                                             forms nb figure illustrates possible graph structure                pbx      pxipaxi                                                    tan                                                                                                                                                          kdb tan generate treestructure graph cooper    given data set   class vari herskovits  proposed algorithm  able bncs characterize joint distribution px                                                       adopts score measure exhaustive search learn  convert conditional distribution py predicting general bn structures  class label                                                          structure learning problem    typical bayesian network classiﬁers      given training data structure learning task ﬁnd  naive bayesian nb network assumes each attribute ing set directed edges best characterizes true  variable depends class variable     density data generally structure learning cat                                                                                       egorized levels macrolevel microlevel          px  pypxy  py pxiy                                                    macrolevel candidate graph structures known                                                        need choosing best order avoid  figure illustrates graph structure nb                                           ﬀ            overﬁtting people use model selection methods    nb ignores dependencies di erent fea bayesian scoring function minimum descriptive length  tures perform data sets violate mdl friedman et al  microlevel  attribute independence assumption bncs structure learning cares edge graph  proposed consider dependence features existed case people usually em                sahami   presented general framework  ploy conditional independence test determine im  limited dependence bayesian networks called kdependence portance edges cheng et al   bayesian classiﬁers kdb                              cases people face situa  deﬁnition  kdependence bayesian classiﬁer   tion candidates graphs edges close  bayesian network allows each feature xi max scores instance suppose mdl used crite  imum feature variables parents number rion people encounter situation candidate bn                                variables paxi equals ‘ ’ means does structure mdl score    count class variable                          spectively chosen say    according deﬁnition nb dependence bn natural select bit smaller mdl  kdb sahami  algorithm adopts mutual information score practice show similar                                                        formance perform better cases  ixi measure dependence ith feature                                                                  fact nonnegligible posterior sense  variable xi class variable conditional mutual                                                          problem pointed presented theoretic  formation ixi jy measure dependence                                                        analysis friedman koller  shows  feature variables xi jthenkdb employs heuristic rule  construct network structure measures models explain data reasonably    kdb does maximize optimal criterion structure model selection makes somewhat arbitrary choice  learning yields limited performance improvement tween models number possible struc  nb keogh pazzani  proposed superparent tures grows superexponentially number random  bayesian networks spbn assumes variables reasons don’t want struc  attribute acting public parent called superparent ture learning directly hope aggregating series sim                                                        pler weaker bns obtain accurate  attributes suppose xi super parent denote                                                        distribution estimation underlying process  corresponding bn pix                                                          note researchers proposed                                       pix     pypxi ypxi xi             schemes purpose examples learning mixtures                                                      dag  thiesson et al  ensembles bayesian net                   pypxiy      px jxi                                 ji                works model averaging rosset segal  webb et  obvious spbn structure special case kdb al  brieﬂy introduce following      figure illustrates graph structure spbn  model averaging bayesian networks  spbn algorithm adopts classiﬁcation accuracies                                                        candidate bns approximations true distri  criterion select best network structure                                                        bution model averaging natural way combine candi    friedman et al  proposed tree augmented naive                                                        dates accurate distribution estimation  bayes tan special case kdb tan  attempts add edges naive bayesian network order mixture dag mdag  improve posterior estimation tan ﬁrst com deﬁnition  px θc gc dag model following                                            putes conditional mutual information ixi equation deﬁnes mixture dag model  feature variables xi obtain                                                                      pxθs    πcpxθc gc  adjacency matrix tan employs minimum span                                                                                        π                                      π ≥  ning tree algorithm mst adjacency matrix obtain where prior cth dag model gcand                                                            π    θ  treestructure bn tan optimal sense  parameter graph gc                                                    ijcai                                                                                                                                                                                                                                                                                    xi    xj    xj      xd                                                                            xd               naive bayesian          superparent kdb      possible structure tan                                     figure  typical bayesian network structures      mdag   learns mixture models maximizing   gabn extensible framework diﬀerent  posterior likelihood given data set mdag link functions considered paper study  combining uses chessemanstutz approximation special link function fi·  log· deﬁning  yand  expectationmaximization algorithm mixture taking exponent sides equation  components structure learning parameter learning                                                                                                                    λ                                                                             λ                  webb et al  presented special simple case   expfz  exp      fiz    pi  mdag classiﬁcation purpose called average                               dependence estimation aode aode adopts series ﬁx fact potential function written  structure simple bns mixture components directly probabilistic distribution given normalization factor  assumes mixture components mdag equal                                                                                   n  mixture coeﬃcient practices show aode outperforms                             λ                                                                        pz                    naive bayes tan                                                        λz                                                                                       boosted bayesian networks  boosting commonly used technique combin λz normalization factor  ing simple bns rosset segal  employed gra         n             n                                                                                   λ  dient boosting algorithm friedman  combine bns λz         exp    λ log                                                                                               density estimation jing et al  proposed boosted                        bayesian network classiﬁers bbn adopted general ad  aboost algorithm learn weight coeﬃcients        likelihood pz called quasilikelihood    given series simple bns pix   ···                                                                                        bbn aims construct the ﬁnal approximation linear ad lλ         log pzk  ditive models px  α α ≥                                                                                   n                                ﬃ              α                                               weight coe cients     generally                                                                                λi log pizk − log λzk  constraint α relaxed α ≥ iskept                                                                                             fx   α case posterior                                                                                                     deﬁned follows                                                         λ ·    −     λ                                                                                   fzk log zk                                  expfx                                                    pyx                                                                                   λ  λ  ··· λ           ···                            y exp fx                             fzk  fzk fnzk   general binary classiﬁcation problem ∈ prob                                                          quasilikelihood optimization problem  lem solved the exponent loss function               lα      exp−yfxk            maximizing quasilikelihood obtain solution                                                      additive parameters make gam model mean  adaboost algorithm friedman et al    ingful tractable add constraints parame                                                        ters ﬁnal optimization problem turns    generalized additive   bayesian networks                                                                               λ  section present novel scheme aggregate          max                                                                                      ≤ λ ≤  series simple bns accurate density estima                                                                                                                  λ    tion true process suppose pix   ···                 given simple bns consider putting framework  generalized additive models gam hastie tibshi equation constraint lagrange multiplier  rani  new algorithm called generalized additive adopted transfer problem unconstraint  bayesian network classiﬁers gabn                   inequation constraints classical interior point    gam framework considered linear method ipm employed ipm utilizes                                                      barrier functions transfer inequation constraints se  additive variables link function space                                                      ries unconstraint optimization problems boyd van                fx    λi fipix         denberghe                                                                              ijcai                                                       adopt used logarithmic barrier function  obtain following unconstraint optimization problem   table  training algorithm gabn                                                                                                                                              input given training set  xi yi                                                                                        lλ rkα    rk     logλi  rk   log − λi      training algorithm                       i                                                                   set convergence precision  maximal step                  α −     λi  lλ                                             λ  λ  ··· λ λ                                                         initialize interior point                                                                                                    ···                   rk logλ  logn − λ ·            generate series simple bns pix                                                            sfor                   α − λ ·  lλ                                                                         select rk  andrk  rk−                                                                                                 λ α  indicates ndimensional vector elements     obtain kth step optimization problem  rk   equal  rk barrier factor kth step ipm calculate gλ quasilikelihood lλ             α  iteration lagrange multiplier                   employ lbfgs procedure solve max lλ rkα    kth ipm iteration step need max  test barrier term  imize unconstraint problem lλ α quasinewton                                                                                                              λ       − λ ·  method adopted purpose                                ak  rk log  logn                                                                  sifak jump continue loop    quasinewton method unconstraint            output optimal parameter λ∗       optimization problem                                   obtain ﬁnal generalized models pz λ∗    solve unconstraint problem max lλ rkα                       λ  gradient wrt                             series ﬁxstructure bayesian networks                           λ α      λ  theorem  gradient  rk  wrt            unresolved problem algorithm listed                                                     table  step generate series      ∂lλ rkα                                gλ       fzk − epzfzk    simple bns weak learner methods          ∂λ                                                                        purpose experiments super parent                        −       ·  − α              bn weak learner readers consider possible                      rk λ    − λ                                                               strategies generate simple bns                                                                                                  ﬀ  proof equation  easy obtain gradient ddimensional data set setting di erent                                                        tribute public parent node according equation   ﬁrst summation term nonsummation terms               ﬀ  present gradient solution second summation generate di erent ﬁxstructure superparent bns                                                          ···  figure depicts example  term log λz inlλ                                                                              kind simple bns improve performance mutual                ∂               ∂                       formation ix  computed removing bns                 log λz     λz                                             ∂λ       λz ∂λ                   lowest mutual information score way obtain                                                     simple bns adopt weak learners gabn                                                          parameters conditional probabilistic table learning                ∵ λz   exp λ · fz                                                      bns common details omitted note                                                     robust parameter estimation laplacian correction             ∂s λz                         fzkexp λ · fzk            estimate cestnik  adopted               ∂λ       zk                                                       discussions         ∂log λz            ∴                      fzkexp λ · fzk       gabn advantages typical linear additive            ∂λ         λz                            zk                         bn models boosted bn bbn gabn                                                        computational eﬃcient bbn given ddimensional                          pzkfzk  epz fzk                            zk                            samples training set hard prove com                                                        putational complexity gabn ond  mnd    computational cost consideration did                                      λ α           ipm iteration steps contrary bbn requires se  compute second order derivative  rk   quentially learning bn structures each boosting step                                             adopted quasinewton method bishop  solve leads complexity oknd boosting  problem paper lbfgs procedure provided step usually large  magnitude                         liu nocedal  employed task   fore gabn dominates bbn scalable learning task prac                                                        tice demonstrates point    ipm based training algorithm                   furthermore gabn presents new direction combin  interior point method starts point feasible ing weaker learners highly extensible framework  region sequentially adjusts barrier factor rk each itera present solution logarithmic link function  tion solves series unconstraint problem lλ rkα  hard adopt link functions gam framework    ··· detailed training algorithm shown table  propose new algorithms existing gam prop                                                        erties optimization methods seamlessly adopted ag                                                    ijcai                                                     gregate simple bns powerful learning machines       cheng et al  cheng bell liu learning                                                                   lief networks data information theory based approach      experiments                                                 artiﬁcial intelligence –                                                                  cooper herskovits  cooper herskovits  section evaluates performance proposed algo     bayesian method induction probabilistic networks  rithm compared bncs nb tan          data machine learning –   kdb spbn model averaging methods aode bbn                                                                                     decision tree algorithm cart  breiman et al       dougherty et al  dougherty kohavi sahami                                                                   supervised unsupervised discretization continuous fea     benchmark platform  data sets uci          tures th intl conf machine learing icml san fran  machine learning repository  newman   et al         cisco  morgan kaufmann  point indicated bncs data sets                                                                                         continuous features ﬁrst adopted discretization method  friedman koller  friedman koller                                                                   bayesian network structure bayesian approach struc  transfer discrete features dougherty et al    ture discovery bayesian networks machine learning –  employed fold crossvalidation error estimation       kept compared algorithms having fold split                                                                 friedman et al  friedman geiger gold  ﬁnal results shown table  results      szmidt  bayesian network classiﬁers machine learning  tan obtained java machine learning          –   toolbox weka  witten frank                                                                                         present statistical meaningful evaluation conducted  friedman et al  friedman hastie tibshirani                                                                   additive logistic regression statistical view boosting annals  paired ttest compare gabn row      statistics    table  shows wintielose summary  signiﬁ  cance level test addition figure  illustrates friedman  friedman greedy function approximation  scatter plot comparison results gabn          gradient boosting machine annals statistics    classiﬁers gabn outperforms        hastie tibshirani  hastie tibshirani general  bncs achieves comparable performance bbn          ized additive models chapman  hall   specially note spbn column shows results best      jing et al  jing pavlovi´c rehg eﬃcient dis  individual superparent bn signiﬁcant worse      criminative learning bayesian network classiﬁers boosted  gabn demonstrates eﬀective meaningful       augmented naive bayes nd intl conf machine learn  use gam aggregating simple bns                           ing icml pages –                                                                 keogh pazzani  keogh pazzani learning      conclusions                                                 augmented bayesian classiﬁers comparison distribution                                                                   based classiﬁcationbased approaches th intl workshop  paper propose generalized additive bayesian        artiﬁcial intelligence statistics pages –   network classiﬁers gabn gabn aims avoid non        langley et al  langley iba thompson  negligible posterior problem bayesian network structure       analysis bayesian classiﬁers th national conf arti  learning transfer structure learning problem  ﬁcial intelligenc aaai pages –   generalized additive models gam learning problem                                                                liu nocedal  liu nocedal limited  ﬁrst generate series simple bayesian networks       memory bfgs method largescale optimization mathemati  bn framework gam adopt           cal programming –   gradientbased learning algorithm combine sim                                                                                    ple bns construct powerful clas     newman  et al  newman hettich blake                                                                   merz uci repository machine learning databases   siﬁers experiments large suite benchmark data sets  demonstrate proposed approach outperforms       pearl  pearl probabilistic reasoning intelligent sys  traditional bncs naive bayes tan achieves     tems networks plausible inference morgan kaufmann   comparable better performance comparison boosted     rosset segal  rosset segal boosting density  bayesian network classiﬁers future work focus     estimation advances neural information processing sys  possible extensions gabn framework                   tem nips                                                                 sahami  sahami learning limited dependence bayesian  references                                                       classiﬁers nd intl conf knowledge discovery data                                                                   mining kdd pages – aaai press   bishop  bishop neural networks pattern recog     nition oxford university press london              thiesson et al  thiesson meek heckerman                                                                   et al learning mixtures dag models conf uncertainty  boyd vandenberghe  boyd vandenberghe       artiﬁcial intelligence uai pages –      convex optimization cambridge university press                                                                 webb  et al  webb boughton zhihai wang  breiman et al  breiman friedman olshen   naive bayes aggregating onedependence estimators     stone classiﬁcation regression treeswadsworthin     machine learning –      ternational group                                                                 witten frank  witten frank data mining  cestnik  cestnik estimating probabilities crucial task practical machine learning tools techniques java im     machine learning th european conf artiﬁcial intelli plementations morgan kaufmann publishers      gence ecai pages –                                                             ijcai                                                            
