peripheralfoveal vision realtime object recognition tracking video           stephen gould joakim arfvidsson adrian kaehler benjamin sapp marius messner                  gary bradski paul baumstarck sukwon chung andrew ng                                            stanford university                                         stanford ca  usa                          abstract                            little changes visual stream                                                        frame expects possible identify      human object recognition physical envi  objects portions scene time using high      ronment far superior robotic resolution fovea tracking previously identiﬁed objects      vision believe reason    peripheral region result possible      this—one heretofore     use computationally expensive classiﬁcation algorithms      signiﬁcantly exploited artiﬁcial vision relatively limited portion scene fovea      literature—is humans use fovea ﬁxate ﬁxated accumulating successful classiﬁcations      near object obtaining high resolu series frames realtime      tion image object rendering easy rec great deal happened recent years area      ognize paper present novel method location identiﬁcation speciﬁc objects object classes      identifying tracking objects multiresolution images work concentrated      digital video partially cluttered environments single frames object taking signiﬁcant      method motivated biological vision sys proportion ﬁeldofview allows accurate      tems uses learned “attentive” map  robust object recognition implies wish      low resolution data stream direct high  able ﬁnd small objects higher      resolution “fovea” objects recognized image resolutions naive approach signiﬁcant      fovea tracked using peripheral vi computational cost going higher resolution      sion object recognition run     continuous video sequences standard technique      small foveal image achieves perfor simply treat each frame image run classiﬁca      mance realtime object recognition tracking tion algorithm frame      simpler systems              frame typically using overlap indicate object                                                        frame object frame    introduction                                       ing kalman ﬁlter stabilize measurements time  human visual far outperforms robotic primary difﬁculty simple method presents  terms object recognition reasons tremendous effort misdirected vast ma  paper focus hypothesis— jority scene does contain objects  heretofore little exploited vi attempting locate kalman ﬁlter used pre  sion literature—that division retina foveal dict new location object way  peripheral regions key importance improving detect appearance new objects enter  formance vision systems continuous video scene approached camera motion  sequences brieﬂy density photoreceptor rod cone solution propose introduce peripheralfoveal  cells varies retina small central fovea model attention directed small portion  high density color sensitive cone cells responsible visual ﬁeld propose using lowresolution wideangle  detailed vision surrounding periphery contain video camera peripheral vision pantiltzoom ptz  ing signiﬁcantly lower density cones large num camera foveal vision ptz allows high resolu  ber monochromatic rod cells responsible tion small portion scene inter  things detecting motion estimates equivalent ested given time refer image scene  number “pixels” human eye vary based supplied lowresolution wideangle camera pe  spatial acuity  degrees edelman weiss  ripheral view image supplied pantiltzoom                                              ×  fahle poggio  appears order   consider example typical coffee mug distance ﬁve  pixels indepth treatment human visual meters appearing standard  ×  resolution ◦ ﬁeldof  ception refer reader excellent text view camera mm tall coffee mug reduced mere   books literature example wandell  pixels high rendering extremely difﬁcult recognize                                                    ijcai                                                    camera foveal view simply fovea            outlines related work section  present probabilis    use attentive model determine regions tic framework attention model directs foveal  peripheral view wish perform object classi gaze experimental results sample hdv video stream  ﬁcation attentive model learned labeled data real dualcamera hardware presented sec  interpreted map indicating probability tion  video streams typical ofﬁce environment  particular pixel unidentiﬁed object contain distractors objects various types  fovea repeatedly directed choosing region previously trained classiﬁers show  examine based expected reduction uncer performance attention driven signiﬁcantly  tainty location objects scene identiﬁca better naive scanning approaches  tion objects foveal view performed using  stateoftheart object classiﬁcation technologies  related work  example viola jones  serre et al   brubaker et al                               early computational architecture explaining visual    ptz camera used realworld robotic applications attention proposed koch ullman    modeled study peak magniﬁcation using model scene analyzed produce multiple fea  highdeﬁnition video hdv camera “foveal” ture maps combined form saliency map  gion selected video stream synthetically extract single saliency map used bias attention regions  ing small region hdv image advantage highest activity researchers example hu  second method input data exactly reproducible et al  privitera stark  itti et al  able evaluate different foveal camera motions suggested adjustments improvements koch ull  recorded video stream figure  shows robot man’s model review various techniques presented  mounted different camera setups         itti koch                                                           number systems inspired physiological mod                                                        els available technologies proposed ﬁnd                                                        ing objects scene based idea visual attention                                                        example tagare et al  propose maximumlikelihood                                                        decision strategy ﬁnding known object image                                                          recently active vision systems proposed                                                        robotic platforms instead viewing static image                                                        world systems actively change ﬁeldofview                                                        obtain accurate results humanoid                                                        tects follows single known object using peripheral                                                        foveal vision presented ude et al  sim                                                        ilar hardware theirs proposed bjorkman¨                                                        kragic  task object recognition pose esti                                                        mation orabona et al  attention driven sys                                                        tem described directs visual exploration                                                        salient object static scene                                                          analysis relationship corresponding                                                        points peripheral foveal views presented ude                                                        et al  describes physically control                                                        foveal gaze rigidly connected binocular cameras                                                             visual attention model  figure  stair platform left includes lowresolution periph  eral camera highresolution ptz camera topright alter visual comprises separate video cameras  native setup hdv camera replacing ptz bottomright ﬁxed wideangle camera provides continuous low  experiments mount cameras standard tripod instead resolution video stream constitutes robot’s peripheral  using robotic platform                        view scene controllable pantiltzoom ptz cam                                                        era provides robot’s foveal view ptz camera    robot stair stanford artiﬁcial intelli commanded focus region peripheral  gence robot project longterm goal build view obtain detailed highresolution image area  ing intelligent homeofﬁce assistant carry tasks scene outlined conduct ex  cleaning room dinner party ﬁnding periments recorded highresolution video streams allow  fetching items home ofﬁce ability repeatability comparing different algorithms  visually scan environment quickly identify objects figure  shows example peripheral foveal view  key elements needed robot accomplish typical ofﬁce scene attention selects  tasks                                           foveal window peripheral view corresponding    rest paper organized follows section  region highresolution video stream used                                                    ijcai                                                    figure  illustration peripheral middle foveal right views scene attentive map showing regions high  left takes approximately  seconds ptz camera new location acquire image    classiﬁcation attentive map generated fea objects  tures extracted peripheral view used determine                                                                                       ξ            ξ  direct foveal gaze discuss          tracked          unidentiﬁed                                                                 ∈t    primary goal identify track ob                 ok∈t  jects time particular like minimize ﬁrst summation objects tracked   uncertainty location identiﬁable objects second summation objects scene  scene computationally efﬁcient way identiﬁed  tention select regions scene tracked  informative understanding robot’s visual envi know total number objects  ronment                                              scene cannot directly compute entropy    able track previously identiﬁed objects                    hξkt                                                        unidentiﬁed objects ok∈t       instead learn  consecutive frames using peripheral vision probability ok ﬁnding previouslyunknown ob  classify new objects appear highresolution ject given foveal region scene based features        fovea  uncertainty tracked object’s position grows extracted peripheral view description  time directing fovea expected position model section  detect new ob  object reclassifying allows update estimate ject terms rightmost sum eq   location alternatively directing fovea different reduced expected reduction entropy taking  scene allows ﬁnd new objects action ﬁxating region fis  fovea instantaneously attention                needs periodically decide following actions Δha  ok     ξkt                                                                              unidentiﬁed              conﬁrmation tracked object ﬁxating fovea                         − htrackedξkt         predicted location object conﬁrm                                                        term       ξkt constant reﬂects      presence update estimate position                unidentiﬁed                                                        uncertainty state untracked objects  search unidentiﬁed objects moving fovea htrackedξkt entropy associated kalman      new scene running object clas ﬁlter attached object detected      siﬁers region                          section                                                           objects tracked peripheral vision uncertainty    decision takes approximately   location grows reduce uncertainty   seconds—limited speed ptz camera—for                                                       observing object’s position through reclassiﬁcation  fovea acquire image new region area expected location use kalman ﬁlter  during time track identiﬁed objects using pe track object easily compute reduction  ripheral vision fovea position search                                                        entropy taking action object ok ∈t  new tracked objects scanning scales shifts                                                                           foveal view standard practice state Δha    ln Σkt−ln Σkt      oftheart object classiﬁers                                                        Σkt Σkt covariance matrices    formally let ξkt denote state kth object                                                                          sociated kalman ﬁlter th object   time  assume independence objects reclassifying object respectively  scene uncertainty simply sum entropy terms                                                           experiments estimated term assuming largevariance    experiments single highresolution video stream distribution peripheral view possible object locations  simulate time required fovea delaying image practice algorithm’s performance appeared insensitive  selected hdv camera required number frames parameter                                                    ijcai                                                      formalism taking action estimation object’s state occlusion ob  duce uncertainty tracked object’s position ject case assume lost track  taking action reduce uncertainty object  unidentiﬁed objects assume equal costs each action objects recognized foveal view  choose action maximizes ex tracked peripheral view need transform coor  pected reduction entropy deﬁned eq   dinates views using wellknown stereo    tracking models calibration technique zhang  compute ex                                                        trinsic parameters ptz camera respect    object tracking                                  wideangle camera given objects far away rel  track identiﬁed objects using kalman ﬁlter ative baseline cameras  assume independence objects associate separate disparity corresponding pixels objects  kalman ﬁlter each object accurate dynamic model each view small corrected local correlation  objects outside current scope approach provides adequate accuracy controlling  use simplifying assumptions track each object’s coordi fovea ﬁnding corresponding location objects  nates image plane state kth tracked tween views  object                                               finally entropy tracked object’s state required                                                        deciding actions state ξt gaus                                                    ξk xk   yk  x˙ y˙k              sian random vector differential entropy                                                                                          represents ycoordinates yvelocity    htrackedξkt   lnπe Σkt     object                                                              Σ ∈ r×    each frame perform kalman ﬁlter motion update         covariance matrix associated                                                                                       step using current estimate object’s state position estimate th object time   velocity update step                   ⎡               ⎤                      model                    Δt                                                                 choose foveal region examine action                   ⎢Δt          ⎥        ξ                     ξ tη            need way estimate probability detecting                 ⎣          ⎦                                                                   previously unknown object region scene                                                                                      deﬁne “interest model” rapidly identi                                                        ﬁes pixels high probability containing objects  ηm ∼n  Σm motion model noise Δt  duration timestep                      classify useful consequence deﬁnition    compute optical ﬂow vectors peripheral view model automatically encodes biological phe  generated lucas kanade  sparse optical nomena saliency inhibition return—the processes  ﬂow algorithm ﬂow vectors measure ve regions high visual stimuli selected fur                                                        ther investigation recently attended regions  locity each tracked object zvt averaging optical                                                                                                       ﬂow object’s bounding box perform prevented attended klein   kalman ﬁlter observation update assuming velocity obser detailed review  vation measurement model                                approach each pixel peripheral view                                                        estimate probability belonging unidentiﬁed ob                                                    ject build map regions scene             zvt              ξktηv                                                             density high map atten                                                        tion determines direct fovea achieve  ηv ∼n  Σv velocity measurement model maximum beneﬁt described  noise                                                  formally deﬁne pixel interesting    object appears foveal view tak unknown classiﬁable object classiﬁable  ing action classiﬁcation returns estimate means object belongs object classes  zpt object’s position use perform cor classiﬁcation trained recognize  responding kalman ﬁlter observation update greatly reduce model interestingness yt pixel periph  uncertainty location accumulated during tracking eral view xt time using dynamic bayesian network  position measurement observation model given dbn fragment shown figure                                                                                                                       th pixel ij     pixel interesting time             zpt              ξktηp             each pixel associated                                                                                                                                               vector observed features φijxt ∈   ηp ∼n  Σp position measurement model                                                           resize image ptz camera size  noise                                                appear peripheral view compute crosscorrelation    incorporate position measurement model resized ptz image peripheral image  special case able reclassify object actual location fovea location maximum cross  object expected appear foveal view correlation small area expected location periph  example misclassiﬁcation error poor eral view                                                    ijcai                                                        yijt                                     procedure hand label single training video auto      yijt                                       matically generate data learning model given    yijt                                       speciﬁc combination classiﬁers foveal movement                                                        policies         yijt                                        adapt parameters probabilistic models       yijt                                        maximize likelihood training data described                                 yijt     yijt                                        model trained  frame video                                                        sequence  seconds  frames second figure         yijt                                     shows example learned map      yijt                                       algorithm automatically selected mugs inter    yijt                                       esting region                                             ijxt    figure  fragment dynamic bayesian network modeling  tentive      belief state entire frame time  yt  xxt exact inference  graphical model intractable apply approximate infer  ence estimate probability space constraints preclude  discussion brieﬂy applied assumed density fil                                                        figure  learned righthand panel shows proba  teringthe boyenkoller  algorithm approximate bility each pixel peripheral image left  distribution using a factored representation individ  ual pixels yt ≈ ij yij experiments  inference algorithm appeared perform    model pixel depends  experimental results  pixel’s motion compensated neighbor evaluate performance visual attention  hood ni previous timestep features ex measuring percentage times classiﬁable object  tracted current frame parameterizations appearing scene correctly identiﬁed count  yijt  ynijt −  yij  φij xt number falsepositives tracked frame  given logistic functions learned parameters compare attention driven method directing fovea  use bayes rule compute φijxt  yijt needed three naive approaches  dbn belief update using model ﬁxing foveal gaze center view  map estimate probability ﬁnding object  given foveal region ok computing mean ii linearly scanning scene topleft  probability pixel foveal region inter right  esting                                              iii randomly moving fovea scene    features φij used predict pixel ex                                                          classiﬁers trained image patches roughly  tracted local image patch include harris corner                                                         ×  pixels depending object class trained  features horizontal vertical edge density saturation                                                        each object class use   positive  color values duration pixel tracked                                                        training examples   negative examples set  object weighted decay number times fovea                                                        negative examples contained examples objects  previously ﬁxated region containing pixel                                                        random images extract subset fea    learning model                      tures serre et al  images learn                                                        boosted decision tree classiﬁer each object  recall deﬁne pixel interesting                                                        good performance comparable stateoftheart sys  unclassiﬁed object order generate training data                                                        tems recognizing variety objects image patch  learn parameters model ﬁrst                                                        size chosen each object achieve good classi  hand label classiﬁable objects lowresolution training                                                        ﬁcation accuracy large prevent  video sequence begins recognize                                                        classifying small objects scene  track objects pixels associated objects                                                            conduct  experiments using recorded high  longer interesting deﬁnition generate                                                        deﬁnition video streams ﬁrst assuming perfect classiﬁ  training data annotating interesting pixels                                                        cation falsepositives falsenegatives  marked interesting hand labeled training video                                                        time fovea ﬁxates object correctly classify  objects currently tracked using                                                        object second using actual trained stateofthe    envisage signiﬁcantly better estimates ex art classiﬁers addition different fovea control algo  ample using logistic regression recalibrate probabilities rithms compare method performing ob  algorithm’s performance appeared fairly insensitive choice ject classiﬁcation frames lowresolution                                                    ijcai                                                    
