                    learning subjective representations planning              dana wilkinson                   michael bowling                      ali ghodsi      school science     department computing science      school science         university waterloo             university alberta            university waterloo         waterloo canada             edmonton ab canada               waterloo canada       dwilkinsonuwaterlooca           bowlingcsualbertaca            aghodsibuwaterlooca                        abstract                          quired input sequence actions observations                                                        agent’s subjective experience semantic meaning      planning involves using model agent’s ac domainspeciﬁc required input used      tions ﬁnd sequence decisions achieve three step process      desired goal usually assumed mod section  reviews action respecting embedding      els given models require ex  technique dimensionality reduction      pert knowledge domain paper ex      speciﬁcally makes use temporal sequence observations      plores subjective representations planning actions learns manifolds capture important      learned directly agent observations ac underlying dynamics highdimensional data      tions requiring initial domain knowledge  fewer dimensions addressing ﬁrst problem sec      nonlinear embedding technique called action  tion  describes method learning operators each ac      specting embedding used construct rep tion applied point learned represen      resentation shown extract  tation addressing second problem examples learned      effects agent’s actions operators operators provided discussion se      learned representation finally learned repre mantic meaning associated actions      sentation operators combined search extracted finally section  shows results planning      ﬁnd sequences actions achieve given goals resulting representation using learned operators      efﬁcacy technique demonstrated resulting plan learned representation result      challenging robotvisioninspired image domain   applying plans original highdimensional domain                                                        compared    introduction  planning essence involves searching appropriately  action respecting embedding  deﬁned state space state space consequence highdimensional data sets sequences images  agent’s model effects actions strips fikes characterized lowdimensional representation  nilsson  markov decision processes puterman related process generating data example   assumed models deﬁned lowdimensional representation image data corre  domain experts planning uses models ﬁnd se spond degrees freedom platform moving cam  quences actions achieve given goals problem era representation ideal planning directly  models known easily built captures actions’ effects world goal  main studied methods learning                                                        temporal sequence data points     zn  agumenting models strips operators wang                                                         associated actions     an− ﬁnd lowdimensional  mdp transitions peng williams                                                         representation zi appropriate planning  techniques require expert intuition domain recently nonlinear manifold learning techniques  provide appropriate state representation used map highdimensional dataset smaller    paper focuses learning appropriate representa dimensional space semideﬁnite embedding sde wein  tion planning using agent’s observations ac berger saul  technique sde learns  tions subjective representation learned kernel matrix represents nonlinear projection  representation extracted based agent’s experi  ence requires expert knowledge domain closely resembles recent work learning predictive representa  approach solves important problems learning ap tions james singh  rosencrantz et al  jaeger  propriate statespace representation ii learning ef  previous work augmenting operators transition  fects agent’s actions representation probabilities approach speciﬁcally designed                                                        highdimensional observation spaces implicitly involves    approach learning subjective representation dimensionality reduction component   algorithm sde ·      zn            algorithm ·      zn     an−       construct neighbors using knn  ·      construct neighbors bowling et al                                                                                              maximize trk subject   ij kij       maximize trk subject   ij kij                                                                                        ∀ij  nij   ∨ nij   ⇒                          ∀ij nij   ∨ nij   ⇒                                                                                                           kii − kij  kjj  zi − zj                       kii − kij  kjj ≤ zi − zj                                                                  ∀ij ai  aj ⇒       run kernel pca learned kernel                       kii − kij  kjj                                                                     kii − kij  kjj     table  algorithm semideﬁnite embedding sde                                                            run kernel pca learned kernel    input data linear representation uses table  algorithm action respecting embedding  kernel pca scholkopf smola  generalization  principle components analysis using feature spaces rep                                                        feature subjective planning constrains learned  resented kernels extract lowdimensional rep                                                        manifold space labeled actions corre  resentation data kernel matrix learned                                                        spond distancepreserving transformations—those consist  sde solving semideﬁnite program simple set                                                        ing rotation translation  constraints important constraints encode com                                                        inputs  action inputs  mon requirement dimensionality reduction non                                                                preserve distance learned feature space letting  linear embedding preserve local distances                                                        Φz  denote input ’s representation feature space  words nearby points original input space                                                                  action a’s transformation  satisfy  main nearby resulting feature representation                   fore sde requires distance metric  ·  original input ∀i faΦzi − faΦzj   space uses metric construct knearest neighbors         Φzi − Φzj               graph adds constraints semideﬁnite program                                                         let  consider case    ensure distance neighbors preserved                                                                                          Φz   Φz    Φz   Φz    optimization maximizes trk variance                                                                            straint   learned feature representation minimize di  mensionality sde algorithm shown table        Φzi − Φzj  Φzi − Φzj     sde does account important pieces terms kernel matrix written  knowledge data temporal ordering                                                            ∀i ai  aj ⇒  vectors zi action labels ai sde  doesn’t guarantee temporallynearby input points      kii − kij  kjj   spatially nearby feature representation sde    kii − kij  kjj                        won’t necessarily result space actions sim  ple interpretation recent action respecting embedding                                                          simply adds constraint  sde’s usual constraints  algorithm bowling et al  extends sde make                                                        arrive optimization algorithm shown table   use exactly type knowledge data    formally takes set ddimensional input vectors experiments deﬁne imagebot synthetic im      zn images temporal order asso age interaction domain used experiments given  ciated discrete actions     an− action ai ex image imagine virtual robot observe small patch  ecuted input zi input zi computes image actions patch  set ddimensional output vectors     xn oneto larger image “image robot” provides excellent  correspondence input vectors provides domain subjective planning demonstrated  meaningful embedding  dimensions modiﬁes   experiments imagebot view  sde key ways exploits knowledge ing   patch   image displayed  images given temporal sequence uses knowl figure  imagebot distinct actions trans  edge build improved neighborhood graph based each lations zoom actions rotation actions al  input’s distances temporal neighbors using provided lowed translations forward  left  local distance metric second constrains embedding right  pixels zoom changes scale  respect action labels associated adjacent underlying image factor  −                                                                                                π  pairs observations ensures actions rotation rotates square left right  radians  simple interpretation resulting feature space   three distinct experimental data sets    second enhancement critical looked paper    fairly robust choice distance notice requiring actions objective space  metrics use simple euclidean distance experiments rotations translations learning nonlinear  paper                                        feature representation                                                                                                                                                                                                                                                                                                    figure  imagebot’s output are’s input data set                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      figure  path imagebot follows generate               figure  imagebot’s world                                                                                            imagebot  follows path looks like “a”      ing translation actions                                            ×  ×  ×  ×  ×  ×  ×                                                                         az  imagebot  follows pattern substituting                    zoom left actions zoom right actions            ×  ×  ×  ×  ×  ×  ×                                                                                                                                                                                            imagebot moves forth line                                                                                       ing actions                                                                 ×  ×  ×  ×  ×  ×  ×                                                                                                     note az actions half  zoomed zoomed note figure  representation learned   opposites actions used example  output imagebot correspondingly input  shown figure  images seen figure  shows “top” view “side” view   data set note know example dimensional manifold learned data set  action label  corresponds action  gets portion manifold corresponding rotating right  semantic information—it gets input images black line portion corresponding mov  labels associated                      ing forward  lightgray line point corresponding    effectiveness demonstrated previ ﬁrst image circled clear  ously bowling et al  evidence shown previous case manifold clearly distinctly captur  power capturing useful representations planning ing structure original path dimensions  figure  shows actual manifold underlying test capturing action capturing action  set imagebot’s path figure  shows manifold figure  original domain actions  learned test set clearly structure distancepreserving structure ex  captured                                        tracted figure  immediately obvious man         figure  different views threedimensional cylindrical manifold learned data set    ifold learned makes resulting Λ matrix lagrangian multipliers tr stands  impressive—not just representation appropriate trace matrix  planning aid intuitive understanding origi                                                              laa ba Λ   nal underlying structure                                                                                                                                  trya ya  trxa aa aaxa  nba ba                                                                                          distance preserving operators                                 −trya aaxa − treba ya                                                                                         learns representation explicit constraints   treba aaxa  trΛaa aa −  actions correspond distancepreserving transformations derivative lagrangian function  representation plan needs spect unknowns set zero  discover transformations each unique action                                                                ∂l                       collection data point pairs xt xt        aaxa  xa − yaxa    connected action way thinking ∂aa                                                                                           function fa faxt  xt            bae  xa  aaΛ  Λ          function needs learned each action  ∂l  distancepreserving constraints fa represented            nba − yae  aaxae                                                                         ∂ba               faxt  aaxt  ba  xt                translation vector equation  gives                                                                             ya − aaxae    recall transformations form encode trans       ba                                 lation ba vector rotation scaling aa ma                                                                                                trix aa ba learned using simple linear regres multiplying equation  aa  right  sion scaling distance preserving ad                                                                                          aaxa  xaaa   aaΛ  Λ  aa    ditional constraint aa does scale aa aa                                                                                       turns similar extended orthonormal pro        yaxa aa  − bae xa aa  crustes problem schoenemann carroll  left hand side symmetric right hand side  allowing global scaling constant solution symmetric substituting equation  right hand  regression problem derived                 side written    let xa matrix columns xt                                                                                                 ee                ee        let ya matrix −                                                                                                      columns xt goal learn  rotation matrix aa translation ba maps xa term symmetric rest expres  ya formally following optimization problem needs sion symmetric simpliﬁed  solved                                                                                                                                                  ee                                                                           ya  −      xa   aa                          minimize  aaxa  bae  − ya                                                                      subject aa aa                         symmetric equivalent transpose                                                                                            t  column vector ones                         ee                      ee                                                              ya  −      xa   aa   aa  ya  −      xa         order obtain squares estimation aa                                 ba write lagrangian function                  easily verify following satisﬁes equation    la    Λ                                                                                                                                                              ee                                                              sw       svd  ya  −      xa       traaxa  bae  − ya aaxa   bae  − ya                                                                                                                           trΛaa aa −                                             aa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        figure  demonstrating distancepreserving operators figure  demonstrating distancepreserving operators  representation learned                                                         representation learned az     svd· singular value decomposition                                                        action labels functions learned each           sw  st                unique action explicitly resulting state                                                    action applied state learning function     orthonormal   orignal space intractable require  diagonal equations   solution application knowledge speciﬁc imagebot  dual function  minΛ laa ba Λ solu underlying image order attain success  tion feasible regards primal problem strong given images ﬁnd shortest path  duality holds original problem nonconvex tween traverses unobserved parts space  results figure  shows twodimensional representa ﬁnd corresponding points lowdimensional  tion generated  solid arrows show representation ﬁnd shortest path  path consists new points resulting applica ing traditional search methods set learned opera  tion operators learned each action  tors each operator lowdimensional space corre  described clearly operators intuitively captur sponds action label orginal space list action  ing essence actions used generate data labels indicate desired path returned    note semantic meaning given following results iterativedeepening depthﬁrst search  input actions meaning derived used path ﬁnal point closest desired  easily tested pair actions opposites goal returned quality path demonstrated  actions tested starting imagebot initial state applying sequence  thogonality independence learned actions showing resulting image  representation captures underlying structure results each three test sets subﬁgures  data learned operators maintain structure shown ﬁrst shows representation learned  relationships successfully hypothesized data set shortest path chosen initial state    figure  similar figure  underlying rep labelled triangle pointing right chosen goal  resentation az data set note state labelled triangle pointing left second ﬁgure  actions opposite each actions contains images left image shows image  orthoganal—the actions independent initial state right image contains highlighted boxes  actions critical facet original data set lightgray dotted box shows image goal state  successfully captured representation learned darkergray solid box highlights image obtained  consequently action functions learned executing resulting sequence actions  manifold note particular zoomed figures show results —the goal      ×                                                                                    way        actions equivalent  actions state image image corresponding ﬁnal state                                 zoomed way action zoomed   path figure note shortest  half way observed learned operators cap path successfully involves moving                                ture fact   actions scale through portions space seen  equivalent   actions scales                                                          figures show results az —the goal                                                        state image image corresponding ﬁnal state    planning                                           path figure note path  lowdimensional representations operators successfully identiﬁes action occur action  representations learned pieces o—if taken action jump  place perform planning points learned represen sired end state state halfway state  tation states operators learned section  deﬁne figures show results r—the goal  transitions states new domain ad state image image corresponding ﬁnal state  vantages original data set dimensionality path figure close each recall  reduced drastically    second data set unlike actions
