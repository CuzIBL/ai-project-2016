    novel approach model generation heterogeneous data classification                                          rong jin huan liu†      dept science engineering michigan state university east lansing mi                                          rongjincsemsuedu    † department science engineering arizona state university tempe az                                           hliuasuedu                        abstract                        widely used approach constructing ensemble                                                    models sample different subsets training data       ensemble methods bagging boosting                                                    create classification model each subset bagging       successfully applied classification briemann  adaboost schapire singer       problems important issues associated  representative methods category bag      ensemble approach generate models                                                    ging randomly draws samples training data       construct ensemble combine replacement adaboost samples training data according       classification paper focus dynamically changed distribution updated       problem model generation heterogeneous                                                    putting weight misclassified examples       data classification partition heteroge smaller weights correctly classified examples       neous data number homogeneous parti clearly methods treat homogeneous data       tions likely generate reliable accurate                                                    heterogeneous data differently        classification models homogeneous parti ensemble methods work effectively heteroge      tions examine different ways forming ho neous data intuitive solution divide het      mogeneous subsets propose novel method                                                    erogeneous data set homogeneous partitions       allows data point assigned multiple create model each partition data member       times order generate homogeneous partitions classifiers built different homogeneous partitions       ensemble learning present details                                                    likely result good diversity ensemble way       new algorithm empirical studies uci realize homogeneitybased partition employ stan      benchmark datasets datasets image classifi dard clustering algorithms kmeans hartigan       cation show proposed approach ef                                                   wong  em clustering algorithm celeux       fective heterogeneous data classification govaert  example gaussian mixture model                                                    gmm general problems     introduction                                  simple clustering approach      ensemble approaches bagging boosting • single cluster membership clustering algo  successfully applied classification problems rithms assume cluster membership mutually exclu  dietterich  bauer kohavi  basic idea sive each data point belong single cluster   ensemble methods construct number classifiers em clustering algorithm allows soft mem  training data classify new data points tak bership data point resulting clusters each data   ing weighted vote predictions impor point belongs single cluster witten   tant issues associated ensemble approach  frank  use clustering algo  generate accurate diverse classification models rithms partition data number clusters large    combine models ensemble classifica subsets training data formed clustering algorithm   tion diverse classifiers ensure good ensembles quinlan mutually disjoint clusters small    paper focus issue em number data points lead unreliable classifi  phasis heterogeneous data classification heterogeneous cation models similar data fragmentation   data classification refers problem input data problem occurred decision tree induction quinlan   single class widely distributed multiple modes  contrast subsets training data produced   arises training data collected different envi bagging adaboost mutually disjoint exam  ronments through different sources example het ple bootstrap sampling each subset contains   erogeneous data classification image classification  original training data    labeled images acquired multiple resources • unbalanced cluster sizes clustering al  exhibit disparate characteristics instance gorithms control cluster sizes unbalanced   images black white colorful  cluster sizes resulting clustering easily cor rected resulting clusters different sizes pling training examples important methods group    classifier built small cluster unreliable include bagging brieman  adaboost schapire    degrade performance ensemble forming singer  methods    final ensemble classification contrary bagging shown effective classification   adaboost data samples similar sizes signed account characteristics heterogeneous    learning different models note previ data paper propose hiss –an algorithm    ous efforts balancing sizes different clusters par constructs homogeneous strata heterogeneous data    ticularly spectral clustering algorithms normal maintains nice property boostrap sampling pro   ized cut algorithm melta  shi  cedure  each stratum contains similar number data    trol cluster size comes indirectly objective func points    tion resulting clusters unbalanced sizes  line research closely related work     sum clustering approach produce homogeneous study clustering algorithms general clustering algo  data partitions ensure similar sizes different rithms categorized parametric approaches   partitions methods like bagging produce equally sized nonparametric approaches parametric approach   partitions partitions homogeneous parametric model minimizes cost function asso  need novel approach partitioning data homoge ciated instancecluster assignments methods  neous subsets similar sizes ensemble learning het clude mixture model celeux govaert    erogeneous data classification                  kmeans algorithm nonparametric approaches     goal work divide heterogeneous data cost function minimized merging separate   homogeneous subsets similar sizes order generate clusters larger dividing cluster   reliable accurate classification models focusing smaller ones representative examples category   homogeneous subsets require each data point agglomerative approach divisive approach    belong subset ensuring similar sizes data sub clustering approaches assume each data point   sets each classification model built similar belongs single cluster assumption   number data points paper propose hiss appropriate ultimate goal clustering group   homogeneous data similar size algorithm specially similar data points uncertain assign   designed purposes heterogeneous data data point single cluster better assigning   classification specifically hiss allows user specify multiple clusters traditional probabilistic   size subset example user ask algo model fuzzy clustering algorithm allow multi   rithm create  subsets each containing  softmemberships uncertainty cluster membership   original data algorithm similar bootstrap sam exploited during process estimation  pling procedure number subsets sulting clusters each data point assigned single   percentage training data covered each cluster cluster furthermore clustering algorithms   specified varied differs simple control size clusters resulting   bootstrap sampling procedure puts similar data clusters unbalanced size clusters   points single subset bootstrap sampling ran small sizes useless learning    domly selects data form subset property impor  tant ensemble learning classifying heterogeneous data   hiss algorithm model generation    use strata homogeneous data partitions   subsets data partitions resulting random sampling   probabilistic clustering hiss     related work                                    traditional probabilistic clustering                                                    algorithm introduce algorithm hiss    previous studies create general idea probabilistic clustering    ensemble models methods constructing en data mixture generative models optimal parame   semble models categorized groups diet ters usually obtained maximizing likelihood    terich   bayesian methods creates en data using mixture model let number input    semble model sampling estimated poste                                                   data points number clusters xx      rior model distribution  sampling training examples                               creates multiple subsets training examples input data mm   mk  underlying models    trains classifier each subsets  sampling input generate data assuming each data point    features creates number subsets input generated mixture modelsmm       features classifier built each subset input fea                                                                       likelihood data written    tures   error correct output code ecoc convert   multiple class problem set binary class problems    nk                                                             kj   injecting randomness generates ensembles classi lmij τ  ∑∑ logτ pxm      fiers injecting randomness learning algorithm       ij    categories work closely related   second creates multiple classifiers sam pxij  likelihood generating xi ing reliable accurate ensemble heterogeneous data                                                  setting γ  reasonably large value     model  τ  likelihood data point xi  work ensure each stratum sufficiently large    jth cluster based assumption each data number examples building statistical learning    point belong single cluster constraint model later reference refer new clustering ap                                               proach “hiss” stands homogeneous data        τ   example probabilistic clustering    ∑                                         similar size     gaussian mixture model gmm τ                                                    optimization hiss    px  parameterized       ij                                           putting equations                                      τθij                                          nk                                                               kj                               −                    maxlm ij  τ  logτ pxm                                                          τ        ∑∑                      xxijjij−Σ−µµ            ii           ij   pxij −      exp            π  Σ                          subject                                                                                                                                                   θ  denotes prior jth cluster µ  τγi ≤≤  γ  jk                                                      ∑                                                             Σ  mean variance matrix jth cluster                                                        ≤≤τ   injk         respectively expectation maximization algorithm em    dempster et al  used search optimal let assume gaussian distribution pxij     parameters                                                     px mj  µ jj σ  following idea em algo                                removing constraint τ   allow each                            ∑                 rithm difference likelihood data    data point belong multiple homogeneous clusters consecutive iterations bound    short strata optimization problem       kk                                                       lmtii −  ττ   lmt  ii                          nk                                                             kj                                                     τ        maxlm ij  τ  logτ pxm           nkj                  τ       ∑∑                               ≥∑∑υ      logt                       ii           ij                             iji                                                                             τi                    subject                                                                                                                                                        nkj          pxij        ≤≤τ   injk          ∑∑ijυi logt                                                                      pxij              τ  constrained   maintain                                                    υ  defined    probability interpretation easy opti   mal solution set τ   means each  τ tpx                                                                   iij                                                            υi                           data point included stratum                           τ tpx                                                                  ∑ iik    avoid trivial solution τ  choose enforce    percentage training data covered each optimal solutions mean variance    cluster predefined constantγ     gaussian distribution obtained follows                                                               nnjj                                                               υυiitx           ii  tx                                                       ∑∑ii             τγ≤≤  γ  jk         µσjjtt −nn              µ          ∑                                              υυjjtt                                                                         ∑∑iiii  constraint guarantee number                                                                     optimal solution τ  difficult   data points support each stratum γ                                        compared single membership constraint new obtain inequality constraints ≤ τ ≤      constraint following advantages  does directly optimizing equation  equality    assume each data point belong stratum                                                                               constraint result following solution τ      new stratifying method average each data point    belong γ  number strata γ                                                                               υi tnγ  larger each data point allowed    τ                                                                                            υ  stratum simultaneously  ensures differ         ∑   ent strata balanced numbers data points  trast clustering algorithms new algorithm en apparently solution nonnegative                                                        sures size each stratum particu  τ  nonnegative does guarantee    larly important research goal paper  generat                                                   τ   greater                                             finding optimal τ               data set   examples  class   features                                                         ecoli                          inputs υ    jk                                                         pendigit                                 outputs τ   maximizes equation     glass        initialization                                      yeast                                                                          vehicle                             τ    jk                                                         imageindoor                    each cluster                               imageoutdoor                                                                   table  description datasets experi            examples                       ment heterogeneous data classification              set τ  τ                                                 convert classification problem multiple classes             compute probability mass           set binary class problems representative examples                                                  include oneagainstall approach error correct output                 sni−γτi                                                        coding ecoc method dietterich  during            recompute                              process multiple classes grouped subsets                             jjυi                                   classes data points subset classes used        ττiits              ∀ st    positive examples remaining used negative                        υ                   ∑                             examples positive negative pools                 itτ                                                    comprised examples multiple classes cre                        ∃jt st τi                   ate data heterogeneity each binary classes     end                                              discussed intuitive solution classifying hetero                                                   geneous data create set classification models       figure  algorithm finding optimal τ   each classifier built homogeneous partition stratum                                                  data combine classifiers final predic                                              order satisfy inequality constraints ≤ τ ≤   tion  traditional clustering algorithms designed    use kkt conditions fletcher  efficiently task potential unbalanced cluster                                                 sizes data fragmentation problem pro   adjust value τ   basic idea reset τ                                                 posed algorithm hiss avoid problems     output equation  violates setting parameter large  experiment                   constraint ≤≤τ   adjustment sum classify heterogeneous data apply                                                    hiss obtain homogeneous strata create classi  compute τ    using equation                                                   fication model each stratum form ensemble                                          procedure adjusting recomputing τ   refer model generation method ‘hissbased                                                  model generation’ empirical study finally   continue τ   violates constraint figure                                                   stacking approach wolpert  used combine mod  shows detailed steps finding optimal solution els generated hissbased model generation       τ   space limit proof optimality method final prediction ensemble    algorithm figure  provided                                                      experimental study     classifying heterogeneous data                                                    experimental study designed answer following     classification problems heterogeneous data questions   applications experiments    proposed model generation method effective     data acquired multiple sources cases classifying heterogeneous data end compare   training data acquired multiple sources proposed model generation method bagging   each source data distribution differ adaboost classifying heterogeneous datasets    ent data merged multiple sources  proposed hiss algorithm effective generating   heterogeneous example consider building reliable models address question apply   classification model outdoor scenes training images proposed hiss algorithm probabilistic   collected different types videos clustering algorithm partition training data build   videos news stories adver classification model each partition    tisement high quality   widely disparate characteristics  experimental design   videos cause merged data heterogeneous seven different datasets used experiments     data converting multiple class problem set multiple class datasets uci machine learning  binary class problems order apply binary class pository blake merz  binary class data  classification algorithm multiple class case need                               adaboost     bagging     hissbased   bagging     adaboost          data set     baseline                                standard  standard    ensemble    stacking   stacking      ecoli                              pendigit                           glass                              yeast                              vehicle                            imageindoor                        imageoutdoor                        table  classification errors baseline model svm adaboost bagging propose model generation      method ‘hissbased ensemble’ column ‘bagging stacking’ refers case ensemble mod     els created bagging algorithm combined through stacking approach using svm      column ‘adaboost stacking’  variance classification error listed parenthesis    sets image classification characteristics vector machine proposed hissbased ensemble learn   seven datasets listed table            ing approach standard bagging standard adaboost       multiple class datasets introduce hetero baseline model performs    geneity data converting original multiple comparing standard bagging adaboost    class problem binary similar oneagainst observation indicates seven heterogeneous data   approach examples popular class used sets difficult standard ensemble ap   positive instances examples remaining proaches learn contrast proposed hissbased en   classes assigned negative class data semble method performs better baseline model    negative class multiple classes ex standard ensemble methods datasets    pect degree heterogeneity inside negative class ‘glass’ ‘vehicle’ ‘imageoutdoor’ improvement    datasets image classification substantial   ‘galss’     binary classification problems heterogeneity data  ‘vehicle’   ‘im   fact images seven different video ageoutdoor’     clips each video clip provides  images each hissbased ensemble method uses stacking    video clip different type varied quality im approach combining different models different    ages expect certain heterogeneity combination method used adaboost bag   data                                ging address difference conduct experi    baseline algorithm used experiment support ments apply stacking method combine models    vector machine burger  experiments each generated bagging adaboost results   ensemble method generates  different svms stacking listed table  right side hissbased ap  approach wolpert  uses svm employed proach titled ‘bagging stacking’ ‘adaboost    combine outputs  models form final stacking’ respectively compared results   prediction ensemble each experiment ran sults ‘bagging standard’ ‘adaboost standard’    domly select  data training remaining substantial change classification     testing experiment repeated  times errors using stacking approach combine models    average classification error runs used ensemble learning seven datasets ensemble    final result variance classification errors  models generated hiss performs best reason                                                    stacking approach useful hissbased model      heterogeneous data classification          generation method models    table  shows classification errors baseline support generated hissbased algorithm di                                                   verse ones generated bagging     data set     hiss       em         em          adaboost result applying layer classifica                          clusters  clusters tion model combine outputs distinguishable   ecoli              models stacking able advantage   pendigit           models obtain best performance   glass              based discussion conclude   yeast               hissbased ensemble model effective classify  vehicle             ing heterogeneous data existing ensemble approaches   imageindoor          imageoutdoor           comparison clusteringbased en  table  classification error using different cluster semble methods   ing algorithms model generation ‘em’ refers advantage hiss versus traditional clustering al  ing expectationmaximization algorithm cluster data  gorithms hiss allows each data point multi                                                   ple different strata ensure number 
