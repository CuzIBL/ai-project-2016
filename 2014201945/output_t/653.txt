                     phase transitions grammatical inference                            nicolas pernot antoine cornuejols´    michele  sebag                       laboratoire recherche en informatique cnrs umr                                   batˆ universite´ parissud orsay                                         orsay cedex france                                            antoinesebaglrifr                        abstract                          phase transition framework pt hogg et al                                                         considers satisﬁability resolution complexity      wellknown feasibility induc csp instances random variables depending order pa      tive learning ruled statistical properties link rameters problem instance constraint density      ing empirical risk minimization principle tightness framework unveiled interesting structure      “capacity” hypothesis space dis  csp landscape speciﬁcally landscape divided      covery years ago phase transition phe three regions yes region corresponding      nomenon inductive logic programming proves     constrained problems satisﬁability probability      fundamental characteristics learn close  average complexity low region      ing problems similarly affect possi corresponding overconstrained problems satis      bility learning general conditions ﬁability probability close  average complexity      work examines case grammatical     low narrow region separating yes      ference show phase     regions referred phase transition region sat      transition considering hypothesis  isﬁability probability abruptly drops        space severe “gap” phe     concentrates average computationally heaviest csp      nomenon affecting effective search space   stances      standard grammatical induction algorithms   phase transition paradigm transported      terministic ﬁnite automata dfa focusing lational machine learning inductive logic programming      search heuristics rpni redblue algo  ilp giordana saitta  motivated fact      rithms show overcome problem   covering test used ilp muggleton raedt      extent subject  equivalent csp anticipated phase transi      generalization paper suggests direc tion phenomenon appears framework ilp wide      tions new generalization operators suited yes respectively region includes hypotheses      phase transition phenomenon                      cover resp reject examples hypotheses                                                        discriminate examples lie narrow pt av                                                        erage computational complexity covering test reaches    introduction                                       maximum  wellknown feasibility inductive learn computational complexity pt phenomenon  ing ruled statistical properties linking empirical risk farreaching effects success relational learning  minimization principle “capacity” hypothesis botta et al  instance wide failure region ob  space vapnik  powerful framework leads served target conceptstraining sets region  deeper understanding machine learning learning algorithms prominent ilp ones ﬁnd  theoretical applicative breakthroughs basically hypotheses better random guessing botta et al   involves statistical information learning search negative results lead better understanding  space socalled vcdimension dynamics intrinsic limits existing ilp algorithms search bi  learning search considered                    ases formally consider greedy specialization topdown    independently new combinatoric paradigm search strategy starting exploration yes region  studied constraint satisfaction community bound make random specialization  early motivated computational complexity concerns choices hypotheses region cover exam  cheeseman et al  really hard prob ple average yes region constitutes rugged plateau  lems worst case complexity analysis poorly search perspective little chance  accounts fact despite exponential worstcase algorithm ends right pt region good  complexity empirically complexity low csp hypotheses lie similar reasoning goes algorithms  instances remarks led developing socalled follow greedy generalization strategy  phase transition paradigm provides chomsky sufﬁciently rich express inter  spective pitfalls facing machine learning focusing esting sequential structures identiﬁcation limit  combinatoric search aspects statistical learning fo positive examples known impossible  cuses statistical aspects                     feasible complete set examples gold     main question studied paper pt known regular language produced  phenomenon limited relational learning threatens ﬁnitestate automaton fsa fsa generates  feasibility tractability learning settings regular language remaining paper    learning setting intermediate complexity use terminology ﬁnitestate automata fsa  relational learning propositional learning tuple  hΣ δi Σ ﬁnite alphabet  considered grammatical inference gi pitt  ﬁnite set states ⊆ set initial states  sakakibara  case finitestate automata ⊆ set ﬁnal states δ transition function  fsa section  considered through paper deﬁned × Σ  speciﬁcally phase transition phenomenon inves positive example fsa string Σ produced  tigated respect three distributions fsa space following path graph linking initial state  incorporating gradually increasing knowledge syntac accepting state  tical search biases gi algorithms               ﬁnite stateautomaton fsa deterministic dfa    ﬁrst distribution incorporates information al contains exactly element ∀q ∈ ∀x ∈  gorithm considers space fsa using set Σ cardδq ≤  nondeterministic  order parameters average coverage automata studied nfa nfa translated equivalent dfa  analytically empirically                         price possibly exponentially complex    second reﬂects bias introduced gener terms number states given fsa a’ exists  alization relations deﬁned fsa space exploited minimum state dfa called canonical dfa  gi algorithms vast majority algorithms ﬁrst la  la la denotes set strings  construct general generalization positive exam accepted loss generality assumed  ples preﬁx tree acceptor pta restrict search target automaton learned canonical dfa  generalizations pta generalization cone set said structurally complete respect    distribution takes account heuristics used dfa covers each transition uses  gi algorithms guiding search trajectory general element set ﬁnal states accepting state  ization cone space limitations study restricted clearly lp   prominent gi algorithms rpni oncina given fsa partition π set states  garcia  redblue lang et al        quotient automaton obtained merging states    paper organized follows section  brieﬂy belong block partition π dupont  troduces domain grammatical inference princi et al  details note quotient automa  ples inference algorithms deﬁnes order param ton dfa nfa vice versa set  eters used rest paper section  investigates quotient automata obtained systematically merging  existence potential implications phase transition phe states dfa represents lattice fsas lattice  nomenons fsa space section  ordered grammar cover relation  transitive    generalization cone section  section  focuses closure  denoted  say aπi  aπj iff  actual landscape explored gi algorithms speciﬁcally                                                                                                   laπi  ⊆ laπj  given canonical dfa set  sidering search trajectories rpni redblue sec structurally complete respect lattice  tion  discusses scope presented study lays rived guaranteed contain  perspectives future research                   assumptions follows paradigmatic ap                                                        proach grammatical inference algorithms    grammatical inference                              coste  dupont et al  pitt  sakakibara                                                         equates generalization state merging op  introducing general notations deﬁnitions sec erations starting pta  tion brieﬂy discusses state art introduces  order parameters used rest paper         learning biases grammatical inference    notations deﬁnitions                         core task gi algorithms select iteratively  grammatical inference concerned inferring grammars pair states merged differences al  positive possibly negative examples reg gorithms related choice search criterion  ular grammars considered paper form merge best ii search strategy  class hierarchy formal grammars deﬁned search space explored iii stopping criterion                                                          shall consider setting learning fsas    precisely preﬁx tree acceptor obtained merging positive negative examples algorithms  states share preﬁx maximal canonical au studied section  setting stopping criterion  tomaton mca represents positive learning set determined negative examples generalization pro  automaton pta dfa treelike structure ceeds long candidate solutions remain correct notcovering negative example                          • state output edges cre    rpni algorithm oncina garcia  uses     ated uniformly selected replacement  depth ﬁrst search strategy backtracking ability fa states ii × distinct letters uni  voring pair states closest start state formly selected Σ iii letters evenly  generalization fsa obtained merging distributed edges  states subsequently applying determinisation op • state turned accepting state prob  erator does cover negative example              ability    redblue  algorithm known bluefringe  lang et al  uses beam search candidate list sampling mechanism nfa differs  selecting pair states evidencedriven state single respect edges origin state  merging edsm criterion generalization required carry distinct letters  involves minimal number ﬁnal states redblue each setting order parameters  independent  performs search limited backtracking based problem instances constructed each considered fsa  complex criterion wider search width rpni sampling mechanisms detailed cover                                                        age rate measured percentage covered examples    order parameters                                  examples strings length  uniformly sam  following methodology introduced giordana pled                                                                                              saitta  pt phenomenon investigated fig  shows average coverage plane                                                        Σ                                called order parameters chosen accordance pa                accepting rate varies                                                                                              rameters used abbaddingo challenge lang et al   branching factor varies   each                                                                                                                                                       point reports average coverage sample string                                                        fsa averaged  fsa drawn accepting rate    • number states dfa                branching factor tested   strings length     • number output edges each state         empirical results analytically explained                                                        simple equations giving probability string    • number letters each edge             length  accepted fsa deﬁned alphabet size    • fraction accepting states taken  Σ branching factor letters each edge    • size Σ alphabet considered          dfa nfa cases number states irrelevant                                                           • length  test examples maximal      length  learning examples explained                                                                             b·l                                                                   ·  Σ         dfa                                                            accept                    study ﬁrst focuses intrinsic properties             ·  −  − Σ   nfa  search space section  using random sampling strategy   letters string independently uniformly coverage fsa decreases decrease  drawn Σ section  examine capacity slope abrupt dfa case nfa  studied learning algorithms approximate target automa case clearly phase transition  ton based positive sampling each training string  produced following path graph randomly select  ing output edge each step      phase transitions fsa space     generalization cone  section investigates percentage coverage deter  ministic nondeterministic finitestate automata  uniformly selected section  selected subspace  actually investigated grammatical inference algorithms  generalization cone section                                                         figure  coverage landscapes deterministic non    phase transition fsa space          deterministic fsa Σ  density  sampling mechanism deterministic fsa accepting states branching factor respectively  space dfa deﬁned follows given order param vary      eter values Σ           paper standard machine learning terminology  pt generalization cone  string said covered fsa iff belongs language  thereof                                              coverage landscape displayed fig  suggest    string cut accepting state met arriving grammatical inference takes place wellbehaved  length  rejected        search space grammatical inference algorithms donot explore fsa space stated section   search restricted generalization cone set  generalizations pta formed set  positive examples step consider search  space actually explored gi algorithms    new sampling mechanism deﬁned explore dfa  generalization cone      experiments examples length       uniformly independently sampled      space strings length   corresponding      pta constructed       experiments ptas constructed      way                                                        figure  coverage landscape dfa generalization cone      experiments generalization paths lead                                                        Σ      far right stand  pta sam      ing each pta general fsa universal                                                        pled circa  states each generalization cone      acceptor ua constructed                                                        each pta includes  generalization paths leading      each generalization path                                                           pta universal acceptor each point reports      ua ith fsa constructed merg                                        i−           coverage dfa evaluated sample  strings      ing uniformly selected states  subse                                       i−              graph shows existence large gap regarding      quently applying determinisation operator                                                        number states coverage dfas    generalization cone sample fsas reached generalization      generalization paths circa  fsas      experiments    sampling mechanism nondeterministic gener  alisation cone differs single respect  determinisation operator applied    fig  shows behaviour coverage dfa      generalisation cone Σ      each dfa                                                                                      depicted point coordinates                                                                                                                                                         number states coverage measured                                                                                   section  coverage rate each fsa sample                                                                                 evaluated coverage rate  test strings length                                                                                                                                                   fig  similarly shows behaviour coverage  nfa generalisation cone Σ                               number states    fig  typical experimental results range ob  servation Σ              shows  clearcut phase transition speciﬁcally coverage figure  coverage landscape nfa generalization  abruptly jumps circa   jump cone order parameters ﬁg   incides gap number states dfas  generalization cone dfa number states    gap dramatic  length training test sequences  increased large interval typically     interestingly smoother picture appears non approximately  falling abruptly random  deterministic case ﬁg  coverage rapidly exploration generalization cone face severe difﬁ  creases number states decreases   culties ﬁnding hypothesis region likely  gap seen number states return hypotheses poor performance target concept  coverage rate                                coverage rate “no man’s land” interval    following focus induction dfas  consequently utmost importance examine                                                        search heuristics used classical grammatical    phase transition search trajectories           inference systems able thwart pri  coverage landscape dfas shows hole ori low density hypotheses gap second  generalization cone density hypotheses coverage able guide search hypotheses appropri                                                        ate coverage rate specially coverage falls gap    difference dfa case determinisation  process forces states merging needed diffusion study focus standard algorithms  like analytical model devised predicts observed start grammatical inference rpni redblue  gap  precision                           algorithms oncina garcia  lang et al                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       experimental setting                                                                      previous experiments considered training sets pos          itive randomly drawn strings sequences          order assess performance learning algorithms                                                                    hypothesis learned compared target au                                                                    tomaton experimental setting used                                                                    section sampling target automata                                                                    construction training test sets data sets include                                                                    positive negative examples gi algorithms                                                                    speciﬁcally rpni redblue use negative examples  order stop generalization process                                                                                             ﬁrst experiments tested heuristically                                                                                                number states    guided inference algorithms ﬁnd good approximations                                                                                                                                                                              target automata considering target automata approx                                                          imately  coverage rate considered inﬂuential                                                        figurer  three rpni learning trajectories target concept                                                              abbadingo challenge middle “gap”                                                        ofe coverage extremity outlined oval  ii  coverage rate                                left doted horizontal line corresponds cover    each target coverage rate used experimental age target concept cloud points corresponds  setting described lang et al  order retain cer random trajectories  tain number target automata mean size states    experiments each automaton  generated  training sets size   labeled  according target automaton equal number        positive negative instances  s−   length         coverage rate computed           uniformly drawn strings intersection train     ing set                                                           second set experiments analyzed learning        performances algorithms respect test errors       false positive false negative                             experiments chose type target automata      setting number states predetermined                                                                           structural properties                                                                                                                                    number states    heuristically guided search space  space limitation graph obtained rpni figure  ﬁgure  coverage  algorithm reported ﬁgure  three typical learn target concept   ing trajectories similar results obtained red  blue algorithm    immediate result rpni edsm  generalization error  heuristics manage densely probe “gap” ex                                                        table  obtained different sizes target automata  plain gap phenomenon discovered                                                        training sets structural completeness   redblue algorithm instance solve                                                        conﬁrms rpni redblue return overgener  cases abbadingo challenge target                                                        alized hypotheses hand average coverage  cepts coverage rate approximately                                                         vastly greater coverage target automata  rpni tends overspecialize target automa                                                        hand tend cover positive test  ton redblue tends overgeneralize                                                          instances cover large proportion nega    order test capacity algorithms return au tive test instances shows heuristics used  tomata coverage rate close target coverage                                                        rpni redblue  inadequate target concepts  repeated experiments target automata coverage low coverage  rate approximately  results ﬁgure  shows  case rpni ends automata coverage    times greater target coverage effect  conclusion  pronounced redblue returns automata  research extended phase transitionbased  average coverage rate                   methodology botta et al  grammatical infer                                                        ence framework ample empirical evidence shows    datasets available httpwww search landscape presents signiﬁcant differences depending  lrifr∼antoinewwwptgi                            search operators considered
