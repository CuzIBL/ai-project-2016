                                  stacked sequential learning                      william cohen                             vitor carvalho         center automated learning  discovery           language technologies institute                 school science                   school science                  carnegie mellon university                   carnegie mellon university                     pittsburgh pa                          pittsburgh pa                      wcohencscmuedu                              vitorcscmuedu                        abstract                          entropy learner generally outperforms conditional random                                                        ﬁelds      new sequential learning scheme      called “stacked sequential learning” stacked se      quential learning metalearning algorithm   motivation hard task memms      arbitrary base learner augmented   motivate novel learning method      make aware labels nearby exam ﬁrst analyze behavior wellknown      ples evaluate method “sequen  sequential learner particular realworld problem      tial partitioning problems” characterized cent paper carvalho cohen  evaluated num      long runs identical labels demonstrate  ber sequential learning methods problem recog      problems sequential stacking consis nizing “signature” section email message each      tently improves performance nonsequential line email message represented set hand      base learners sequential stacking im crafted features “line contains possible phone num      proves performance learners crfs     ber” “line blank” each email message repre      designed speciﬁcally sequential tasks sented vector featurevectors     xn xi      sequentially stacked maximumentropy   featurevector representation ith line mes      learner generally outperforms crfs               sage line labeled positive signa                                                        ture section negative labels message                                                        represented vector yi label    introduction                                       line  paper consider application sequential dataset contains  labeled lines  email  probabilistic learners sequential partitioning tasks se messages  lines labeled “positive” sig  quential partitioning tasks sequential classiﬁcation tasks nature sections fall end message usually  characterized long runs identical labels examples  lines experiments data split  tasks include document analysis video segmentation training set  sequencesemails test set  gene ﬁnding                                      remaining sequences used “basic” fea                                                        ture set carvalho  cohen    motivated anomalous behavior observed  sequential learning method particular partitioning task complete dataset represented set exam  derive new learning scheme called stacked sequen ples      xt yt     xm ym sequen  tial learning like boosting stacked sequential learning tial learning problem learning dataset  metalearning method arbitrary base learner sequential classiﬁer—ie function fx  augmented—in case making learner aware produces vector class labels clearly ordinary                                                        nonsequential learning algorithm used sequential  labels nearby examples sequential stacking simple                                         implement applied virtually base learner learning ignoring sequential nature data                                                                                                      imposes constant overhead training time previous paper carvalho cohen    implementation sequentially stacked version base ported results nonsequential sequential learn  learner trains seven times slowly ers signaturedetection problem including non                                                        sequential maximum entropy learner berger et al     experiments partitioning tasks sequential  stacking consistently improves performance non                                                           speciﬁcally build dataset nonsequential exam  sequential base learners surprisingly sequential stack                                                        ples xti yti use train classiﬁer maps  ing improves performance learners speciﬁcally single featurevector label use classify  designed sequential tasks conditional random each instance xi vector  hx     xni separately ignoring  ﬁelds discriminatively trained hmms finally sequential position append resulting predictions yi  set benchmark problems sequentially stacked maximum output vector         method   noise   error  min error            sages learned memm makes false positive classiﬁca                                             tion signature starts “gets           memm                                stuck” marks subsequent line signature           crf                                  behavior consistent previouslydescribed lim           memm                              itations memms known memms represent           crf                               proper subset distributions repre                                                        sented crfs lafferty et al  “label                                                        bias problem” does explain memms perform worse  table  performance sequential learners                                                        nonsequential memms clearly represent  signaturedetection problem                                                        strictly distributions                                                          klein manning  “observation  henceforth conditional random ﬁelds lafferty et bias problem” memms little weight  al  henceforth crfs plausible sequen history features case relative  tial learning method apply task maximum weights assigned crf memm  entropy markov models memms mccallum et al  weight history features verify en  called maximumentropy taggers ratnaparkhi  couraged memm downweight history features  conditional markov models klein manning   adding noise training test data each training  recurrent sliding windows dietterich  emailsequence consider each featurevector xi ∈  model conditional probability label sequence turn probability  swap line xi                                                        label   chosen uniformly  qgiven instance sequence deﬁned pryx                                                                                     sequence adding “sequence noise” dou    pryiyi− xi local model pryiyi− xi  learned follows constructs extended dataset bles error rate crfs reduces error rate  collection nonsequential examples memms course type noise does affect non  form xi yi− yi xi yi− denotes instance sequential experiment supports hypothesis  original feature vector xi augmented memm overweighting history features  adding feature yi− xi yi− extended  instance yi− history feature note yi  stacked sequential learning  class label extended example xi yi− yi    constructing extended instances trains   description  maximumentropy conditional model extended   poor results memm described intu  dataset inference using viterbi search ﬁnd itively explained mismatch data used train  best label sequence                            local models pryiyi− memm data    memms number nice properties relative used test models noisefree training data  recentlyproposed crf model memms easy case signature line followed  implement inference learning time signature lines memm’s local model tends weight  relatively quick train memms easily general history feature yi− heavily strong regu  ized replacing local model uses longer larity does hold model applied test data  “history” previous labels—ie model form course executing viterbi algorithm  pryiyi−     yi−k xi—and replacing viterbi search local model applied extended examples xi yˆi−  beam search learner scales yˆi− prediction local model  tory size number possible classes             theory course trainingtest mismatch compen    unfortunately table  shows memms perform ex  sated viterbi search turn driven  tremely badly signaturedetection problem conﬁdence estimates produced local model  error rate times error rate crfs fact assumptions theory violated instance  problem memms perform worse non  highorder interactions accounted  sequential maximumentropy learner               maximumentropy model local model’s conﬁdence esti    memm’s performance better changes mates incorrect  threshold used classify examples letting pˆi prob way correct trainingtest mismatch mod  ability pryi  xi yi− computed memm   ify extended dataset used train local model  each learner threshold θ rule yi  true previous class yi− extended instance   ⇔ ˆpi  θ gives lowest test error rate column xi yi− replaced predicted previous class yˆi−  labeled “min error” table  gives “best possible” low outline way  sult “min error” memms improved   assume given sample  xt yt size  higher nonsequential                  sequential learning algorithm previous work    high error occurs test email mes metalearning method called stacking wolpert                                                         suggests following scheme constructing sample    used implementations memms crfs pro yˆ pairs yˆ vector “predicted” classlabels  vided minorthird minorthird  limit  optimiza partition equalsized disjoint sub  tion iterations limit does substantially change results sets     sk  learn functions     fk  wherestacked sequential learning                          arbitrary sequential learner speciﬁc    parameters history size wh future size wf  cross tory feature parameterized set features  validation parameter                                 experiments introduced small important    learning algorithm given sample  xt yt sequen reﬁnement each “history feature” yˆ added extended  tial learning algorithm                            example simply predicted class numeric value                                                        indicating logodds class makes accessible    construct sample predictions yˆt each xt ∈ fol       lows                                             conﬁdences previously used viterbi search       split equalsized disjoint subsets     sk  initial results             let fj  − sj                                                         applied stacked sequential learning base       let sˆ  xt yˆt  yˆt  fj xt xt ∈ sj                                                       learner henceforth sme signaturedetection dataset    construct extended dataset instances xt yt used       sme method      verting each follows   hx                                                                obtains error rate  signaturedetection        yˆ      yˆ  yˆ ith component           i−wh     iwf                          task—less baseline method                                   ˆ      yˆt label vector paired xt        higher crfs  certain extensions dra    return functions      matically improve performance                                                          sme impact “history” features  inference algorithm given instance vector                                                        add new features extended instances like    let yˆ  fx                                     memms sme efﬁciently handle large histories    carry step  produce extended instance signaturedetection task increasing history size       using yˆ place yˆt                       duces error slightly                   return                                       sme extended instance xi include pre                                                        dicted classes previous instances “fu   table  sequential stacking metalearning algorithm ture” instances—instances follow xi sequence                                                        explored different “window sizes” sme                                                        “window size” means wh  wf      −  construct set               previous following predicted labels added each                                                    extended instance value   reduces error rates                                                          reduction crf’s error rate           sˆ  xt yˆt  yˆ  fjxt xt ∈ sj                                                        improvement statistically signiﬁcant    words sˆ pairs each xt yˆt associated finally stacked sequential learning applied  performing kfold crossvalidation intent learner—in particular extended examples se  method yˆ similar prediction produced quential applied sequential learner evalu  learned sizem sample does include ated stacked sequential crfs henceforth scrfs vary    procedure basis metalearning algorithm ing window sizes problem value    table  method begins sample se duces error rates  statistically signiﬁcant improve  quential learning method discussion ment crfs moderately large window val  assume used sequential data        ues little performance difference scrf    using crossvalidation techniques ﬁrst pairs sme  each xt ∈ vector yˆt associated performing  crossvalidation predictions used  discussion                                      create dataset extended instances  sim graphical view memms shown figure  plest case simply vectors composed instances use usual convention nodes known values  form xi yˆi− yˆi− − th label yˆ shaded each node associated maximumentropy    extended examples used train model conditional model deﬁnes probability distribution    nonsequential maximumentropy given input values  learner step similar process building “local figure presents similar graphical view classi  model” memm difference history fea ﬁer learned sequential stacking wh   wf    tures added xi derived true history xi inference model stages ﬁrst middle  approximations offsample predictions layer inferred later layer  classiﬁer                                         inferred middle layer nodes middle layer    inference time  run examples partly shaded indicate hybrid status—they  extended adding prediction features yˆ considered outputs model inputs model  “test” distribution similar “training” distribution   used inner loop viterbi beamsearch  process instead predictions yˆ produced using non twotailed paired ttest rejects   conﬁdence  sequential maximumentropy model learned null hypothesis difference error rate sme  algorithm table  simply generalizes idea crf randomly selected sequence mean zero                                         yi   yi     yi            yi     yi     yi                                                                        yi  yi  yi   yi  yi                                                                                                    yi   yi     yi                                                                                                                                                                   yi  yi  yi   yi  yi            xi     xi     xi                                             xi   xi     xi                                                                        xi  xi  xi   xi  xi               memm              sequential stacking wh   wf   sequential stacking                            figure  graphical views alternative sequentialstacking schemes      way interpret hybrid layer means mak task memm   crf   sme   scrf  ing inference robust middlelayer nodes aaigen       treated ordinary unobserved variables toplayer aainn          ditional model  rely heavily conﬁdence aaix          sessments lowerlayer model forcing  treat taigen       variables observed quantities allows  develop tainn         model yˆ predictions correlate taix            actual outputs allows  accept downweight video        f’s predictions appropriate suggested dotted video           line ﬁgure stacking conceptually creates “ﬁrewall” mailsig          insulating  possible errors conﬁ  dence                                  table  error rate comparison different sequential algo    figure shows sequential stacking model win rithms set benchmark tasks  dow wh  wf   simplify ﬁgure edges  eventually lead node yi shown                                                            conclude discussion note described vs maxent                                                            vs memm  sequential stacking increases runtime base learning vs crf                                                               xy  method approximately constant factor      note sequential stacking requires training   classi  ﬁers classiﬁers     fk used crossvalidation  ﬁnal classiﬁers  data plentiful train    ing time limited possible simply split orig  inal dataset disjoint halves train                                    classiﬁers respectively                                                         error  stacked maxent  extended predictions produced  scheme leaves training time approximately unchanged    lineartime base learner decreases training time    base learner requires superlinear time                                                                                                          experimental results                                       error learner    additional segmentation tasks                                                    figure  comparison error rates sme  evaluated nonsequential memms crfs error rates memm crfs  scrfs sequential partitioning  tasks stacking used   window size  wh  wf    problems parame  three tasks aigeneral aineuralnets aix  ter values explored section changes adopted dietterich et al  data consists   sequential stacking algorithm developed  long sequences each sequence corresponding single  based observations signaturedetection task faq document total each task contains                                                labeled lines current implementation se    set tasks involved classifying lines faq doc quential stacking supports binary labels consid  uments labels like “header” “question” “answer” ered labels “trailer” “answer” separate  “trailer” used features adopted mccallum et al tasks each faq leading total new benchmarks  set tasks video segmentation tasks task     memm          crf   sme   scrf  goal sequence video “shots” music                sequence adjacent frames taken camera   music                classify categories “anchor” “news”  “weather” dataset contains  sequences each corre table  comparison different sequential algorithms  sponding single video clip total  shots music classiﬁcation task   features produced applying lda  bin rgb color histogram central frame  shot data provided yikcheung tam tral” “sad” “very sad” pearson’s corre  mingyu chen constructed separate video partition lation walpole et al  used measure inter  ing tasks corresponding common labels annotator agreement pearson’s correlation coefﬁcient    additional tasks similar ranges − perfect disagreement  perfect agree  signaturedetection task contain long runs iden mentand interannotator agreement stu  tical labels leading strong regularities constructed dents   tory features error rates learning methods learn song classiﬁer represented each song  tasks addition previous signaturedetection sequence second long “frames” each frame la  task shown table  each case single traintest beled emotion song contains  split used evaluate error rates boldfaced entries learned sequence classiﬁers labeled sequences  lowest error rate row                   classify unknown song used sequence classiﬁer    observe memms suffer extremely high error rates label frames song ﬁnally labeled song  new tasks ﬁnding “answer” lines aigeneral frequent predicted frame label  aineuralnets suggesting “anomalous” behavior “frames” produced extracting certain numerical  shown signaturedetection uncommon properties waveform representation song  sequential partitioning tasks                     ms averaging second intervals each     comparing sme sme improves second frame features mean standard deviation  error rate   tasks leaves unchanged each property numerical properties computed                                                                                                       furthermore sme lower error rate crfs   using marsyas toolkit tzanetakis cook    times error rate based shorttime fourier transform tonality  case memms lower error rate sme cepstral coefﬁcients    overall sme preferable three music dataset contains  frames   older approaches memms crfs  songs  features each frame looked  somewhat apparent scatter plot figure  versions problem predicting ﬁve labels mu  plot each point placed yaxis position er sic predicting “happy” versus “sad” labels mu  ror sme xaxis position error earlier sic preliminary experiments suggested large windows  learner points line  cases effective used following parameters  outperforms learner readability range experiments   window size wh  wf    axis truncated—it does include highest er music problem   wh  wf    ror rates memm                                   music    stacking improves crf problems  results summarized table  scrf  effect consistent scrf improves error rate  sme outperform nonstacking counterparts sme   tasks leaves unchanged twice increases error outperforms crfs furthermore scrf improves substan  rate twice table stacked learners tially unstacked crfs twoclass problem reduc  lowest error rate   tasks            ing error rate     applying onetailed sign test improvement sme  relative statistically signiﬁcant    conclusions  improvement scrf relative crf statistically sequential partitioning tasks sequential classiﬁcation    sign test does consider amounts tasks characterized long runs identical labels exam  error rates changed ﬁg ples tasks include document analysis video segmen  ures tables clear error rates lowered tation gene ﬁnding paper evaluated  substantially raised small performance certain wellstudied sequential probabilistic  proportion “aaix” benchmark crfs     learners sequential partitioning tasks observed                                                        memms obtain extremely high error rates er    sequence classiﬁcation task                    ror analysis suggests problem “label  ﬁnal test explored additional nontrivial task bias” lafferty et al  “observation bias” klein  classifying popular songs emotion task consid manning  mismatch data used  ered code development complete train memm’s local model data  completely prospective test sequential stacking col memm’s local model tested particular memms  lection  popular songs annotated students trained “true” labels tested “predicted” labels  ﬁvepoint scale “very happy” “happy” “neu strong correlations adjacent labels associated se
