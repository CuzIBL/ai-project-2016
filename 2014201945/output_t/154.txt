fully connectionist model generator covered firstorder logic programs           sebastian bader∗     pascal hitzler†  steffen holldobler¨ ∗  andreas witzel‡         ∗ international center computational logic technische universitat¨ dresden germany                                  † aifb universitat¨ karlsruhe germany               ‡ institute logic language computation universiteit van amsterdam                          abstract                          necessary preliminaries section  make follow                                                        ing novel contributions section  deﬁne new multi      present fully connectionist   dimensional embedding semantic operators reals      learning ﬁrstorder logic programs gen construct feedforward network approximate      eration corresponding models given program  operators present new learning method using      set training examples embed asso main knowledge resulting evaluated sec      ciated semantic operator feedforward net tion  finally draw conclusions point      work train network using examples    needs future section  overview      results learning ﬁrstorder knowledge related work refer d’avila garcez et al       damaged noisy data handled gracefully bader hitzler                                                            preliminaries    motivation                                                        section preliminary notions logic program  three longstanding open research problems connection ming connectionist systems presented  ism questions instantiate power core method approach integrate paradigms  symbolic computation fully connectionist  smolensky  represent reason struc  firstorder logic programs  tured objects structure sensitive processes fodor logic program ﬁrstorder language set  pylyshyn  overcome propositional ﬁx clauses form ← ∧···∧ln atom  ation mccarthy  use connectionist sys li literals atoms negated atoms  tems symbolic learning reasoning proposi called head clause li called body lit  tional logic shown feedforward networks erals conjunction ∧···∧ln called body  universal approximators artiﬁcial neural networks clause  called fact clause ground  turing complete know symbolic computation does contain variables local variables  possible principle time mentioned variables occurring body correspond  results mainly theoretical                       ing head logic program covered clauses    concerned model generation ﬁrst contain local variables  order logic programs sets rules contain example  following covered logic program  variables ranging inﬁnite domains approach serve running example  based following ideas ﬁrst expressed holldobler¨ et  al  various semantics logic programs coincide    iseven  ﬁxed points associated semantic operators given esx ← ox  successor sx odd  semantic operator continuous reals operator                                                        ox ←¬ex       odd  approximated arbitrarily feedforward network  addition operator contraction ﬁxed point herbrand universe ul set ground terms  computed recurrent extension feedforward herbrand base bl set ground atoms  network                                              assume inﬁnite – case ﬁnite bl    approach purely theoretical reduced propositional setting ground instance  ﬁrstorder case paper show feedforward net literal clause obtained replacing variables  works approximating semantic operator given ﬁrst terms ul logic program  gp  denotes set  order logic program constructed show ground instances clauses   networks trained using inputoutput examples level mapping function assigning natural number  demonstrate obtained connectionist robust a≥ each ground atom negative ground literals  damage noise particular stating deﬁne ¬a  logic program called acyclic                                                    ijcai                                                     exists level mapping ·such clauses  core method  ←  l∧· · ·∧ln ∈gp   li  ≤ ≤ holldobler¨ kalinke  hitzler et al                                                   sn    method proposed translate propositional logic pro  example  consider program example  let      denote nfold application esn   gram neural network network set  osn   ﬁnd acyclic    tle stable state corresponding model pro                                                        gram achieve goal singlestep operator tp asso    herbrand interpretation subset bl ciated implemented using connectionist  atoms ∈ said true general approach nowadays called core method  ∈ said false il denotes set bader hitzler   interpretations interpretation herbrand model holldobler¨ et al  idea extended ﬁrst  logic program symbols  ifi model order logic programs shown tp operator  each clause gp  usual sense              acyclic programs represented continuous function                                                        real numbers exploiting universal approximation                             example  program   example    capabilities layered feedforward networks shown    esn   ∪osm                                       odd             networks approximate tp given ac    given logic program  singlestep operator tp  curacy algorithms generation net  il →il  maps interpretation set exactly works given programs presented ﬁnally  atoms clause ← body ∈gp    bader et al  preliminary fashion  body true operator tp captures  semantics herbrand models  fineblend  exactly preﬁxed points interpre section ﬁrst discuss new embedding  tations tp ⊆ logic programming purposes terpretations vectors real numbers extends  usually preferable consider ﬁxed points tp  instead approach presented holldobler¨ et al  computing  preﬁxed points intended meaning programs mdimensional vectors instead single real number  ﬁxed points called supported models program apt allowing higher scalable precision  et al  example  obviously intended model show construct connectionist approxi   supported bl model supported mating tp operator given program given    logic programming established mature paradigm accuracy ε mentioned bader et al  ﬁrst  knowledge representation reasoning lloyd algorithms presented accuracy obtainable   recent applications areas like rational agents practice limited through use single real num  semantic web technologies angele lausen  ber embedding approach presented allows                                                        arbitrarily precise approximations additionally    connectionist systems                            present novel training method tailored speciﬁc set                                                        ting presented ﬁne blend techniques  connectionist network simple computational supervised growing neural gas sgng fritzke  units accumulate real numbers inputs  approach presented bader et al   send real number output each unit’s output  nected units’ inputs certain realvalued weight  embedding  units incoming connections called input obviously need link space interpretations  units outgoing ones called output units space real vectors order feed    consider layered feedforward networks net connectionist end ﬁrst extend level  works cycles outputs units layer mappings multidimensional setting use  connected inputs units layer represent interpretations real vectors  ﬁrst layers contain input output units respec deﬁnition  mdimensional level mapping bijective                                                                                 tively intermediate layer called hidden layer function  ·   bl →  mfora ∈blif    each unit input function uses connections’   called level dimension  weights merge inputs single value respectively deﬁne  ¬a      function example socalled radial basis input deﬁnition  let ≥  let ∈bl atom                                    function w  → ixi − wi  xi   mdimensional embedding ι  bl →                                                                                    inputs wi corresponding weights possible extension ι  il → deﬁned ιa                                                output functions sigmoidal function  → e−x  ιaιma                                                                    hidden layer identity  → usually used    −l                                                                                                   output layer unit layer allowed output ι                 ιi    ιa                                                                                  value  layer implements winnertakeall behavior                       a∈i    connectionist systems successfully used learn cm denote set embedded interpretations                                                                                   ing complex functions raw data called training sam  ιi  ∈il⊂r   ples desirable properties include robustness respect                                                                                    ¯  damage noise rojas  details     ι injective   use                                                     ijcai                                                                                                                                            fp                 fqx                           ¯                                                            ¯                   ¯                                                                                                    ¯                                        ιm                                                                                                                                                                                ¯                  ¯                                                                                      ¯                                                        figure  fp program example  embed                                                      ding example  shown left piecewise  figure  left right ex                                                         stant approximation fq level  shown right                                                                                                                           construction                                                                                              section show construct connection    figure  ﬁrst steps constructing limit ist network given covered program given                                                        accuracy ε dimensionwise maximum distance                                                        dfp fn maxxjπjfp − πjfn  example  using dimensional level mapping                                                       embedded tp operator fp function fn computed  example  obtain depicted figure                                                       ε use layered network  left using dimensional level mapping  es           osn                   winnertakeall hidden layer                                   obtain                −lnb−ε  depicted right ιmb ≈         lnb   obtain level    embedding        interpretations agree atoms level                                                       readers familiar fractal geometry note dimension ﬁnd ιji − ιjj≤ε cov  classical cantor set dimensional variant ered program  construct ﬁnite subset ⊆gp   barnsley  obviously ι injective bijec ∈il tp tqi agree atoms                                       tive level mapping bijective  using level dimensions dfp fq ≤ ε fur  dimensional embedding tp operator embedded thermore ﬁnd embedding fq constant  real vectors obtain realvalued function fp  hypersquares level bader et al  obtain                                                        piecewise constant function fq dfp fq ≤ ε  deﬁnition  mdimensional embedding  tp  namely                                      −  fp  →    deﬁned fp xι tp ι x       construct feedforward network follows                                                                                                                                     each hypersquare level  add unit hid      dimensional embedding preferable den layer input weights encode position  introduced holldobler¨ et al  used bader center unit shall output  selected  et al  allows scalable approximation winner  weight associated  precision real computers  atoms                                                        connections unit value fq hyper  represented  bits                          square obtain connectionist network approximat    introduce hypersquares play im                                                        ing semantic operator tp given accuracy εto  portant role sequel going figure  determine winner given input designed locally  shows ﬁrst  steps construction big                                                     receptive activation function outcome smallest  square ﬁrst replaced shrunken copies closest “responsible” unit responsible units  result replaced smaller copies                                                     deﬁned follows given hypersquare units  limit iterative replacement  use ci                                                             positioned subhypersquares  denote result th replacement figure  default units                                          called             responsible inputs  picts       readers background inputs subhypersquares containing  fractal geometry note ﬁrst  appli units does default units units po  cations iterated function barnsley  sitioned subhypersquares responsible inputs  squares occurring intermediate results construc                                                        units’ activations lo  tions referred hypersquares sequel cally computed unit smallest value selected  notes hypersquare level squares occur        cm                                          winner  ring  approximation level                                                                                                    following example taken witzel   yield function constant hypersquares level  used convey underlying intuitions constructions  deﬁnition  largest exclusive hypersquare vector work mdimensional embeddings general clar                                              u ∈ set vectors  vvk⊆c  ity graphs result dimensional level mapping  noted hexu  does exist hyper                           u ∈    ∩  ∅        example  using program example    square  level                                                           smallest inclusive hypersquare nonempty set vectors dimensional level mapping example  obtain                                                      fq level depicted figure  corresponding   uuk⊆c  denoted hinu hyper  square greatest level ⊆           network consists  input  hidden  output units                                                      ijcai                                                       training                                                                                        section adaptation sys                                          tem during training weights structure  network changed given training samples input                                           desired output way distribution                                        lying training data better represented network                                                                                                                                                                 error  process used reﬁne network resulting                                            units    incorrect program train network scratch                                         training samples case come original non ap  proximated program observed real                                          world given experts discuss adaptation                         units fineblend                                                                                            error fineblend   weights adaptation structure adding                        units fineblend                                                                                          error fineblend                                                                                                     removing units methods used adap                        tations ideas described fritzke                  examples  tailed discussion training algorithms modiﬁcations  refer witzel                                    figure  fineblend  versus fineblend     adapting weights let x input y desired quite obvious hidden unit fails receptive area  output winnerunit hidden layer taken units speciﬁc results learned  adapt change output weights u’s receptive area lost corruption input  desired output wout ← η · y − η · wout fur weights cause changes network function  thermore center c hinx generally alter unit’s receptive area output  win ← μ · c − μ · win η μ prede weights corrupted certain inputs effected  ﬁned learning rates note winner unit moved damage occurs during training  input center smallest hyper paired quickly indicated experiment reported  square including unit input intention section  noise generally handled gracefully  units positioned center hypersquare wrong unnecessary adjustments reﬁnements  responsible                       training process    adding new units adjustment described enables  evaluation  certain kind expansion network allowing units  positions responsible larger ar section discuss preliminary experiments  eas input space reﬁnement care diagrams use logarithmic scale error axis  densifying network areas great error caused error values relative ε value  desig  unit selected reﬁnement try nates absolute error ε incorrect network initializa  ﬁgure area responsible suitable position tion used following wrong program  add new unit    occupies hypersquare largest            esx ←¬ox  hypersquare considered u’s responsibility area         ox ←  ex  smallest hypersquare containing  moved center area informa training samples created randomly using semantic  tion gathered used determine subhypersquare operator program example   center new unit placed set  weights new unit                           variants fine blend                                                        illustrate effects varying parameters use  removing inutile units each unit maintains utility value setups softer utility criteria fineblend   initially set  decreases time increases stricter ones fineblend  figure  shows start                                            unit contributes network’s output unit’s ing incorrect initialization decreases  utility drops threshold unit removed initial error paying increasing number units    robustness                                       signiﬁcantly decreases number units paying                                                        increasing error performance net  described able handle noisy data cope work critically depends choice parameters  damage effects damage optimal parameters obviously depend concrete setting    error given sample ascribed winner unit kind noise present training data  predeﬁned number training cycles unit greatest methods ﬁnding investigated fu  accumulated error reﬁned error exceeds given threshold ture experiments use fineblend     contribution unit expected increase error parameters resulted mixture intuition  unit removed fritzke                 nonexhaustive comparative simulations                                                    ijcai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            error                                                  error                                                 units                                                 units                                                                                                                                                                                                                                                                                                                                                                                        units fineblend                                                                                    error fineblend                                    units sgng                                        units fineblend                                     error sgng                                         error fineblend                                                                                                                                                                        examples                                              examples              figure  fineblend  versus sgng                     figure  effects unit failure                                                                               fine blend versus sgng                                           ¯  figure  compares fineblend  sgng fritzke   start similarly soon sgng fails improve fur  ther increasing number units partly fact                       error threshold used inhibit reﬁnement    cause constantly high error level                             choice sgng parameters subjective                         ¯  testing ﬁnd far  optimal finding optimal parameters sgng figure  iterating random inputs dimensions  scope paper clear input vectors plotted each ε  perfectly suited speciﬁc application neighborhood ﬁxed point shown small box  comparison established generic architecture shows                                                                                          specialized architecture actually works able  choose random input vector ∈ nec  learn achieves goal specialization essarily valid embedded interpretation use  outperforms generic architecture speciﬁc setting initial input network    unit failure                                        iterate network reaches stable state                                                            outputs stay inside εneighborhood  figure  shows effects unit failure fineblend                                                                                                        network correctly initialized reﬁned through train example program unique ﬁxed point                                                         ing  samples hidden units given example  figure  shows input space                                                               ε                 removed randomly training continued noth neighborhood  intermediate                                                                                 ing happened network proves handle damage results iteration random initial inputs ex  gracefully recover quickly relative error exceeds ample computations converge underlying pro   slightly drops soon number units gram acyclic witzel  holldobler¨ et al  af                                                                    continues increase previous level recreating ter steps network stable cases fact  dundancy necessary robustness                    completely stable sense outputs stay ex                                                        actly εneighborhood    iterating random inputs                          corresponds roughly number applications                                                        program’s tp operator required ﬁx signiﬁcant atoms  original aims core method obtain                                                        conﬁrms training method really implements  nectionist systems logic programs iteratively                                                        intention learning tp  fact network ob  feeding output input settle stable state cor                                                        tained through training scratch converges sense  responding approximation ﬁxed point pro                                                        underlines efﬁcacy training method  gram’s singlestep operator running example unique  ﬁxed point known exist check  reﬂects proceed follows                     conclusions work                                                        reported new results overcoming propo    train network scratch relative error sitional ﬁxation current neuralsymbolic systems      caused network  network outputs              ε                                         best knowledge ﬁrst constructive approach      neighborhood desired output   approximating semantic operators ﬁrstorder logic    transform obtained network recurrent programs ﬁxed points fully connec      connecting outputs corresponding inputs tionist setting showed semantic operators                                                    ijcai                                                     
