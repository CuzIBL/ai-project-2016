                         faster heuristic search algorithms planning                                      uncertainty feedback                                  blai bonet                                      hector geffner                    science department                          departamento tccnologfa                university california los angeles                crea  universitat pompeu fabra                     los angeles ca  usa                            barcelona  espafia                       bonetcsuclaedu                            hectorgeffnertecnupfes                               abstract                               comprises loop states reachable                                                                  greedy policy bellman residuals greater        recent algorithms like rtdp lao combine                                                                  given  inconsistent states up•       strength heuristic search hs dynamic                                                                  dated left findandrevise makes explicit        programming dp methods exploiting knowl                                                                 basic idea underlying hsdp algorithms including rtdp        edge initial state admissible heuris•                                                                 lao prove convergence complexity op       tic function producing optimal policies                                                                  timality findandrevise introduce second al•       evaluating entire space paper in•                                                                 gorithm hdp builds adds labeling mech•       troduce analyze three new hsdp algorithms                                                                  anism detecting solved states based tarjans efficient        general algorithm schema simple                                                                  stronglyconnected components procedure tarjan         loop inconsistent reachable states                                                                  state solved reaches consistent states        residuals greater given                                                                  solved states skipped future searches hdp termi•       updated states serves                                                                  nates initial state solved hdp inherits con•       make explicit basic idea underlying hsdp                                                                  vergence optimality properties general flndand       algorithms leaving commitments aside                                                                  revise schema strongly competitive existing ap•       second algorithm builds adds                                                                  proaches algorithm hdpi like hdp        labeling mechanism detecting solved states                                                                  hdp computes value function enforcing        based tarjans stronglyconnected components                                                                  consistency reachable states reachable       procedure competitive existing                                                                                                                                   greedy policy hdpi enforces consistency        approaches algorithm approx•                                                                 likely reachable states show approxima•       imates enforcing consistency                                                                  tion suitably formalized lead great savings time        value function likely reachable states                                                                  memory apparent loss quality transi•       leads great time memory savings                                                                  tions probabilities differ greatly value        apparent loss quality transi•       tions probabilities differ greatly value      motivation twofold gain better understanding                                                                  hsdp methods planning uncertainty de•                                                                 velop effective hsdp algorithms optimal    introduction                                                 approximate planning   heuristic search algorithms successfully used   computing optimal solutions large deterministic problems     preliminaries   ikorf  presence nondeterminism    model   feedback solutions action sequences   policies policies characterized     model nondeterministic planning problems   computed dynamic programming dp methods bellman         feedback state models differ used    howard  dp methods states account   classical setting ways state transitions   scale large problems boutilier et al   probabilistic second states fully observable result•   recent algorithms like rtdp ibarto et al      ing models known markov decision processes mdps   lao hansen zilberstein  combine strength      specifically stochastic shortestpath problems   heuristic search dynamic programming methods         ibertsekas  given   exploiting knowledge initial state sq admissi•  ble heuristic function lower bound computing opti•  mal policies having evaluate entire space   paper aim contribute theory practice   heuristic searchdynamic programming methods for•  mulating analyzing three new hsdp algorithms   algorithm called flndandrevlse general schema       search                                                                                                                 positive action costs                               point equation    fully observable states                                                                                              presence feedback standard    markovian assumptions solution mdp takes       known bellmans equation stochastic short•  form function mapping states actions       est path problems like mm border condition    function called policy policy assigns prob•  vs   needed goal states value it•  ability state trajectory starting state            eration solves  plugging initial guess    given product transition probabili• righthand side  obtaining new guess left  ties assume                           hand side form vi known asynchronous value   actions goal states costs produce changes    iteration bertsekas  operation expressed                                              expected cost     associated policy starting state sq given                                                          weighted average probability trajectories times   cost optimal solution policy                                                                  vector size initialized arbitrarily normally   minimum expected cost possible initial                                                                   equality  replaced assignment   states optimal solution guaranteed exist provided                                                                  use expression  updating state value   following assumption holds bertsekas                                                                   called state update simply update standard syn•  goal reachable state nonzero       chronous value iteration states updated parallel        probability                                              asynchronous value iteration selected subset                                                                  states selected update time cases   initial state fixed   principle need compute complete policy partial   known states updated infinitely value   policy prescribing action states function converges eventually optimal value func•                                                                 tion practical point view value iteration stopped   reached following policy traditional dynamic   programming methods like value policy iteration compute     bellman error residual defined difference   complete policies recent heuristic search dp methods    left right    like rtdp lao compute partial policies achieve   means suitable heuristic functions hs provide   admissible estimates lower bounds expected cost   reach goal state                               states sufficiently small discounted mdp                                                                  formulation bound policy loss difference be•   dynamic programming                                        tween expected cost policy expected cost                                                                  optimal policy obtained simple expres•  heuristic value function defines greedy policy tt sion discount factor maximum residual                                                                  stochastic shortest path models similar closedform bound                                                               known bound computed bertsekas                                                                   execute value iteration max•                                                                 imum residual smaller given   expected cost resulting states as•                                                                 bound policy loss higher desired pro•  sumed given greedy action                                                                  cess repeated smaller   value function denote optimal ex•                                                                 hansen zilberstein  similar idea   pected cost state goal                                                                  reasons basic task computa•  known greedy policy  optimal op•                                                               tion value function vs residuals greater   timal cost function                                   given parameter       possible presence ties  greedy                                                                    definition known results pro•  policy unique assume paper                                                                  ceed say cost function monotonic iff   ties broken systematically using static order•  ing actions result value function defines                                                            unique greedy policy optimal cost function   defines unique optimal policy define relevant         notice monotonic value function   states states reachable sq using opti• decreases updated increase   mal policy constitute minimal set states updated state residual   optimal value function needs defined               greater deterministic setting non•     value iteration vi standard dynamic programming      monotonic cost function monotonic simply   method solving mdps based computing op•      taking value vs max vs   timal cost function plugging greedy policy  righthand side bellmans equation following results    optimal cost function solution fixed known                                                                  theorem  optimal values model mjm      definition relevant states restricted   barto et al  includes states reachable nonnegative finite monotonicity admis•  optimal policy                                         sibility value function preserved through updates                                                                                                                   search    start lower bound function      repeat          state greedy graph gv           revise      state      return                    algorithm  flndandrevlse       findandrevise    flndandrevlse schema general asynchronous vi        figure  graph stronglyconnected components    algorithm exploits knowledge initial state    admissible heuristic computing optimal nearly optimal                                                                    presence cycles greedy graph   policies having evaluate entire space let                                                                  algorithms common ao implementations    say value function consistent inconsistent                                                                  used reachable greedy graph    state residual greater greater                                                                  descendant bottomup approaches un•   consistent consistent                                                                  able label state solved labeling mechanism    states reachable  greedy policy                                                                 works presence cycles presented bonet    flndandrevlse computes consistent value function                                                                  geffner  improving convergence rtdp basi•   simply searching inconsistent states greedy graph                                                                  cally each rtdp trial attempt label    updating states left alg                                                                   unsolved state trial triggering systematic search      greedy graph refers graph resulting                                                                  inconsistent states state    execution greedy policy starting                                                                  updated new trial executed state    single root node gvand nongoal state                                                                  unsolved descendants labeled solved    vi children states result executing new cycle rtdp trials labeling checks triggered    action                                              idea improve removing need      procedures revise parameters    extra search label checking label checking    flndandrevlse procedure convergence opti     dfs search over•   mality complexity flndandrevlse assume     head exploiting tarjans linear algorithm detecting    searches graph systematically revise    stronglyconnected components directed graph itarjan    updates possibly states     correspondence stronglyconnected    operations taking time                                components greedy graph minimal collections    theorem  convergence planning model mim            states labeled time    initial value function admissible         consider directed greedy graph relation    monotonia flndawrevlse yields value pairs states holds   function number loop iterations greater        reachable reachable                             each iteration time com   gv stronglyconnected components gv                                                                  equivalence classes defined relation form parti•                                                                 tion set states gv example greedy    theorem  optimality planning model ml ms       graph fig    terminal goal states    initial admissible monotonia value function value components   function computed findandrevlse approaches op        — tarjans algorithm detects strongly   timal value function relevant states goes  connected components directed graph time                                                                  traversing graph depthfirst stands     labeling                                                    number states gv number                                                                  edges    consider particular instance general     relationship labeling stronglyconnected    andrevise schema operation carried      components gv quite direct let say    systematic depthfirst search keeps track    component econsistent states    states visited addition consider labeling scheme consistent component solved state    search detects overhead  solved lets define graph    state solved skipped     nodes components directed edges    future searches state defined solved   state reachable    value function econsistent states state clearly acyclic graph compo•   reachable greedy policy clearly     nents reachable each collapsed    condition holds updates needed states equivalence class addition    reachable  resulting algorithm terminates    initial state solved econsistent    state solved iff component solved    value function obtained                                  furthermore        search                                                                                                                 component solved iff consistent com•       ponents solved   problem labeling states cyclic graph gy   mapped problem labeling components   acyclic graph gy   fashion     fig  easy visualize component graph asso•  ciated greedy graph  inconsistent   state example label components ci   solved leaving unsolved     code simultaneously checks depthfirst fashion   consistency states possibility labeling   shown alg  resulting algorithm   hdp hdp inherits convergence optimality properties   findandrevise schema correctness   labeling mechanism     space explain hdp code   clear familiar tarjans algorithm   particular use state visit number sidx   lowlink number slow detecting new compo•  nent flag flag normal propa•  gation visit numbers prevent component   labeled solved inconsistent reach in•  consistent component   theorem  correctness value function computed   hdp planning model mm given initial admissible   monotonic value function tconsistent      experimental results   evaluate performance hdp comparison   recent heuristic searchdp algorithms sec•  ond code lao hansen zilberstein    improved lao ilao labeled rtdp lrtdp   recent improvement rtdp accelerates convergence   bonet geffner  use parallel value iteration   baseline weve implemented algorithms   experiments run sun firer    mhz gb ram     domain use experiments racetrack   barto et al  states tuples               table  including number states optimal cost per•  represent position speed car  centage states relevant   dimensions actions pairs — ax ay instan•                                                                  heuristic follow bonet geffner  use   taneous accelerations uncer•                                                                domain independent admissible monotonic heuristic   tainty domain comes assuming road                                                                 hmini obtained replacing expected cost bellman   slippery result car fail accelerate                                                                 equation best possible cost total time spent com•  desaccelerate precisely action  ax ay                                                                 puting heuristic values roughly different   intended effect probability   prob•                                                                algorithms vi shown separately fifth   ability action effects correspond action                                                                 row table value  experiments        car hits wall velocity set                                                                                                       carried three heuristics   zero position left intact different                                                                     barto et al  reason car moved   start position                                         results shown table  hdp dominates     consider track largeb barto et al      algorithms instances lrtdp                                                                 best exceptions weaker heuris•  track hansen zilberstein    tracks squares rings different size informa•  tics  used hdp best   tion instances three rows exploiting good heuristic information instances                                                                 lrtdp bootstraps quickly quickly computes        taket source code lao                    good value function hope understand reasons                                                                                                                  search table  problem data convergence time seconds different algorithms different heuristics results   —   probability   faster times shown bold     differences future                                code hdpo corresponds exactly                                                                 code hdp possible successors state    approximation                                               greedy graph replaced plausible successors                                                                    hdpi computes lower bounds tend quite tight   hdp like findandrevise computes value function     states reached plausibility smaller   enforcing consistency states reachable   run time executions contain sur•  greedy policy ny final variation consider    prising outcomes taking envelope   hdpi works way enforces   states quality value function cor•  consistency value function states  responding policy poor deal situations   reachable greedy policy    define version hdpi called hdptj inter•  minimum likelihood                                           leaves planning execution follows hdpi plans     efficiency formalize notion likelihood using  means hdpi algorithm exe•  nonnegative integer scale  refers normal out• cutes policy state trajectory plausibility mea•  come  refers somewhat surprising outcome  sure greater equal leading nongoal   surprising outcome measures    state obtained point algorithm replans   plausibilities kept mind  refers hdpi execution replanning cycle   plausible outcomes plausibility greater followed reaching goal clearly sufficiently   means plausibility measure smaller equal iy large hdpij reduces hdpi large hdpi     obtain transition plausibilities kass   reduces hdp   corresponding transition probabilities following dis•     table  shows average cost hdpij     cretization                                                  plausible transitions considered                                                                 values replanning thresholds each entry ta•                                                             ble correspond average  independent execu•                                                                tions include average cost greedy policy   plausibilities                                  respect bottomline reference fig•  formalized plausible states  ures memory table refers number evaluated   plausibility  transition plausibilities com• states results show smooth tradeoff be•  bined rules calculus ispohn    tween quality average cost goal time spent   calculus isomorphic probability calculus gold initial planning posterior replannings parameter   szmidt pearl  plausibility state trajectory vary class problems hmtn   given initial state given sum transition heuristic delivers good greedy policy   plausibilities trajectory plausibility reach• research necessary assess goodness hdij   ing state given plausibility plausible hmin heuristic   trajectory reaching state     hdpi algorithm nonnegative integer com•     related work   putes value function enforcing consistency   states reachable plausibility greater   built ibarto et al  bertsekas    equal hdpi produces approximate policies fast   recently hansen zilberstein    pruning certain paths search simplest case results bonet geffner  crucial difference       search                                                                                                              
