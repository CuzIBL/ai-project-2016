                        learning count think aloud imitation                                                laurent orseau                                       insairisa rennes france                                              lorseauirisafr                          abstract                           uses paperandpencil approach learn im                                                        itation algorithms complex making division      necessary learning discover new solu assumptions clearly ease learn      tions long difﬁcult suppos ing paperandpencil approach provides external inﬁ      edly simple tasks counting  nite memory useful recall mechanisms      hand learning imitation provides simple way contains basic arithmetic tools      acquire knowledge watching agents tion number numerical superiority furthermore      order learn complex tasks imitation able learn make function calls access subrou      mere sequences actions think aloud pro tines teacher explicitly specify function      tocol introduced new neurosymbolic net use inputs outputs calling function      work uses time way  special action      time delay neural network added basic     paper speciﬁc action sequences      ﬁrst order logic capacities tested benchmark composed completely apriorimeaningless symbols      counting task learning fast generalization symbols interchanged changing      accurate initial bias task able make basic automatic function calls      counting                                         explicitly specifying initial internal feature                                                        biases counting                                                          section  think aloud imitation learning described    introduction                                       usual neural network superseded  learning count difﬁcult task wellknown section presence network growing neuro  children learn parents symbolic network orseau section   teachers provide important active help  augmented special connections develop    hand ﬁeld online machine learn ment explained simple task experiments  ing teacher does provide help recur counting given showing learning faster  rent neural networks interestingly successful orders magnitude usual neural network  long shortterm memory lstm hochreiter schmid important learning issues circumvented  huber  learn counters network learning complex tasks online examples  requires trained through tens thousands se possible  quences contains counting feature  speciﬁcally designed           think aloud imitation learning    active teaching automatic discovery stands imitation environment gives task solve  learning imitation schaal  learner watches produces events envt  teacher knows  “teacher” solve task teacher aware solve produces tcht wheret denotes present time  presence learner helpful sequences during learning agent tries predict teacher’s  actions example make robot acquire complex motor action succeeds tcht   behavior schaal  calinon et al  learner agent given sequences events  intermediate actions         abcdefgh events provided environment    fewer studies imitation focus learning sequences written lowercase letters provided  complex sequences physical actions teacher capitals seen sym  algorithms programming demonstration cypher  bols  letters alphabet used events  deals issue interested humanmachine paper better legibility spaces  graphical interaction makes assumptions times introduced sequences goal learner  tasks represented maybe work clos predict teacher’s events  est presented paper furse furse predicts fact acts like teacher                                                    ijcai                                                      imitation problem recurrent neural networks  presence network  example internal states learner                                                        tdnn number input connections single hid  “see” states inside teacher difﬁ          cult learn                                        den neuron wj    τni whereni  number                                                        inputs grows quickly τ ni grows makes    think aloud paradigm teacher                                                        learning difﬁcult number training examples  forced “think aloud” means does inter                                                            said  experiments described section   nal states “hear” says recurrence                                                        time steps neurons inputs needed using  external internal easily imitated                                                        mere tdnn unpractical tdnn backpropagation  learner “listens” teacher learning                                                        important limitations learning tasks  agent learning anymore thinks aloud                                                        robins frean  prone catastrophic  hear allows teacher make intermedi                                                        getting learning concept concept modiﬁes  ate computation steps imitated learner dur                                                        knowledge concept issue arises  ing learning agent receives sequence events                                                        unmodiﬁed weights network tend provide  teacher environment et envt tcht                                                         swers learning weights frozen  learning during testing agent                                                        concept adding new neurons  tcht φ agent listens teacher                                                        temporarily mask acquired knowledge learn  etenvt    seen teacher                                                        ing adapted prevent  forcing technique williams zipser                                                           instead mere tdnn use presence network    ﬁrst simpliﬁed view architecture given orseau neurosymbolic network  order time account having internal tailored learning sequences events dealt  states use sort time delay neural network tdnn uses time way tdnn                               sejnowski rosenberg   computes outputs main properties fast learning local connectivity  given τ time steps present time itcom                     −       −            automatic creation neurons slight modiﬁcation  putes net        τ             “speciﬁcity” introduced circumvent issue ini    sequel indices used respectively tial network presence network described  inputs hidden neurons output neurons xi activa use speciﬁc context orseau  tion input variables writing yi shortcut heuristic idea new hidden neurons connected  yit wheret present time excep                       −                         inputs active short past  tion xi stands xi  writing means  shortterm τmemory number connections  input activated time inputs encoded does fully depend number inputs partic  ofn way                                         ularly useful event type coded single input    usually tdnn architecture explained delayed sequences composed mutually exclusive events  copy sets inputs follows each                                                      event time step neuron  nection weight wji input unit hidden unit connection time step network initially                             delay dsothatifeti wji transmits activation hidden neuron new ones added online  unit time  hidden neuron connected errors occur                                           input through number τ connections wji each hidden neuron optimization rule given  having different delay τd≤  network order show network represent action  produce recurrent inﬁnite behaviors                 selection scheme given show circum    fact agent hear says during vent issue representation capacities network  testing gives recurrent capacities actions learning protocol follows  injected inputs resulting recurrent network quite  similar narx neural network lin et al   hidden neuron  learning considerations through recurrence  propagation through time example agent given sequence new neuron created spec  sequences like abababab    learns say af iﬁed section  new neuron connected  ter ab ba hears ab puts active short past corresponding  says heard abaandthensaysbhas delay ditssetwj input connections  heard abab says produces inﬁnite   ∀ −     ≤    ∃            −   behavior learning occurs tdnn       wj    wji   τt      wjii         instead direct supervision outputs use instant sequence used create called sequence ref                                  inforcements kaelbling et al  single output erence rj   general autonomous agent like robot                                                          let θj wj modiﬁable set  used imitation imitating act          ∀ ∈        ing like teacher innerly rewarding actions selected initial weight value wji wj wji θj ensures                                                                inputs each time step predict id wji  output weight wkj reinforce  actioninput each input tested predicts ment output set                                                                                              best reinforcement time step chosen    let σj weighted sum σj  wjixi  activation                                                    ijcai                                                                function yj                                         action selection                  ⎧                  ⎨            σj ≤  − θj     inactive neuron action selection scheme basically sec                      σj −−θj                                    tion  passive mode neuron ﬁlter recognizes              yj                − θj σj    active neuron                  ⎩   −−θj                                      sequence predicts reinforcement value active mode                                    ≥                                 σj              active neuron  agent predicts action time step                note  − θj activation threshold rj sequence recognized              presented agent recognizes yj  neurons consider events present              optimization rj activate active inputs main drawback presence network              event substituted yj           inactive inputs taken account                                                                    activity important said way network                optimization hidden neuron                  distinguish exception cases general cases example              optimize neuron each seen infor consider sequences abcz adcz aecz suppose              mative close θ close  negative exception case aycx neuron              weight neuron activated “quanta” embodies case ayc naturally predicts               weights moved inactive connections transmit time neuron representing general case              ted activation active ones converse aczpredictsz select right action              neuron active                          neuron acz inhibitory link input                                                                    preserve interesting properties presence                                                                    network connect inactive inputs potentially               algorithm  update rule input weights hidden great number new neuron precedence action selec               neuron α learning rate er error received tion automatically given cases speciﬁc               algorithm                                     speciﬁc neurons events account                δ  αθjer         maximum modiﬁcation  “explain” accurately given sequence                                                                                                                               ∀wji ∈ wj                                            usefulness connection deﬁned uwji                                                           min                     xi  er   orxi     er              wjiθj  speciﬁcity ψj means how neuron                   Δd         −                                                                                               minδ    wji                              explains current sequence ψj  xi uwjithen                                               Δi  −minδ wji                              preferred select speciﬁc neuron                                                                   activated ones algorithm                                Δd                      i∈i  xd                                speciﬁcity neuron recognizes aczis                                            −                                                ψ   middle weight                      i∈i  xd Δi                                                                   speciﬁcity neuron recognizes aycx                 minss−                                      optimized ψ  means se                ∀  ∈                                           quence ayc agent choose preferentially                 wji  wj                                 neuron representing exception case similar general                                        −                   xi  si    si                   case nonzero weights ensures se                    ←    Δd                     wji   wji     sd                              lected priority                                                                                                        interesting property general case needs                                                                   modiﬁcation learning speciﬁc cases                                                                learning algorithm ensures id wji  se              quence reference rj recognized jall algorithm  action selection              weights stay   corresponding events              present neuron created logical  event activates each time step              weights inhibitory neuron ﬁl ∀j ej  argmaxet  yjt               ter active recognizes conjunction events compute corresponding speciﬁcity ψjt               appearing different time steps describes poten  active specialized neurons              tially general sequence events weights non   yjt   ψjt   maxh ψht               informative time steps pushed zero           select specialized active neuron                output weight chosen neuron updated  argmaxj∈a yjt               simple delta rule bounded   wkj ← wkj ηer select action ep exists φ              η learning rate value critical fact              output weight inﬂuence concept em              bodied hidden neuron used “dis              card” neuronconcept statistically wrong    learning protocol                example single neuron represent sequence catastrophic forgetting backpropagation happens              abcz meaning agent receives sequence abc partly knowledge shared neurons              neuron proposes answer examples neurons modiﬁed time              adcz aecz neuron generalized acz avoid  neurons modiﬁed               means possible symbol                    time order compare neurons present                                                                ijcai                                                                sequence neuron active yj  preactivation single event referred  deﬁned y˜j  σj − θj                        nections deﬁnition speciﬁcity modiﬁed        ˜                                               event delay taken account exam    let preactivated neuron                   connection pointing et  order able ple abc initially ψ onaba  normal                                                        connections plus one tdic ψ  logically  predict                                                                      ˜                                                    ψj  maxi xi uwji   argmaxj  y˜j  yj  y˜j   ∃wjid                        ˜                                 resulting represent prolog logic   et  exists  φ                    programs muggleton  lists online    accurate selection mechanism nearest neu way explicit function parameter  ron high learning rate used course nearest tdics bind variables through time allow comparison  neuron correct optimize variables variables events time steps  probably optimized way typical ﬁrst order logic capacity    algorithm  describes learning protocol decide  neurons created modiﬁed neuron  simple task equality  added time modiﬁed example network works develop  wrongly optimized neuron ac ment shown trained simple equality task  tive means recognized sequence predict agent answer true false given events  right answer neuron needs added    identical static task essence examples                                                        sequences ab dd ph cc vv   algorithm  learning protocol                        network  inputs τ higher initially                                                        hidden neuron    time  predict teacher action       ←                                                 ﬁrst sequence ab provided agent during                                                     ﬁrst time steps happens net                   correct action selected                                                work puts symbols τmemory symbol fthe       optimize selected neuron error                                                        network predicted did       er −   wkpyp                                                        neuron new neuron created order          create new neuron current speciﬁc case        make correlate ab connected input                                                        delay tob delay andtof        ∃h yh   wkhyh  maxj wkj yj                                                        input weights set      sequence ab           selected                                                                                            presented agent network predicts fthen          optimize exists er − wkpyp                                                        teacher says activated expecting reward effec          optimize er − wkhyh                                                        tively receives reward modiﬁcation needed                         ˜                                    sequence dd provided agent          optimize exists er                      active preactive neuron predict                                                        tsoanewneuronn     added  connections                                                         similar previous case  tdic                                                        created repetition delays     time delayed identity connections                     answer correctly dd note  time delayed identity connections tdic allows know tdic necessary learn case heart  repetition event short term useful optimized  orseau presented form short sequence ph fispresentedn’s tdic point  term memory special inputs dynamic connec ing does transmit activation neuron  tions enhance network gener predicted fsoanewneuronn added answer correctly  alization capacity allow compare events sequence preactivated chosen  passive mode repeat past event active mode slightly modiﬁed predict weight active    creating neuron present time sequence nections transmitted activation increased  events eiftwoeventset et identical input weights decreased input                                    dd  τt−  t−  connection wji added connections considered noise  setting θj neuron following properties sequence cc predicts  addition normal delay  − slightly activate repetition    − pointing event compare teacher says instead neuron activated new neu  connection dynamic each time step  target input ron added modiﬁed nbeing       dd             wji changed  et −  preactivated neuron modiﬁed particular  connection behaves exactly normal delay dnote weight tdic increased pointing  special case  normal connection     sequence vv predicts predicts                                                        ψn approximately ψn   explains         −θj happens neuron single connection speciﬁcally sequence network predicts  yj  et                    correct neuron added optimized    et − does exists idle input unit better predict higher value                                                    ijcai                                                       network predicts correctly sequence like group agent ﬁrst learn heart group  γγ recognized nandγβ recognized each letter given letter group learn  γ β letters                    generalize answer yes letter group                                                        nootherwise    experiments                                          task nicely shows intermediate computations                                                        reuse background knowledge necessary generalize  present work close continual learning framework                                                          ﬁrst task agent given  sequences             ring  agent reuse previously acquired type gpaa gpzf gp  knowledge order perform better future tasks task context learning stops training set  interested tasks reuse knowledge                                                        correctly processed learning neurons frozen  useful necessary generalize called facilitate learning second task  function static tasks using temporal frame                                                          second task agent given sequences like  work access generalization capacities                                                        fb gpfb y“isf group bthegroup    order show usefulness context  agent make automatic function calls section  yes” bc gpba context gp used  use task described orseau tested appropriate function context  tdnn backpropagation “tdic” inputs   ﬁrst task conﬂicts tasks    previous tasks seen static tasks section  neurons general answers  given typical dynamic domain languages time needed context allows choose  sequence xnyn sequence ﬁnite number nearest neuron accurately sequence bc  followed sequence       agent makes intermediate computations saying gpb    inputs  letters alphabet α η  heard gpb autostimulated  τ set  shorter sequences create connec answer given knowledge ﬁrst task tdics  tions presence network τ set sufﬁ allows repeat letter given teacher auto  ciently high value events account matically gp agent learned function  learning stops given number correctly predicted se gp speciﬁc symbol tell function  quences number taken account table parameter function consequence  learning presence network deterministic knowledge represented used tdics  order training sequences randomized during test used compare group given teacher  ing agent predicted actions agent knows  place teacher’s events  successive trials learning during training  ﬁrst letters  ﬁrst  testing each task results table  group letters used learning stops  successive cor                                                        rectly processed sequences tdnn used orseau                                                        necessary  letters generalization  table  average results  successive trials tr ts induces errors importantly necessary  number training testing sequences etr ets force agent say during  time steps  number sequences agent errors during end sequences ﬁrst task order prevent  training testing number neurons created neurons disturbing learning second task  task learned total number weights problems arise presence network  neurons  tasks trained normal furthermore results table  show learning  tdnn                                                 faster important online learning general   task        tr   etr   ts   ets                ization perfect letter group   gp        na                     gp                                counting   gp      na                         agent special internal feature generalize   gp                            counter acquire knowledge numbers   ﬂw                               agent needs learn increment    xnyn                            digit number uses incrementation function                                                        solve task xnyn    hardly directly compared incrementation  methods closest furse  task given digit number agent answer  important differences section  successor number followsaaabexamplesof  similarities inductive logic programming muggleton sequences ﬂw aa ab ﬂw ab acﬂw az ba ﬂw ba   aims different ilp online ex bbﬂw zy zz  plicit function names uses global search mechanisms  general cases learn ﬂw γa γb ﬂw γb                                                        γc ﬂw γy γz  speciﬁc cases ﬂw az ba ﬂw    groups                                           bz caﬂw yz za total  cases  alphabet decomposed  groups consecutive let training set contains  speciﬁc cases  ters group group  ﬁrst cases sequences chosen randomly                                                    ijcai                                                    
