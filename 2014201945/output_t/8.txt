           learning global models based distributed data abstractions                                   xiaofeng zhang william cheung                                      department science                                       hong kong baptist university                                        kowloon tong hong kong                                   xfzhangwilliamcomphkbueduhk                        abstract                          rough ensemble illustration sharing lo                                                        cal model parameters global analysis degree data      increasing demand massive dis  privacy preservation bandwidth requirement dis      tributed data analysis achieving highly accurate tributed environment readily controlled paper      global data analysis results local data privacy emlike algorithm proposed learning global gmm      preserved increasingly important   directly aggregated version local abstractions      search issue paper propose adopt      modelbased method gaussian mixture model      local data abstraction aggregate local model  problem formulation      parameters learning global models support      global model learning based solely local gmm     local data abstraction                                                                      parameters instead virtual data generated let ti ∈  denote observed data instance θl denote      aggregated local model novel emlike algo parameters lth local model gmm abstraction      rithm derived experiments performed lth source probability density function lth      using synthetic datasets proposed method  local model plocaltiθl kl components given      demonstrated able achieve global                                                                            kl                kl      model accuracy comparable using data     plocaltiθl  pj αjlpj tiθlj  pj αjl                                                                                    regeneration approach lower computa                   −    −               −                                                         pj tiθlj   π Σlj  exp− ti − µlj  Σlj ti − µlj       tional cost                                                                                                                                 sets local gmms’ parameters θ θ  θl ob    introduction                                       tained performing local modelbased density estimation  machine learning data mining algorithms work em algorithm sent global server learn  basic assumption data ﬁrst pooled global data model  centralized data repository recently exist  growing number cases data physically  learning global model parameters  distributed practical constraints privacy start derivation learning algorithm consid  concerns need tackle calls recent ering ﬁrst conventional setting expectation maxi  search distributed data mining           mization em algorithm let rik denote estimated indi    common methodology distributed data mining cator kth component global model generating  ﬁrst perform local data analysis combine results ith data instance                                                                                              th  form global kargupta et al   major lim assume rlk new indicator local com  itation local analysis result loss informa ponent underlying data generated kth                                                    tion global pattern discovery zhang et al   global component rik denotes average estimate  model parameter exchange approach proposed im probability data instances lth local compo  prove global model accuracy nent generated kth global component approx  generic methodology easily generalized dif imating rik  ferent types global models alternative adopt  ﬂexible model local data abstraction                                                                                pi∈lthsource rik  subsequent analysis based local model parameters       rik ≈ rik                                                                                                                nl  adopt paper                                                                                                th    inspired merugu ghosh  gaussian mix nl denotes number data source  ture model gmm bishop  chosen local data  abstraction different parameter settings gmm note abuse index “l” refer local component  terpolate set data detailed information aggregated local modelnew msteps                                     rlkµl               pl                       µk           αk  pl rlk                    rlk               pl                                                                   rlkΣlµlµ                 pl                         Σk                − µkµk                          rlk                    pl  kldivergence adopted compute rlk estep resulting                  exp−dpglobaltφkplocaltθl         rlk                                                       exp−dpglobaltφiplocaltθl               pi  dp denotes kldivergence  probabilistic models derived       kldivergence model learned                                                             ing proposed method different numbers             Σl          −              −                      components global model   ln      traceΣ Σk  µk − µl Σ µl − µk −                                       Σk      experiments  experiments performed using synthetic data sets  performance evaluation data sets ground truth  generated using known mixture models      components respectively data instances  randomly partitioned sources local gmms  learned conventional em algorithm each source  global models trained using proposed  method using conventional data regeneration ap  proach shown table  proposed method  achieve speedup factor ranging   highly   gmm   components learned  comparable model accuracy                                     based  local gaussian components                                                                 aggregated local model  table  comparing global models learned using pro  posed method data regeneration method gx denotes figure  experimental results sizeable dataset  true data model components            timespeedup loglikediff meandiff kl                  global mediator local data sources compro                  mising individual local data sources’ privacy levels                  adopted improve global model accuracy                                    acknowledgement                                                        work partially supported rgc central al    proposed approach applied location group research grant hkbu  sizeable dataset generated using gmm  compo  nents global models different number components references  learned corresponding kldivergence values                                                        bishop  bishop neural networks pattern  compared true model shown figure                                                            recognition oxford university press   divergence value decreased rapidly number  components increased   started satu kargupta et al  kargupta park hersh  rate determine optimal number global com berger johnson collective data mining new  ponents setting threshold kldivergence value perspective distributed data mining ad  cutoff figure  shows result  global compo vances distributed parallel knowledge discovery  nents identiﬁed                                         mitaaai press                                                         merugu ghosh  merugu    ghosh    conclusion future plan                            privacypreserving distributed clustering using gen  paper proposed novel emlike algorithm erative models proceedings ieee international  learning global model directly aggregated local conference data mining   model parameters promising experimental results zhang et al  xiaofeng zhang cm lam  synthetic datasets currently extending obtained william cheung mining local data sources learn  results learn sophisticated global models ing global cluster models local model exchange  investigating active negotiation ieee intelligent informatics bulletin  
