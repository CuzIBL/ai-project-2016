                     generalization error linear neural networks                                 empirical bayes approach                               shinichi nakajima   †‡ sumio watanabe     †                                      † tokyo institute technology            mailbox  nagatsuda midoriku yokohama kanagawa  japan                           nakajimascspititechacjp swatanabpititechacjp                                            ‡ nikon corporation                        oazamiizugahara kumagaya saitama  japan                      abstract                          neglected proposed hinton van camp                                                         mackay  attias  ghahramani beal       known unidentiﬁable models  paper consider alternative      bayes estimation advantage generaliza subspace bayes sb approach sb approach      tion performance maximum likelihood esti  empirical bayes eb approach parame      mation accurate approximation    ters model regarded hyperparameters      posterior distribution requires huge computational gard parameters layer hyperparameters      costs paper consider empirical bayes analytically calculate marginal likelihood three      approach parameters   layer models consequently      garded hyperparameters sub   ﬁnd hyperparameter value maximizing marginal like      space bayes approach theoretically analyze   lihood computational costs sb approach      generalization error threelayer linear neu posterior distribution approximation      ral networks show subspace bayes ap   mcmc methods ﬁrst paper prove three      proach asymptotically equivalent positive layer linear neural networks sb approach equivalent      jamesstein type shrinkage estimation positivepart jamesstein js type shrinkage estimation      haves similarly bayes estimation typical james stein  clarify generalization      cases                                            error considering delicate situations important                                                        situations model selection problems statistical tests                                                        kullbackleibler divergence true distribution    introduction                                       singularities comparable inverse num                                                                              unidentiﬁable parametric models neural networks ber training samples conclude sb approach  mixture models wide range applica provides good performance bayes estimation typ  tions models singularities parameter space ical cases  conventional learning theory regular sta section  neural networks linear neural networks  tistical models does hold recently generalization brieﬂy introduced framework bayes estima  formance unidentiﬁable models theoreti tion eb approach sb approach  cally clariﬁed maximum likelihood ml estimation described section  signiﬁcance singularities  asymptotically equivalent maximum poste generalization performance importance analysis  rior map estimation generalization error linear neu delicate situations explained section  sb solu  ral networks proved greater reg tion generalization error derived section  dis  ular models dimension parameter space cussions conclusions follow section  section   model redundant learn true distri respectively  bution fukumizu  hand bayes  estimation generalization error neural networks lin  linear neural networks  ear neural networks mixture models proved ∈ rm                         ∈ rn                                                       let        input column vector    output  regular models watanabe        aoyagi watanabe  yamazaki watanabe  vector parameter vector neural network model                                                        described parametric family maps f·    bayes posterior distribution seldom rm → rn                               exactly realized furthermore markov chain monte carlo           threelayer neural network hidden  mcmc methods used approximation pos just derived variational bayes solution linear  terior distribution require huge computational costs neural networks clariﬁed generalization error training  alternative variational bayes approach corre error nakajima watanabe  lation parameters parameters clariﬁed training error  correlation parameters hidden variables nakajima watanabe aunits deﬁned                                                                                                                                 h                                                                                  qyx               fx   bhψ ahx                      yx                 dxdy                                                                          log yx nyn                                                                                                     ahbh ∈  ×    summa  rizes parameters ψ· activation function kullbackleibler kl divergence predictive dis  usually bounded nondecreasing antisymmetric nonlin tribution true distribution ·qxny denotes  ear function like tanh·andt denotes transpose expectation value sets training samples  matrix vector assume output observed                                                         empirical bayes approach  noise subject nn σ wherendµ Σ denotes  ddimensional normal distribution average vector µ         subspace bayes approach  covariance matrix Σandid denotes d×d identity matrix little information prior distribution  conditional distribution given        eb approach originally proposed cope                                                                            y −            introduce hyperparameters prior distribution    yx                 −                     example use prior distribution depends                 exp                                    πσ                 σ                 hyperparameter τ                                                                                                paper focus linear neural networks ac                           w  tivation function linear simplest multilayer models φw           exp  −                                                                                        πτ         τ   linear neural network model lnn deﬁned                                                                                                                                       τ                   fx bbax                   marginal likelihood eq depends inaneb                                                        approach τ estimated maximizing marginal likeli                        aah  × input parameter ma hood slightly different way efron morris   trix bbh  × output parameter akaike  kass steffey  extending idea  matrix transform → tabt− does introduce hyperparameters model dis  change map nonsingular × matrix  tribution sb approach eb approach  parameterization eq trivial redundancy ac parameters model regarded hy  cordingly essential dimension parameter space perparameters following sections analyze ver  given                                              sions sb approach ﬁrst regard output                                                       parameter matrix map eq hyperparame                  hm    −                                                       ter marginalize likelihood input parameter  assume ≤ ≤ paper       space mip regard input pa                                                        rameter matrix instead hyperparameter    framework learning methods                      marginalize output parameter space mop    bayes estimation                                                       unidentiﬁability singularities  let   xxn  yyn arbitrary  training samples independently identically taken say parametric model unidentiﬁable map  true distribution qx yqxqyx marginal parameter probability distribution  ditional likelihood model pyx given  toone neural network model eq unidentiﬁable                                                       cause model independent ah bh orvice                                                                              versa continuous points denoting distribution         zy      φw      pyixiwdw                                                        called singularities fisher information                                                        matrix degenerates true model  φw prior distribution posterior distribu                                                        singularities asymptotically affect prediction  tion given                                                        conventional learning theory regular                             n                        φw      pyixiw            models holds hand true model          pwxnyn                                                     zy nxn                  singularities signiﬁcantly affect generalization                                                        formance follows ml estimation extent  predictive distribution deﬁned average set points denoting true distribution increases  model posterior distribution follows                                                       neighborhoods ﬂexibility imitating noises                                                accelerates overﬁtting bayes es     pyx      pyx wpwx dw       timation large entropy singularities increases                                                        weights distributions near true  generalization error criterion generalization perfor suppresses overﬁtting lnns property appears  mance deﬁned                                   acceleration overﬁtting selection largest singu                                                    lar value components random matrix sb ap              gngx     qxny                                                               proaches lnns property appears jamesstein    linear neural network model toy useful model type shrinkage shown following sections  known reducedrank regression model applications suppression overﬁtting accompanies insensitivity  reinsel velu                              true components small amplitude tradeoffwhich ignored asymptotic analysis proof given appendix  consider situations true model dis independence makes  tinctly singularities paper posterior distribution localized following lemma holds  consider delicate situations kl divergence  true distribution singularities comparable lemma  predictive distribution sb approaches  inverse number training samples n−which written follows                                                                                  −  important situations model selection problems              statistical tests ﬁnite number samples follow pyx  π vˆ                                                                                             ing reasons ﬁrst naturally exist true compo            −                                 n−                                    vˆ                   −  nents amplitude comparable    · exp −y−vˆ bˆaxˆ   y−vˆ bˆaxˆ        smallest largest model selected secondly              selected model involves components essen                                                                           −  tially affects generalization performance            vˆ      ·denotes determinant                                                        matrix    theoretical analysis                                                        proof predictive distribution written follows    subspace bayes solution                                                                                                                τ            pyx pyx abpaxny nb    distinguish hyperparameter                                    parameter example pyx wτ assume                        ∗ ∗                                                              ∝ qyx exp  bˆaˆ −            variance noise known equal unity                                 paxny nb  conditional distribution lnn mip version sb  approach given                                  ·p denotes expectation value distribution                                                                    ∗ ∗       −                                 y − bax             bˆaˆ −      sb approaches    yx ab                −                               π exp                       expand predictive distribution follows                                                                               use following prior distribution                yx nyn ∝ yx      yt bˆaˆ − b∗a∗                                                                                                                                                                                                                 tr                                                   φa            exp  −                                            vv        −                  πhm                                                                                                                                                         paxny nb                                 pyx ba    φb  note similarly prepare                          √  mop version assume true conditional  nbˆaˆ− b∗a∗x ndimensional vector              yx a∗b∗        b∗a∗  distribution             true map  order calculating expectation value expanding           h∗  ≤                ∗  rank        wedenoteby      true value logarithm eq arrive lemma  qed  parameter hat estimator parameter comparing eq ml estimator              ˆ ˆ  example bh simplicity assume                                                                                                    −  input vector orthonormalized xx qxdx  im           bˆaˆ           ω  ω  rq                                                                        mle    bh bh                consequently central limit theorem leads following  equations                                        baldi hornik  ﬁnd sb estimator                                   −                 −            each component asymptotically equivalent positive       qx           xx   im                             i                                js type shrinkage estimator virtue             −         ∗  ∗      −    rx        yx            lemma  substitute model sb estimator  qxn     ×   symmetric matrix    predictive distribution asymptotically insigniﬁ  rxnyn  ×  matrix abbreviate impact generalization performance  qxn  qandrxnyn                         conclude sb approach asymptotically equivalent    let γh hth largest singular value matrix shrinkage estimation note variance prior  rq−  ω                                             distribution eq asymptotically effect pre           ah corresponding right singular vector  ω                                                     diction generalization performance far   bh corresponding left singular vector ﬁnd                     ∗                         −     positive ﬁnite constant thedegreeofshrink  eq γh h≤     order                                                          age remember modify theorems  combining eq                                                        paper ml estimation letting      ω  rqρ    ω    n−       h∗  h≤        bh       bh                          −∞  ρ∈       ∞  arbitrary constant  generalization error  sb estimator deﬁned expectation value sb using singular value decomposition true map  posterior distribution given following theorem b∗a∗ transform arbitrary a∗ b∗ change  theorem  let   mip version  map matrix orthogonal row vectors                                  mop version lh maxl nγh sb estimator matrix orthogonal column vectors respec  map lnn given                         tively accordingly assume orthogonalities                             −         −      −           loss generality lemma  implies    bˆaˆ       −  lωb  ω  rq                              bh                        kl divergence eq set training samples isgiven                                                large scale approximation                                                          ∗ ∗                             similar fashion analysis ml estimation                   b   − bˆaˆx     gxnyn                          on−       fukumizu  second term eq analyt                                                                qx               ically calculated large scale limit hand                                                          ∗                  h                                      inﬁnity order deﬁne following                         xnyn     n−                                   ∗         ∗                                                  scalars α   −h  −h    β                                                          −  h∗n  − h∗andκ    lm    lm  −  h∗                                                                                                                                                 let  random matrix subject wn                   ∗ ∗t     ∗  ∗t                                             −  ghx      tr bhah − ˆbhaˆh bhah − ˆbhaˆh  uun   eigenvalues  measure                                                       empirical distribution eigenvalues deﬁned                                          ·  contribution th component tr  denotes       −                                                           du     δ     δ    ···  δ     trace matrix denote wdm Σ Λ                                 dimensional wishart distribution degrees freedom δ                                    Σ                       Λ                      denotes dirac measure  large scale  scale matrix  noncentrality matrix  abbreviate limit measure eq  converges  wdm Σ central wishart distribution                         theorem  generalization error lnn sb             − umum −                                                         pudu                     θum uum   du   approaches asymptotically expanded                           παu                                                                     √                   √                         −      −                                                                          gnλn                           um     α −  um     α    watcher                                                         calculating moments eq obtain fol  coefﬁcient leading term called general lowing theorem  ization coefﬁcient paper given                                                        theorem   generalization coefﬁcient lnn   λ h∗m     − h∗                                                      large scale limit given                  h−h∗                                                                                                           ∗        ∗                                                               ∗            ∗    −  −                        θγh    −   γh          λ    −                                         γh                                                      πα                                          qγ                                                                                                                                                                                                   jst− κjstκ  jst −                                                      θ· indicator function event γh  hth largest eigenvalue random matrix subject                 ∗                                                         wn−h∗   −  in−h∗  ·qγ denotes ex                          −                                                         jsα−s     − cos     pectation value distribution eigenvalues           √                                                             js−    α   − αcos−   proof according theorem  difference                              √  sb ml estimators true component posi                            −  α  αs α                              −                                           −  − αcos        √           tive singular value order  furthermore gen                            αs   α  α  eralization error ml estimator component js −  regular models identiﬁabil   √                          √                                                               √                               ααsα                                                                α √  −s   −    −s  α    −    √  ity eq obtain ﬁrst term eq     αsα   cos    −α cos  αs αα  contribution ﬁrst h∗ components                                                                                         α         hand ﬁnd eq theorem  redun                                                                  −s −   −               α  dant component identifying rq− affects sb         cos                                        n−                                                         √                 estimator order   does affect                             −                                                      st max   κ −   α α  παβ  generalization coefﬁcient say general −  diagonalized matrix × matrix singular · denotes inverse function js                        Ω uΩ          Ω      Ω  value decomposed       awhere           delicate  ×  × orthogonal matrices respectively        situations  let general diagonalized matrix randd ordinary asymptotic analysis considers situations  − h∗ × −  h∗ matrix created removing ﬁrst amplitude each component true model  h∗ columns rows ﬁrst h∗ diagonal zero distinctlypositive theorem  holds  elements correspond positive true singular value situations mentioned paragraph  components d consists noises d section  important consider delicate situations                                                                    ∗  ∗  general diagonalized matrix n−rwherer true map b√a tiny nonnegligible singular values         ∗           ∗                                                 ∗  −   × −   random matrix elements  nγh ∞ theorem  holds situa                                         t                                                         −  independently subject sothatr subject tions replacing second term eq                    ∗                                               ∗        ∗  −     ∗   n−h            n−h   redundant components  regard number of√distinctlypositive true singu         −                                                            ∗−  imitate    using theorem  eq lar values γh  loss general  obtain second term eq contribution ity assume b∗a∗ nonnegative general diagonal  − h∗ components complete proof matrix diagonal elements arranged nonincreasing  theorem  qed                                   order let r∗ true submatrix created removing                                                                                               sbmip                                               sbmip                                 sbmop                                            sbmop                                     ml                                                 ml                                      bayes                                                regular                                  regular                                                                                                                                                                                                       lambda                                         lambda                                                                                                                                                                                                                                                                                                                                         sqrtn gamma              figure  generalization error                   figure  delicate true components        h∗                     b∗a∗        d                    ﬁrst   columns rows      deﬁnedin                                       sbmip  proof theorem  general diagonalized matrix                        sbmop   −                                    t                                     bayes      wherer   random matrix                                 mlregular                         ∗          ∗ ∗                               ∗  −     ∗ nr  subject n−h          n−h                   obtain following theorem                             theorem  generalization coefﬁcient lnn    general situations true map b∗a∗ delicate   lambda                             √   ∗                              singular values   nγh  ∞ given                                                                                      h         h−h∗                           ∗            ∗        ∗                                                            λh    −h        nγh        θγh                            sqrtn gamma                         hh∗                                  figure  singleoutput lnn                                                                     √      −      γ−  −      γωt nr∗ω      inconsistent proved superior         γ          γ  bh       ah                                           qr   ity bayes estimation learning method                                                        use true prior distribution suspicion cleared       γ ω    ω         ah  bh th largest singular value consideration delicate situations following  r corresponding right singular vector corre using theorem  numerically calculate sb  sponding left singular vector respectively ·qr  ml generalization error delicate situa  denotes expectation value distribution  tions true distribution near singularities fig                                                        ure  shows coefﬁcients lnn input    discussions                                         output   hidden units assump                                                        tion true map consists h∗ distinctlypositive    comparison ml estimation                component three delicate components singular val               bayes estimation                                                        ues identical each the√ null com                                                                                            ∗        ∗    ∗  figure  shows generalization coefﬁcients lnn ponent horizontal axis indicates nγ whereγh  γ   input  output  hidden units     bayes generalization error deli                                         ∗  horizontal axis indicates true rank  verti cate situations previously clariﬁed watanabe amari  cal axis indicates coefﬁcients normalized param  unfortunately singleoutput lnns  eter dimension given eq lines correspond   figure  shows coefﬁcients  generalization coefﬁcients sb approaches clariﬁed solnn input units assumption  paper ml estimation clariﬁed fuku h∗ true singular value component  mizu  bayes estimation clariﬁed aoyagi indicated horizontal axis delicateweseeinfig  watanabe  regular models respec sb approaches property similar bayes  tively results fig  calculated large estimation suppression overﬁtting entropy  scale approximation using theorem  singularities delicate situations  numerically calculated creating samples subject mip worse bayes estimation shows consis  wishart distribution using theorem  tency superiority bayes estimation  results coincide each clude typical cases suppression singularities  hardly distinguish fig  sb mip comparable stronger  approaches provide good performance bayes esti                                                             mation mip greater general solnn regarded regular model view point                                                                                                                                                                                         → ∈ Ê  ization coefﬁcient bayes estimation arbitrary h∗ ml estimation transform   makes                                                        model linear identiﬁable ml general    regular models normalized generalization coefﬁcient ization error identical regular models  equal leads penalty term akaike’s solnn property unidentiﬁable models view point  information criterion akaike                  bayesian learning methods shown fig 
