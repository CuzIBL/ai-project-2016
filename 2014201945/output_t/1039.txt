  analysis laplacian methods value function approximation mdps                                                 marek petrik                                      department science                                        university massachusetts                                            amherst ma                                             petrikcsumassedu                        abstract                          cently new framework automatically constructing                                                        basis proposed mahadevan  approach      recently method based laplacian eigenfunc  based analysis neighborhood relation      tions proposed automatically construct ba states analysis inspired spectral methods      sis value function approximation mdps  used machinelearning tasks framework      show success explained drawing section       connection spectrum lapla     following use denote value function      cian value function mdp expla denote reward vector matrix denotes      nation helps identify precisely identity matrix appropriate size use denote      ditions method requires achieve good number states mdp      performance based propose modi      important property vfa methods      ﬁcation laplacian method  guaranteed converge approximately optimal      rive analytical bound approximation er lution methods based approximate policy iteration      ror show method related  maximal distance optimal approximate value function      augmented krylov methods commonly used    vˆ optimal value function v∗ bounded munos      solve sparse linear systems finally empiri        cally demonstrate basis construction aug                                                                      ∗           γ      mented krylov methods signiﬁcantly outper         lim sup v − vˆμ ≤       sup vk − v˜kμ                                                                                 − γ                  form laplacian methods terms speed       k→∞                            quality                                      vk v˜k true approximated value step                                                        given current policy norm ·μ denotes weighted                                                        quadratic norm distribution μ distribution μ ar    introduction                                       bitrary μk depends μ current transition ma  markov decision processes mdp puterman  trix munos  similar bounds hold algorithms based  widelyused framework planning uncertainty bellman residual minimization vk − v˜k                                                                 r − − γp   ˜v        paper focus discounted inﬁnite horizon prob placed      πk   πk transition  lem discount γ  γ assume  matrix current policy addition bounds hold  ﬁnite state action spaces solving problem max norm bertsekas tsitsiklis   quires polynomial time practical problems following refer value function vk approx  large solved precisely motivated development imation v˜k v˜ respectively using index  approximate methods solving large mdps  sparse structured                                  main focus paper construction good basis    value function approximation vfa method ﬁnd algorithms minimize v − v˜ each iteration  ing approximate solutions mdps received focus quadratic approximation bound  lot attention bertsekas tsitsiklis  linear ap bounds related notice ·∞ ≤·  proximation popular vfa method simple paper organized follows section  describes  analyze use representation value function spectral methods vfa greater main  linear schemes linear combination basis vectors contribution paper explanation good  optimal policy usually calculated using approximate value formance spectral methods vfa connection  iteration approximate linear programming bertsekas methods solving sparse linear systems described  tsitsiklis                                     section  propose new alternative al    choice basis plays important role solving gorithms section  show theoretical error bounds  problem usually basis used represent space methods section  demonstrate  handcrafted using human insight topologi method signiﬁcantly outperform previously  cal properties problem sutton barto  proposed spectral methods                                                    ijcai                                                      protovalue functions                                 analysis  section greater protovalue section show use actual transition  function methods proposed mahadevan  matrix instead random walk laplacian adjacency  methods use spectral graph framework construct ba matrix method justiﬁed analyzed                                                                                                   sis linear vfa respects topology problem assume following transition matrix                                                                       states mdp ﬁxed policy represent nodes reward function available                                                                                                    undirected weighted graph ne graph   reasonable explain performance using instead                                                                                         represented symmetric adjacency matrix  past applications usually                                                                −l  wxy represents weight nodes diagonal similar transition prob  matrix node degrees denoted deﬁned lems symmetric adjacency matrix based                                                                                                     dxx          wxy deﬁnitions random policy notice eigenvectors            y∈n                                         −                         λ                  graph laplacian common normal    eigenvalue                                       −    −          − λ eigenvalue − lr  ized laplacian deﬁned  −  wd    advantage normalized laplacian symmet  spectral approximation  ric eigenvectors orthogonal use closely                                            −          assuming transition matrix ﬁxed markov  related random walk laplacian lr  −                                                        policy express value function puterman  combinatorial laplacian lc  −  com                                                          monly used motivate use laplacian making                         ∞  connection random walks graphs chung                − γp−r      γpir           function graph ne mapping each                                        vertex  welldeﬁned measure smoothness second equality follows neumann series expan  function graph sobolev norm eigen                                                      sion fact synchronous backups modiﬁed policy  vectors seen good approximation smooth eration calculate series adding term each iteration  functions functions low sobolev norm chung assume transition matrix diagonalizable                                                  analysis nondiagonalizable matrices similar    application spectral framework vfa use jordan decomposition let xn  straightforward value function seen func eigenvectors corresponding eigenvalues  tion graph nodes correspond states edge λ λn loss generality xj   weight nodes determined probability matrix diagonalizable xn  transiting way corresponding states given linearly independent decompose reward  ﬁxed policy usually weight  transition pos                                                                               n  sible way  notice weights                     symmetric unlike transition probabilities                      assume value function smooth induced graph                  spectral graph framework lead good results                                                                              cn using xj  λ xjwehave  adjacency matrix edge weights  usually used                         mahadevan  discusses schemes    n  ∞             n              n                                                                 γλ ic                      approach interesting suffers                    − γλ         problems opinion good performance                            method sufﬁciently explained                                                          considering subset eigenvectors xj basis  construction adjacency matrix motivated                                                        lead following bound approximation error  addition construction adjacency matrix makes                        method hard analyze show later using           v − v˜ ≤   dj             actual transition matrix instead adjacency matrix leads                   j∈u  better motivated algorithm easy derive  analyze                                              value function approximated    requirement value function smooth considering xj greatest dj assuming cj  partially resolved using diffusion wavelets mahade equal best choice minimize bound   van maggioni  maggioni mahadevan  consider eigenvectors high λj identical  brieﬂy diffusion wavelets construct loworder approxima taking low order eigenvectors random walk lapla  tion inverse matrix disadvantage using cian proposed spectral protovfa framework  wavelets high computational overhead needed transition matrix used instead using analysis  struct inverse approximation advantage propose new algorithm subsection   inverse constructed reused dif  ferent rewards long transition matrix ﬁxed  krylov methods  compare approach diffusion wavelets wellmotivated base choices  different goals computational complexities eigenvectors choice use vectors  methods                                              neumann series  denote vectors                                                    ijcai                                                            yi  ∈ ∞ calculating value func  require   number eigenvectors ba  tion progressively adding vectors series sis  total number vectors  modiﬁed policy iteration potentially requires let zk real eigenvectors  ﬁnite number iterations linear vfa  zk ←  methods consider linear combination basis vec  ←   tors need linearly independent vectors ikthen          ∞                                     sequence  preferable choose       zi ← pzi−  small simple calculate           end    interestingly shown just need ym− ←  −                                    represent value function degree    zi ← zi −zjzizj  minimal polynomial ipsen meyer  − γp      end  taking fewer vectors good  zi≈  choice cases show ym− vectors      break  sufﬁcient precisely represent value function let end                      pa      α ai                                minimal polynomial          let              zi ←     zi                                                                    zi                        m−                                end                          α   ai                       α                                                         figure  augmented krylov method basis construction  algebraic multiplication ba  having  value function represented             contain complex numbers require revi                                                        sion approximate policy iteration algorithms                  m−                m−        br         α   − γpir     β         issues resolved viable alternative                α                                                                      methods                                                          second algorithm propose use vectors  βi rigorous derivation augmented krylov method combine vectors  example ipsen meyer  golub loan  krylov space eigenvectors pseudo  space spanned ym− known krylov space code algorithm figure  algorithm calculates  krylov subspace denoted kp previ orthonormal basis augmented krylov space using  ously used variety numerical methods gm modiﬁed gramschmidt method  res lancoz arnoldi golub loan itis using krylov space eliminates problems non  common combine use eigenvectors krylov diagonalizable transition matrices complex eigenvectors  space known augmented krylov methods saad reason combine methods ad   methods actually subsume methods based vantage approximation properties methods intu  simply considering largest eigenvectors  dis itively krylov vectors capture shortterm behavior  cuss method subsection                    eigenvectors capture longterm behavior                                                        reliable decision rule determine right num    algorithms                                       ber augmenting eigenvectors preferable  section propose algorithms based pre number relatively low usually  vious analysis constructing good basis vfa expensive compute vectors krylov space  algorithms deal selection basis  arbitrarily incorporated algorithm based  approximate policy iteration                            approximation bounds    ﬁrst algorithm refer weighted spec section brieﬂy present theoretical error bound  tral method form basis eigenvectors guaranteed augmented krylov methods bound  transition matrix contrast method pre characterizes worstcase approximation error value  sented mahadevan  uses lapla function given basis constructed using current  cians propose choose vectors great transition matrix reward vector focus bounding  est dj value contrast choosing ones largest λj quadratic norm implies max norm  reason leads minimization bound show bound r − v˜  γpv˜ bound ap                                                  plies directly algorithms minimize bellman resid    practical implementation algorithm faces ual bertsekas tsitsiklis  implies  major obstacles issue standard eigen                                                                               −            −  vector solver efﬁciently calculate eigenvec v − v˜∞  i − γp − − γp − γp˜v∞  tors regard dj sparse matrix                                                                              ≤        r − − γp˜v   method developed future addition          − γ                transition matrix diagonalizable need calculate  jordan decomposition timeconsuming follows neumann series expansion  unstable finally eigenvectors eigenvalues verse p ∞                                                     ijcai                                                      following denote set krylov vectors pu squares projection matrix note  km  chosen set eigenvectors uwe papu − auy  invariant space  use ec denote ellipse set complex − pu  ∗ perpendicular  numbers center focal distance major semiaxis theorem follows approximation chebyshev  approximation error basis constructed cur polynomials described deﬁnition matrix  rent policy bounded following theorem states valued function approximation golub loan                   − γpxsx−  theorem  let                  diagonalizable   bound shows important choose invari  matrix let                                                                                                                  −                    ant subspace corresponds eigenvalues              φ maxxtx             x                                              x∈u ⊥x                            shows chosen highest                                                       gree perpendicular remaining eigenvectors finally         diagonal matrix  place eigenvalues bound implies approximation precision increases                eigenvectors  approximation error using basis lower γ decreases size ellipse cover    ∪        bounded                              eigenvalues                                                                          cm      cm     r − − γp˜v ≤ max   κx     φ                                                                               cm      cm             experiments                                                                                                  section demonstrate proposed methods  cm chebyshev polynomial ﬁrst kind           κxx    x−                  tworoom problem similar used mahadevan  gree                    parameters         mahadevan maggioni  problem  acd chosen ecda includes       n−u             i−γp      ec         typical representative stochastic planning problems  lower      eigenvalues             includes   encountered ai  eigenvalues                                    mdp use tworoom grid singlecell    value φ depends angle invari doorway middle wall actions  ant subspace eigenvectors  main direction use policy  perpendicular symmetric φ  equal probability taking action size each room  proof simplify notation denote − γp   cells problem size intentionally small  show standard bound approximation make explicit calculation value function possible  krylov space saad  ignoring additional eigenvec notice structure problem                                                        policy random walk laplacian identical eigenvectors  tors case objective ﬁnd ∈ km  minimizes following                              transition matrix order allows                    n               n                 evaluate impact choosing eigenvectors order                                                    proposed weighted spectral method   r − aw  r −    wia r      −wia  r                                                  used problem various reward vectors dis                                                        count factors transition structure   − notice deﬁnes polynomial                                                  reward vectors synthetically constructed show possi    multiplied constant factors determined  ble advantages disadvantages methods  let pm denote set polynomials degree               ∈p                              shown projected grid figure  vector “reward ”    satisﬁes       minimization  represents arbitrary smooth reward function vectors “re  problem expressed ﬁnding polynomial ∈                 par                              ward ” “reward ” perpendicular    minimizes       related value   eigenvectors random walk lapla                                        polynomial complex numbers golub loan cian respectively                                                           lower discount rate generally favorable krylov             pa ≤ κxmaxpλi                                              methods value closer value reward  λi eigenvalues practical bound received state seen theorem                                                                γ  obtained using chebyshev polynomials exam shrinks eigenvalues use prob  ple saad  follows                      lems generally favorable krylov methods                                                                                                    γ                                                   evaluate approach using high discount rates                                                        γ    min   max  pλi≤ min    max   pλ≤                    p∈p       p∈p                                             λ∈ecda                experiments evaluate mean squared er  ellipse ec covers eigenvalues ror mse value approximation regard num    approximation eigenvectors minimization ber vectors basis basis value function  expressed follows                          calculated based policy equiprobable                                                        action choice obtained true value function explic   min r − av˜ minmin   r − aw − auq                                 −                 w∈km q∈u                             itly evaluating − γp notice true     minmin      par  auq                       value does need used approximation         p∈pm q∈u                                       required determine approximation error compared     minmin      pai − pu  papu − auq    following  methods         p∈pm q∈u                                                        laplacian technique using eigenvectors     minpai     − pu r         p∈pm                                               random walk laplacian adjacency matrix pro                                                    ijcai                                                                               reward                 reward                 reward                                                                                                                                                                                                                                                                               reward                                           reward                                                                  reward                                                                    −                                             −                    −                                                                                                                                                                                                                                                                                                                                                                                                                                                                                figure  reward vectors projected grid                               reward                 reward                 reward                                                                                                                                                                    laplacian                                  wl                                                                                                   krylov                                                            ka                       −                      −                    −                                                                                   mse                     mse                     mse                         −                      −                    −                                                                                         −                      −                    −                                                                                                                                                                  vectors                  vectors                 vectors             figure  mean squared error each method using discount γ  mse axis log scale        posed mahadevan  results normal constructing invariant space eigenvectors using mat      ized laplacian shown practi lab standard pc took  seconds calculate       cally identical random walk laplacian     eigenvectors took  second calculate  wl  weighted spectral method described sub ﬁrst  vectors krylov space “reward ”      section  determines optimal order      eigenvectors used based value dj    discussion  krylov augmented krylov method eigen                                                        presented alternative explanation success      vectors described subsection                                                         laplacian methods used value approximation ex  ka  augmented krylov method  eigenvec planation allows precisely determine      tors proposed figure  number eigenvec method work shows meth      tors chosen running experiments ods closely related augmented krylov methods      domain                                           demonstrated limited problem set basis constructed    results γ  figure  γ  augmented krylov methods superior  figure  “reward ” intentionally violates smooth structed eigenvectors addition weighted  ness assumption easiest approximate spectral method augmented krylov method  methods laplacian case “reward ” sume value function smooth calculating  γ  weighted spectral method random vectors krylov space typically cheaper calculat  walk laplacian outperform ordinary krylov method ing eigenvectors  ﬁrst  vectors additional vectors approach statespace compression partially ob  krylov augmented krylov methods signiﬁcantly outper servable mdps pomdp uses krylov space  form                                            proposed poupart boutilier  pomdps    results suggest krylov methods offer superior generalization mdps approach practical  performance compared using eigenvectors lapla large mdps implicitly assumes number  cian methods effective observations small true mdps  sparse linear systems saad  perform number observations equivalent number states  basis construction mdp problems addition objectives approach somewhat dif  addition constructing krylov space typically faster ferent method similar                                                    ijcai                                                    
