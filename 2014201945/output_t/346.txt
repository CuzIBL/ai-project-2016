                               challenges web search engines             monika henzinger rajeev motwani craig silverstein                google department science google           bayshore parkway stanford university  bayshore parkway         mountain view ca  stanford ca  mountain view ca            monikagooglecom rajeevcsstanfordedu csilversgooglecom                          abstract                          combination thereof web ranking optimiza•                                                         tion services fee claim place given web site       article presents highlevel discussion   highly given search engine       problems unique web search en•      unfortunately spamming prevalent ev•      gines goal raise awareness stimulate ery commercial search engine measures       research areas                           identify remove spam measures qual•                                                         ity rankings suffers severely                                                            traditional research information retrieval    introduction                                         deal problem malicious content corpora                                                          quite certainly problem present benchmark   web search engines faced number difficult prob• document collections used researchers past   lems maintaining enhancing quality perfor• collections consist exclusively highquality content   mance problems unique domain newspaper scientific articles similarly spam   novel variants problems studied liter• problem present context intranets web   ature goal writing article raise awareness exists corporation   problems believe benefit increased approach deal spam problem construct   study research community deliberately ignore in• spam classifier tries label pages spam notspam   teresting difficult problems subject challenging problem best knowl•  active research earlier version paper appeared edge addressed date   henzinger et al      begin highlevel description problems                                                          content quality spam problem did exist   subsequent sections                                                          troubling issues concerned quality                                                          content web web noisy lowquality   spam users web search engines tend examine unreliable contradictory content reasonable   page search results silverstein et al silverstein approach relatively highquality content as•  et al  showed  queries sume document collection authoritative   result screen requested inclusion result accurate design techniques context tweak   screen usually shows  results lead techniques incorporate possibility lowquality   increase traffic web site exclusion means content democratic nature content creation   small fraction users actually link web leads corpus fundamentally noisy   web site commerciallyoriented web sites poor quality useful information emerges sta•  income depends traffic tistical sense designing highquality search engine   ranked  results query relevant start assumption typical document can•  content web site                               trusted isolation synthesis                                                          large number lowquality documents provides best     achieve goal web authors try deliberately                                                          set results   manipulate placement ranking order various   search engines result process commonly called step direction outlined   search engine spam paper simply refer extremely helpful web search engines able iden•  spam achieve high rankings authors use text tify quality web pages independent given user re•  based approach linkbased approach cloaking approach quest linkbased approaches instance                                                          pagerank brin page  estimating quality      work author visiting google web pages pagerank uses link struc•  work supported nsf grant iis ture web estimate page quality   research grants okawa foundation veritas better estimate quality page requires additional       invited speakers                                                                                        sources information page reading   web pages html fall middle contin•   level page different pages correlation uum structure documents close free    content                                                  text wellstructured data instead html markup pro•                                                                 vides limited structural information typically used con•                                                                 trol layout providing clues semantic information    quality evaluation evaluating quality different                                                                  layout information html limited utility    ranking algorithms notoriously difficult problem com•                                                                 especially compared information contained languages    mercial search engines benefit large amounts                                                                  like xml used tag content fact    userbehavior data use help evaluate rank•                                                                 particularly valuable source metadata unreliable cor•   ing users usually make effort explicit                                                                  pora web value layout information stems    feedback nonetheless leave implicit feedback information                                                                  fact visible user metadata    results clicked research issue                                                                  uservisible particularly sus•   exploit implicit feedback evaluate different rank•                                                                 ceptible spam techniques layout information    ing strategies                                                                  difficult use spam affecting user experi•                                                                 ence initial partly related work    web conventions creators web pages fol•      vein nestorov et al  chakrabarti et al     low simple rules anybody imposing rules    chakrabarti  believe exploitation layout    example use anchor text link pro• information lead direct dramatic improvement    vide succinct description target page    web search results    authors behave way refer rules web    conventions formalization    spam    standardization rules      search engines rely web conventions improve     web authors try deliberately manipulate place•   quality results consequently webmasters   ment rankings various search engine result•   violate conventions confuse search engines    ing pages called spam traditional information retrieval    main issue identify various conventions collections did contain spam result    evolved organically develop techniques accu•  research making search algorithms resistant    rately determining conventions violated   spam techniques web search engines hand                                                                  consistently developing improving techniques                                                                  detecting fighting spam search engine techniques    duplicate hosts web search engines try avoid crawling     developed new spam techniques developed re•   indexing duplicate nearduplicate pages   sponse search engines publish antispam tech•   add new information search results clutter  niques avoid helping spammers circumvent    results problem identifying duplicates set historical trends indicate use variety spam    crawled pages studied search engine continue increase challenging research is•   avoid crawling duplicate content place sues involved detecting spam developing rank•   gain larger general predicting page ing algorithms resistant spam current spam falls    end duplicate alreadycrawled page chancy following three broad categories text spam link spam    work problem tractable limit cloaking spammer use combina•   finding duplicate hosts hostnames serve tion    content ways duplicate hosts arise    artifact domain dns   text spam    hostnames resolve physical machine                                                                  search engines evaluate content document de•   preliminary work duplicate hosts                                                                  termine ranking search query text spam techniques   problem bharat et al                                                                   used modify text way search en•                                                                 gine rates page particularly relevant   vaguelystructured data degree structure present       modifications increase perceived relevance hu•   data strong influence techniques used     man reader document   search retrieval extreme database commu•        ways try improve ranking    nity focused highlystructured relational data concentrate small set keywords try improve   information retrieval community    perceived relevance set keywords instance   concerned essentially unstructured text documents     document author repeat keywords   late movement middle      document hoped disturb   database literature considering imposition structure user text presented small type   almoststructured data similar vein document man•  rendered invisible written pages   agement systems use accumulated metainformation intro•     background color accomplish   duce structure emergence xml led           technique try increase number key•  flurry research involving extraction imposition mainte• words document perceived relevant   nance partiallystructured data                            search engine naive approach include subset                                                                                                        invited speakers dictionary web page increase benefits link text spam inconveniencing   chances page returned obscure queries  human readers web page   naive approach add text different topic page   make appear main topic page  defending spam   example porn sites add names famous per•     general text spam defended heuristic fash•  sonalities pages order make pages appear  ion instance common sites hide text   user searches personalities                   writing white text white background ensuring                                                                  human readers affected search engines    link spam                                                  misled content result search engine   advent link analysis search engines ac•     companies detected text ignored reactive   companied effort spammers manipulate link anal•    approaches obviously optimal proactive ap•  ysis systems common approach author      proaches succeed approaches com•  link farm page site link  bined possible search engine notice   farm collection links points page pages change response launch new anti  site site author controls goal spam heuristic consider pages potential spam   manipulate systems use raw counts incoming links    pages   determine web pages importance completely        typically linkspam sites certain patterns links   linked link farm easy spot sophisticated techniques easy detect patterns mutate   like pseudo webrings random linkage member       way link spam detection techniques heuristic   group used                                      approach discovering link spam required possi•     problem link farms distract reader  bility case text spam use global   pages legitimate content   analysis web instead merely local pagelevel site  sophisticated form link farms developed    level analysis example cluster sites suddenly   called doorway pages doorway pages web pages         sprout thousands new interlinked webpages can•  consist entirely links intended viewed  didate linkspam site work kumar et al    humans constructed way makes finding small bipartite clusters web step   likely search engines discover doorway    direction   pages thousands links including multiple    cloaking discovered crawling website   links page textspam equivalent      twice using http client cloaker believes   doorway pages text unlike links analyzed    search engine client cloaker believes   search engines perpage basis                           search engine good web      link farms doorway pages effective        pages typically differ downloads legitimate rea•  link analysis sensitive absolute num•       sons changing news headlines   ber links techniques concentrate instead         interesting challenge build spam classifier   quality links pagerank brin page        reliably detects large fraction currently existing spam   brin et al  particularly vulnerable    categories   techniques                                                                   content quality    cloaking                                                   spams attempts deliberately mislead search en•  cloaking involves serving entirely different content      gines web replete text — intentionally     search engine crawler users result   — misleads human readers example   search engine deceived content page    webpage claims falsely thomas jef•  scores page ways human observer ferson president united states web•  arbitrary                                                     sites purposefully contain misleading medical infor•     cloaking used intent help search mation sites contain information cor•  engines instance giving easily digestible text rect date example sites giving names   version page heavy multi•     elected officials   media content provide linkbased access database     great deal research determin•  normally accessible forms search      ing relevance documents issue document quality   engines navigate typically cloaking  accuracy received attention   used deceive search engines allowing author achieve web search forms information retrieval in•                                                                 stance trec conference explicitly states rules        search engine crawler program downloads web pages considers document relevant does men•  purpose including search engine results typ• tion accuracy reliability document   ically search engine download number pages using understandable typical research corpora including   crawler process pages create data structures used   service search requests steps repeated continuously  study showed reputable medical sites contain contra•  ensure search engine searching uptodate dictory information different pages site berland et ai   content possible                                               — particularly difficult contentquality problem       invited speakers                                                                                                      ones used trec corporate intranets consist      approach simply collect clickthrough data    document sources deemed reliable au•     subset users — users — ranking    thoritative web course corpus tech• algorithm experimenter compute metrics    niques forjudging document quality essential generat•  percentage clicks  results number    ing good search results successful approach  clicks search    heuristically approximating quality web based    recently joachims  suggested experimen•   link analysis instance pagerank brin page  tal technique involves merging results    brin et al  hits kleinberg  tech•    ranking algorithms single result set way each    niques good start work practice user performs comparison algorithms joachims    ample room improvement                             proposes use number clicks quality metric      interesting aspect problem document quality   shows weak assumptions clickthrough   specific hypertext corpora web evaluating   ranking higher clickthrough   quality anchor text anchor text text typically retrieves relevant links   displayed underlined blue web browser    used annotate hypertext link typically webbased     web conventions   search engines benefit including anchortext analysis   scoring function craswell et al   web grown developed   little research perils anchortext analy• evolution conventions authoring web pages search   sis spam methodologies avoiding     engines assume adherence conventions improve   pitfalls                                                      search results particular three conventions                                                                  assumed relating anchor text hyperlinks meta      instance kinds lowquality pages                                                                  tags   anchor text high quality possible judge   quality anchor text independently quality rest  • discussed section  fact anchor text   page possible detect anchor text in•       meant descriptive web convention   tended editorial purely descriptive addi•     exploited scoring function search engine   tion fundamental issues remain open application                                                                    • search engines typically assume web page au•  anchor text determination document quality con•                                                                      thor includes link page   tent case documents multiple topics anchor                                                                       author believes readers source page   text analysis used identify themes                                                                       destination page interesting relevant      promising area research combine estab•                                                                      way people usually construct web pages as•  lished linkanalysis quality judgments textbased judg•                                                                      sumption usually valid promi•  ments textbased analysis instance judge                                                                       nent exceptions instance link exchange programs   quality thomas jefferson page noting ref•                                                                      web page authors agree reciprocally link   erences president united states web                                                                       order improve connectivity rankings ad•  corpus attribute role george washington                                                                       vertisement links humans adept distinguishing                                                                       links included primarily commercial purposes    quality evaluation                                                included primarily editorial purposes search                                                                       engines   search engines easily improve ranking algo•  rithms running tests compare quality          complicate matters utility link   new ranking technique old performing com•            binary function instance pages links   parisons human evaluators quite workintensive          allowing download latest version adobes   runs danger correctly reflecting user needs    acrobat reader visitors acrobat   best end users perform evaluation task        reader link useful certainly useful   know needs best                              downloaded                                                                       program similarly sites terms service      users typically reluctant direct feedback                                                                       link page user   web search engines collect implicit user feed                                                                      enters site link useful   using log data position clicks search                                                                       user browses webpages site links   time spent each click data incom•                                                                      usefulness immediately decreases   plete instance user clicks search result   search engine does know pages user visits      • web convention concerns use meta tags   user returns search engine hard   tags currently primary way include   tell user clicking page actually ends finding    metadata html theory meta tags in•  page relevant useful                                       clude arbitrary content conventions arisen     given incomplete nature information exper•        meaningful content meta tag particular impor•  imental setup used college implicit user data par•       tance search engines socalled content meta   ticularly important clickthrough          tag web page authors use content   data collected metrics computed           document convention dictates content   data                                                     meta tag contains short textual summary                                                                                                        invited speakers      page brief list keywords pertaining content  host merely domain        page                                              dns duphosts arise fact dns        abuse meta tags common          names resolve ip address companies typi•       attempt deceive break cally reserve dns increase        convention ignorance overzealous   visibility protect domain squatters        ness instance webpage author include      instance currently bikesport com bike       summary entire site meta tag  sportworld com resolve ip address        just individual page author in•   result sites httpwwwbikesportcom        clude keywords general page war•   httpwwwbikesportworldcom display identi•       rants using meta description cars sale   cal content        web page sells particular model car         unfortunately duplicate ip addresses necessary        general correctness meta tags difficult sufficient identify duplicate hosts virtual hosting        search engines analyze visible    result different sites sharing ip address round       users constrained useful     robin dns result single site having multiple ip ad•       visitors web page authors     dresses        use meta tags correctly web search en•                                                                   merely looking content small site        gines correctly judge usefulness text                                                                  homepage equally ineffective        given meta tag search results potentially                                                                  domain names resolve website homepages        improved significantly applies con•                                                                 different viewings instance        tent normally displayed alt text associated                                                                  page includes advertisement dynamic content        image tag                                                                  hand unrelated sites web      link analysis increasingly important    identical construction home page   technique webbased information retrieval                                                                    work duphosts prob•  research different types links                                                                  lem bharat et al   means solved prob•  web research try distinguish commercial                                                                                     lem difficulty solution needs   editorial links links relate metainformation                                                                  expensive bruteforce approach compares   site site best viewed start linkbrowser xcnd                                                                  pair hosts instance approach down•  link links relate actual content site                                                                  load page hosts look graph iso•     extent existing research link analysis help•                                                                 morphism defeats purpose project   ful authors highly visible web pages likely                                                                  download pages sites   contravene established web conventions clearly                                                                  duphosts   sufficient instance highly visible pages   likely include advertisements av•  furthermore web crawls complete link  erage page                                                    structure approach robust missing      understanding nature links valuable  pages specifically transient network problem problem   enables sophisticated treat• server downtime crawler crawling page   ment associated anchor text potential approach       host duphost pair likewise   use text analysis anchor text com•     increasing dynamic content web   bined metainformation url link    textbased approaches check exact duplicates   conjunction information obtained web graph        hand duphosts problem simpler                                                                  general problem detecting mirrors duphosts al•   duplicate hosts                                              gorithms advantage fact urls   web search engines try avoid crawling indexing du•      duphosts similar differing hostname com•  plicate nearduplicate pages pages increase    ponent furthermore need worry content re•  time crawl contribute new information     formatting common problem mirror sites   search results problem finding duplicate near    finally — trivial matter — duphost   duplicate pages set crawled pages studied brin analysis benefit semantic knowledge dns   et al  broder  re•      instance candidate duphosts httpfoocom   search identifying duplicate nearduplicate directory    httpfoocouk things equal   trees cho et al  called mirrors                      likely duphosts candidates http foo com     mirror detection individualpage detection try     ht tp  bar com likely duphosts   provide complete solution problem duplicate   pages simpler variant reap benefits re•  quiring computational resources simpler problem                                                                     fact necessary resolve ip ad•  called duplicate host detection duplicate hosts duphosts  dress duphosts just resolve webserver   single largest source duplicate pages web   technically necessary minimum requirement   solving duplicate hosts problem result signifi• resolve computers serve content   cantly improved web crawler                                   hostnames question       invited speakers                                                                                                      
