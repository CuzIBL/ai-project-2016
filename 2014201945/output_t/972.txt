                       semisupervised regression cotraining                                         zhihua zhou      ming li                            national laboratory novel software technology                                 nanjing university nanjing  china                                     zhouzh limlamdanjueducn                          abstract                            prominent achievement area cotraining                                                        paradigm proposed blum mitchell       practical machine learning data min  trains classiﬁers separately sufﬁcient redun      ing applications unlabeled training examples dant views attribute sets each sufﬁcient      readily available labeled ones fairly expen learning conditionally independent given      sive obtain semisupervised learn class label uses predictions each classiﬁer      ing algorithms cotraining attracted unlabeled examples augment training set      attention previous research mainly focuses    dasgupta et al  shown require      semisupervised classiﬁcation paper ment sufﬁcient redundant views met cotrained      cotraining style semisupervised regression algo classiﬁers make generalization errors maxi      rithm coreg proposed algorithm    mizing agreement unlabeled data unfortu      uses knearest neighbor regressors differ nately requirement hardly met sce      ent distance metrics each labels unla narios goldman zhou  proposed algorithm      beled data regressor label does exploit attribute partition algorithm      ing conﬁdence estimated through consulting quires using different supervised learning algorithms      inﬂuence labeling unlabeled examples partition instance space set equivalence classes      labeled ones experiments show coreg     employs cross validation technique determine      effectively exploit unlabeled data improve label unlabeled examples produce ﬁnal      regression estimates                             hypothesis requirement sufﬁcient                                                        dundant views quite strict cotraining paradigm al                                                        ready used domains statistical parsing    introduction                                       noun phrase identiﬁcation hwa et al pierce  practical machine learning data mining appli cardie sarkar steedman et al   cations web user proﬁle analysis unlabeled training noteworthy previous research mainly focuses  examples readily available labeled ones fairly ex classiﬁcation regression remains untouched  pensive obtain require human effort paper cotraining style semisupervised regression al  fore semisupervised learning methods exploit unlabeled gorithm named coreg cotraining regressors  examples addition labeled ones attracted proposed algorithm employs knearest neighbor  tention                                              knn regressors each labels unlabeled data    current semisupervised learning methods use gen during learning process order choose  erative model classiﬁer employ expectation appropriate unlabeled examples label coreg estimates  maximization em dempster et al  model labeling conﬁdence through consulting inﬂuence  label estimation parameter estimation process ex labeling unlabeled examples labeled examples  ample mixture gaussians shahshahani landgrebe ﬁnal prediction averaging regression es   mixture experts miller uyar  naive timates generated regressors coreg utilizes  bayes nigam et al  respectively used different distance metrics instead requiring sufﬁcient  generative model em used combine labeled redundant views applicability broad experi  unlabeled data classiﬁcation mental results show algorithm effectively exploit  methods using transductive inference support vec unlabeled data improve regression estimates  tor machines optimize performance speciﬁc test set rest paper organized follows section   joachims  constructing graph examples proposes coreg algorithm section  presents anal  minimum cut graph yields optimal labeling ysis algorithm section  reports experimental  unlabeled examples according certain optimization results finally section  concludes raises issues  functions blum chawla                future work  coreg                                              diverse through instantiating differ                                                        ent values setting bring proﬁt  let  · · ·  xl yl denote labeled ex usually difﬁcult decide value better  ample set xi ith instance described concerned task functions regressors  tributes yi realvalued label expected real somewhat complementary combined  valued output number labeled examples                                                                                 Ã                let denote unlabeled data set instances                     xd                                                                                                 described attributes realvalued labels minkowskypxr xs    xrl − xsl       unknown number unlabeled examples                           regressors generated each order choose appropriate unlabeled examples la  reﬁned help unlabeled exam bel labeling conﬁdence estimated  ples labeled latest version conﬁdently labeled example identiﬁed classi  gressor knn regressor dasarathy  used ﬁcation relatively straightforward mak  base learner instantiate labels ing classiﬁcations classiﬁers provide esti  new instance through averaging realvalued labels mated probability approximation classiﬁcation  knearest neighboring examples                       naive bayes classiﬁer returns maximum posteriori    use knn regressor base learner hypothesis posterior probabilities used  following considerations regressors reﬁned bp neural network classiﬁer returns thresholded classiﬁcation  each learning iterations neural networks realvalued outputs used  gression trees used each iteration regressors labeling conﬁdence estimated through consulting  retrained labeled examples addition probabilities unlabeled examples labeled dif  newly labeled ones computational load ferent classes example suppose probability                      quite heavy nn lazy learning method instance classiﬁed classes   does hold separate training phase reﬁnement  respectively instance   knn regressors efﬁciently realized second order  respectively instance conﬁdent  choose appropriate unlabeled examples label label labeled class  ing conﬁdence estimated coreg estimation unfortunately regression estimated  utilizes neighboring properties training examples probability used directly  easily coupled knn regressors  trast classiﬁcation number class labels    noteworthy initial regressors diverse predicted ﬁnite possible predictions regression  identical regressor inﬁnite key coreg mechanism  unlabeled examples labeled regressor estimating labeling conﬁdence  labeled regressor heuristically conﬁdently labeled example  algorithm degenerates selftraining nigam ghani regressor property error   single learner standard setting regressor labeled example set decrease  training use sufﬁcient redundant views enables conﬁdently labeled example utilized  learners different previous research shown words conﬁdently labeled example  natural attribute partitions makes regressor consistent  sufﬁcient redundancy attributes fairly rea labeled example set mean squared error mse  sonable attribute partition enable cotraining exhibit regressor labeled example set evaluated  advantages nigam ghani  extended ﬁrst mse regressor utilizing information  cotraining algorithm does require sufﬁcient provided xu ˆyu evaluated labeled exam  redundant views diversity learners achieved ple set xu unlabeled instance ˆyu  through using different learning algorithms goldman realvalued label generated original regressor let ∆u  zhou  coreg does assume sufﬁcient denote result subtracting mse  redundant views different learning algorithms diver mer mse note number ∆u estimated equals  sity regressors sought channels number unlabeled examples finally xu ˆyu    diversity achieved through utilizing different sociated biggest positive ∆u regarded  distance metrics fact key knn learner conﬁdently labeled example  determine distances different instances repeatedly measuring mse knn regres  minkowsky distance shown eq  usually used sor labeled example set each iteration  purpose note different concrete distance metrics timeconsuming considering knn regressor mainly uti  generated through setting different values distance lizes local information coreg employs approximation  der roughly speaking smaller order each xu coreg identiﬁes knearest neighbor  robust resulting distance metric data variations ing labeled examples uses compute mse  bigger order sensitive resulting distance let Ω denote set knearest neighboring labeled  metric data variations vicinities identiﬁed examples xu conﬁdently labeled example    given instance different using minkowsky identiﬁed through maximizing value ∆xu eq                                                                                                   distance different orders knn regressors denotes original regressor denotes                                                      regression estimates order simplify discussion    table  pseudocode describing coreg algorithm effect pool  considered blum                                                        mitchell  unlabeled examples assumed   algorithm  coreg                                    picked unlabeled example set directly   input labeled example set unlabeled example set each learning iteration coreg each unlabeled ex         number nearest neighbors                 ample xu knearest neighboring labeled examples         maximum number learning iterations       set Ω mentioned newly labeled exam         distance orders                         ple make regressor consistent   process                                             labeled data set criterion shown eq       ←  ←                                     used evaluate goodness xu origi                                                                             create pool  randomly picking examples nal regressor reﬁned xu yˆu                                                        value ∆ positive utilizing  yˆ  beneﬁcial     ← knnl ← knnl                                                repeat rounds                                                                                                                                                                       ∈                                     ∆u          yi − hxi −        yi − xi                                                                                       each xu ∈                                         xi∈l                  xi∈l           ˆyu ← hjxu                                                                                            Ω ← neighborsxu lj                       coreg  algorithm unlabeled example                                                       maximizes value ∆ picked labeled           hj ← knnlj  ∪ xu ˆyu pj                                 xu                                   ¡          ¢     fore question unlabeled example chosen           ∆   ←      −  −  −              xu                              according maximization ∆ result positive                  xi∈Ω                                                                xu        end                                      ∆u value                                                          assume  yˆ  knearest neigh        exists ∆xu                                                                                                    bors examples Ω knearest        x˜j ← arg max ∆xu  y˜j ← hj˜xj                                          xu∈u                                 neighbors examples case obvi                                               πj ← x˜j y˜j ← − πj              ous utilizing xu yˆu change regression es        πj ← ∅                                     timates examples Ω eq  eq       end                                        comparing eqs   maximiza      ←  ∪ π ← ∪ π                                                        tion ∆xu results maximization ∆u      changes exit                                                                                                                                                                                                                                          ∆u         yi − hxi −      yi − xi         ← knnl ← knnl                                         replenish  randomly picking examples        xi∈Ω                 xi∈Ω    end repeat                                         second assume xu yˆu knearest                       ∗                                 neighbors example Ω case value ∆xu   output regressor ←  hx  hx                                                        zero xu yˆu won’t chosen coreg                                                          assume xu yˆu knearest neigh  reﬁned regressor utilized information provided bors examples Ω examples                                                        −  Ω assume examples  − Ω   xu ˆyu note ˆyu  hxu                                                                                       · · ·  xm ym eq  eq         ∆        − hx  − − hx            xu                                                                                                             ∆         − hx  − − hx               xi∈Ω                                                                                                                                        ∈Ω    pseudo code coreg shown table               ³          ´    ³          ´                                                                                             function knnlj pj returns knn regressor            − hx    −   − hx                                                                                               labeled example set lj distance order pi learn q∈···m  ing process stops maximum number learning                                                          maximizing ∆   maximize ﬁrst sum term eq   erations  reached unlabeled exam       xu                                                        enable ∆ positive refer  ple capable reducing mse                                                                          second sum term unfortunately value sum term  gressors labeled example set according blum                                                        difﬁcult measured neighboring rela  mitchell ’s suggestion pool unlabeled examples                                                        tionships labeled examples  yˆ   smaller used note each iteration unla                                                                                            evaluated exist cases unla  beled example chosen won’t chosen                                                       beled example chosen according maximization ∆  extra mechanism encouraging diversity                                             xu                                                        decrease ∆  cost coreg takes using  regressors similar exam                                                               ∆    efﬁciently computed approximate  ples label each different xu                                                        ∆u experiments show cases                                                        approximation effective    analysis                                             using regressor label unlabeled  section attempts analyze learning process examples feasible unlabeled ex    coreg  use unlabeled examples improve amples chosen according maximization ∆xu considering labeled example set usually original ones example mexican hat  tains noise use regressors helpful reduce new attributes constructed   overﬁtting                                           − knn regressor built    let Λ denote subset noisy examples built each iteration each  unlabeled instance xu regressors knn regressor chooses unlabeled example maxi    identify set knearest neighboring labeled examples mizes value ∆xu eq  label regres  xu assume sets Ω Ω respectively sor ﬁnal prediction averaging regression  use different distance orders Ω Ω usually estimates reﬁned regressors knn  different Ω ∩ Λ Ω ∩ Λ usually gressor using labeled data tested baseline  different suppose xu labeled xu hxu comparison denoted labeled  hxu suffers Ω ∩ Λ knn regressors used self artre la  unlabeled instance xv close xu beled employ ndorder minkowski distance                                                                                      knearest neighbors identiﬁed similar value set  pool  used coreg  Ω xu hxu replaced previous neigh used each iteration self artre maximum  bor hxv suffer Ω ∩ Λ seriously number iterations set   hxu does instance xu labeled runs experiments carried each  xu hxu hxv suffer data set each run performance algo  Ω ∩ Λ xu close xv rithms coreg self artre labeled eval                                                        uated randomly partitioned labeledunlabeledtest splits    experiments                                        average mse each iteration recorded note  experiments performed data sets listed table  learning processes algorithms stop  “ attribute” denotes number input attributes maximum number iterations reached case  data sets used zhou et al  ﬁnal mse used computing average mse  detailed descriptions data sets note following iterations  input attributes realvalued labels improvement average mse obtained exploiting  normalized                          unlabeled examples tabulated table  com                                                        puted subtracting ﬁnal mse initial mse              table  experimental data sets           divided initial mse            data set         attribute size                                                           table  improvement average mean squared error            mexican hat                                                                        data set         self   artre   coreg            mexican hat                      friedman                                  mexican hat                      friedman                                  mexican hat                        friedman                                  friedman                          gabor                                       friedman                          multi                                       friedman                          plane                                       gabor                                 polynomial                                  multi                               sinc                                        plane                                                                                 polynomial                                                                           sinc                       each data set  data kept test set  remaining  data partitioned labeled  unlabeled sets   data used la table  shows self artre improve regres  beled examples remaining   data sion estimates ﬁve data sets coreg improves  used unlabeled examples                       data sets table  discloses im    experiments distance orders used provement coreg bigger self  knn regressors coreg set   respectively artre observations tell coreg effectively  value set  maximum number iterations exploit unlabeled examples improve regression estimates  set  pool  contains  unlabeled examples studying compared algorithms average  randomly picked unlabeled set each iteration mse different algorithms different iterations plotted    selftraining style algorithm tested comparison fig  average mse knn regressors  denoted self algorithm uses knn used coreg depicted note each subﬁg  gressor each iteration chooses unlabeled exam ure curve contains  points corresponding     ple maximizes value ∆xu eq  label learning iterations addition initial condition  cotraining style algorithm denoted  explicitly depicted better presentation  artre tested experimental data sets fig  shows coreg exploit unlabeled examples  sufﬁcient redundant views artiﬁcial improve regression estimates data sets  dundant view developed through deriving new attributes friedman  improvement plane             −                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 mean  squared error          mean  squared error           mean  squared error                                                                                                                                                                                                                                                                                                                                                               iterations                    iterations                    iterations                 mexican hat           mexican hat            friedman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 mean  squared error           mean  squared error           mean  squared error                                                                                                                                                                                                                                                                                                iterations                    iterations                    iterations                  friedman                friedman                   gabor                                                                             −                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                mean  squared error           mean  squared error                                                                        mean  squared error                                                                                                                                                                                                                                                                                                                                                                       iterations                    iterations                    iterations                     multi                     plane                   polynomial                                −                                                                                                                                                                                                              mean  squared error                                                                                                                                                                       iterations                                    sinc                       legend               figure  comparisons average mean squared error different algorithms different iterations    performance degenerated self artre ﬁnal regression estimates coreg signiﬁcantly bet  generate regression estimates ﬁve data sets fried ter artre data sets  man   multi plane average mse friedman  better furthermore  ﬁnal prediction coreg ﬁnal regression estimates coreg signiﬁcantly better  best friedman  artre slightly better self labeled data sets  plane labeled best semi plane labeled better fried  supervised learning algorithms degenerate performance man  signiﬁcant difference  observations disclose coreg apparently sults ttests conﬁrm coreg strongest  best algorithm comparison                     compared algorithms effectively exploit unla                                                        beled data improve regression estimates    pairwise twotailed ttests  signiﬁcance level  veal ﬁnal regression estimates coreg signif  icantly better initial regression estimates  conclusion  data sets plane better paper proposes cotraining style semisupervised  friedman  signiﬁcant difference gression algorithm coreg algorithm employs
