                     learning inference constrained output                  vasin punyakanok           dan roth         wentau yih          dav zimak                                      department science                                university illinois urbanachampaign                         punyakan danr yih davzimakuiucedu                          abstract                          tional random ﬁelds lafferty et al  perceptronbased                                                        learning structured output collins  carreras      study learning structured output discrimi marquez  maxmargin markov networks      native framework values output vari allow incorporating markovian assumptions output      ables estimated local classiﬁers  variables taskar et al       framework complex dependencies                                                          incorporating constraints during training lead solu      variables captured constraints dictate                                                        tions directly optimize true objective function      global labels inferred compare                                                        perform better nonetheless real world      strategies learning independent classiﬁers                                                        applications using technique show signiﬁcant ad      inference based training observing behav                                                        vantages important discover      iors different conditions experiments theo                                                        tradeoffs using each schemes      retical justiﬁcation lead conclusion using      inference based learning superior lo   paper compare three learning schemes      cal classiﬁers difﬁcult learn require ﬁrst classiﬁers learned independently learning      examples discernible difference   lo second inference used maintain struc      observed                                  tural consistency learning learning plus inference                                                        li ﬁnally inference used learning pa                                                        rameters classiﬁer inference based training ibt    introduction                                       semantic role labeling srl observed punyakanok  making decisions real world problems involves assigning et al  carreras marquez  lo  values sets variables complex expressive cal classiﬁcation problems easy learn li outperforms  structure inﬂuence dictate assignments ibt using reduced feature space  possible example task identifying named enti problem longer locally separable ibt  ties sentence prediction governed constraints like come poor local classiﬁcations yield accurate global  “entities overlap” example exists scene classiﬁcations  terpretation tasks predictions respect constraints section  provides formal deﬁnition problem  arise nature data task example section  compare three learning  “humans arms legs head”       schemes using online perceptron algorithm applied    exist three fundamentally different solutions three settings collins  details three set  learning classiﬁers structured output ﬁrst tings use linear representation li ibt  structure ignored local classiﬁers learned used share decision function space conjectures  predict each output component separately sec relative performance different schemes pre  ond learning decoupled task maintaining struc sented section  despite fact ibt pow  tured output estimators used produce global erful technique section  provide experiment  consistent structural constraints shows li outperform ibt exist accu  learned each output variable separately discrimina rate local classiﬁers depend structure  tive hmm conditional models punyakanok roth  examples learn complex structural depen  mccallum  et al  dynamic programming  dencies theoretically justiﬁed section   based schemes used context sequential predictions  fall category class solutions  background  corporates dependencies variables learn  ing process directly induce estimators optimize structured output classiﬁcation problems ﬂavors  global performance measure traditionally solutions paper focus problems natural  generative recent developments pro split task smaller classiﬁcation tasks  duced discriminative models type including condi solve directly single task section  consider thesemantic rolelabeling problem input nat  learning  ural language features output position present ways learn scoring function pa  type semanticrole sentence problem rameters differing structurebased  learn set local functions “is ference process leveraged during training learning  phrase argument ’run’” global classiﬁer pre                                                        sists choosing function  ∗ → y∗ hy  dict semanticroles addition natural structural                                                        pothesis space typically data supplied set  constraints dictate example semantic roles                                                                   distribution  single verb overlap structural constraints                                                                                        ∗ × y∗ concepts general fo  linguistic constraints yield restricted output space cus online learning linear representations using variant  classiﬁers operate                         perceptron algorithm collins     general given assignment  nx collec                                   ∈                    learning local classiﬁers learning standalone local  tion input variables        struc                                       nx              classiﬁers straightforward setting  tured classiﬁcation problem involves identifying “best”                                                        knowledge inference procedure used  assignment ∈  yny collection output variables                                                        each example ∈ learning algorithm en         yn  consistent deﬁned struc                                                     sure          ture structure thought constraining    yt                                                                                       figure online perceptronstyle algorithm  output space smaller space cyny  ⊆ yny          ∗      ∗                                                                                                                                           presented global constraints used har     →    constrains output space structurally peled et al  details section  experiments  consistent                                             learning global classiﬁers seek train classiﬁers    paper structured output classiﬁer function produce correct global classiﬁcation        nx       ny       →     uses global scoring function end key difference learning locally feed       nx    ny     ×   →  ir assign scores each possible ex inference process determines classiﬁers  amplelabel pair given input hoped correct modify classiﬁers inference  output achieves highest score consistent outputs procedure yield desired result collins               yˆ  hx  argmax fx           carreras marquez  train according global                         y∈cyny                      criterion algorithm presented online proce                                                        dure each step subset classiﬁers  nx ny depend example hand addition dated according inference feedback figure  view global scoring function composition set details perceptronlike algorithm learning infer                                                nx  local scoring functions fyx ty∈y  fy  × ence feedback       ny → ir each function represents score note practice common problems mod  conﬁdence output variable yt takes value      eled way local classiﬁers dependent                                ny                      output input sort interaction                                                        incorporated directly algorithm learning global            fx     yny   fyt                               xt                     classiﬁer long appropriate inference process used                                                        addition provide fair comparison lo li    inference task determining optimal assign ibp setting care ensure  ment given assignment sequential structure learning algorithms appropriate task order  constraints polynomialtime algorithms viterbi remain focused problem training  cscl punyakanok roth  typically used inference feedback experiments analysis presented  efﬁcient inference general structure constraints concern local classiﬁers interaction  generic search method beam search ap  plied recently integer programming shown  effective inference approach nlp applica  conjectures  tions roth yih  punyakanok et al   section investigate relative performance    paper consider classiﬁers linear rep classiﬁer systems learned inference feed  resentation linear local classiﬁers linear functions competing factors initially lo                                  dy  fyx  α  · Φ α ∈ ir   weight  cal classiﬁcation problems “easy” likely  vector Φyx ∈ irdy feature vector learning local classiﬁers lo yield accu  easy show global scoring function rate classiﬁers accurate model structural  written familiar form fx  α · Φx constraints additionally increase performance learning                                          yt                            plus inference li local problems  Φ    Φ tiyty accumula  tion outputp variables features occurring class difﬁcult learn accurate model structure  α   α     αy concatenation αy’s important overcome suboptimal lo  Φx  Φx     Φyx concatenation cal classiﬁers despite existence global solution  Φyx y’s global classiﬁer      local classiﬁcation problems increasingly difﬁ                                                        cult unlikely structure based inference ﬁx poor            hx  yˆ  argmax α · Φx            classiﬁers learned locally case training                       y∈cyny                        ference feedback ibt expected perform                                                      claim  ﬁxed number examples           algorithm onlinelocallearning                       xy     ∗    ∗                    local classiﬁcation tasks separable li               input    ∈ ×                        outperforms ibt               output fy   ∈                           y∈y                             task globally separable locally sepa                                            initialize αy ∈ irΦ  ∈             rable ibt outperforms li sufﬁcient ex               repeat converge                        amples number correlates degree                 each ∈ dxy                  separability local classiﬁers                         ny                                αy                        yˆt  argmaxy · Φ              experiments                     yˆt  yt                                                  present experiments show relative performance                       α  α  Φ                                                  learning plus inference li compares inference based                       αˆt  αˆt − Φˆt                                                        training ibt quality local classiﬁers                                                        training data varies               inference feedback                                                          synthetic data                                                        experiment each example set points          algorithm onlinegloballearning                                                                                    xy     ∗    ∗                   dimensional real space      xc ∈ ir ×              input   ∈  ×                                         output fy   ∈                           × ir label sequence binary variable                          y∈y                                                                                                         yc ∈    labeled according              initialize α ∈ irΦ              repeat converge                               xy                           hx  argmax    yifixi −  − yifixi               each ∈                                                                                                             y∈cy  xi                 yˆ  argmax    ny α · Φx                           y∈cy                                                                  yˆ                         cy  subset   imposing random                                                                                α  α  Φx − Φx yˆ           straint fixi  wixi θi each fi corresponds                                                          local classiﬁer yi  gixi  ifixi clearly dataset                inference feedback             generated hypothesis globally linearly separable                                                        vary difﬁculty local classiﬁcation generate ex                                                        amples various degree linear separability lo  figure  algorithms learning infer cal classiﬁers controlling fraction κ data  ence feedback key difference lies inference step hx  gx  gx     gcxc—examples la  argmax inference learning locally trivial bels generated local classiﬁers independently violate  prediction simply considering each la constraints gx ∈ cyc  bel locally learning globally uses global inference figure  compares performance different learning  argmaxy∈cyny  predict global labels            strategies relative number training examples used                                                        experiments   true hypothesis picked ran                                                        dom cyc random subset half size    ﬁrst attempt formalize difﬁculty classiﬁca yc training halted cycle complete errors  tion tasks deﬁne separability learnability classi  cycles reached performance averaged  ﬁer ∈ globally separates data set iff exam  trials figure shows locally linearly separable case                                              ples ∈ fx  fx  ∈  li outperforms ibt figure shows results  locally separates iff examples ∈ case difﬁcult local classiﬁcation tasksκ                                                ny  fyt  fyx ∈  yt ∈  ibt outperforms li figure shows case  learning algorithm function data sets data totally locally linearly separableκ    say globally locally learnable case li outperforms ibt number train  exists ∈ globally locally separates ing examples small cases inference helps    following simple relationships exist local  global learning  local separability implies global separa  realworld data  bility inverse true  local separability implies section present experiments realworld  local global learnability  global separability implies problems natural language processing – semantic role  global learnability local learnability result labeling noun phrase identiﬁcation  clear exist learning algorithms learn global sep  arations given examples ibt outperform semanticrole labeling  li learning examples limited semantic role labeling srl believed important  cause expensive label learning task natural language understanding imme  algorithms simply scale examples diate applications tasks information extraction  ﬁxed number examples li outperform ibt                                                           total possible output labels c· ﬁxes random                                                        fraction legitimate global labels                                                                                                                                                                                                                 accuracy                                                                                                                                      lo                                   li                                                        ibt                                                                                   lo                      training examples                                                                                                li                                                                                                ibt                                                                               κ                                                                                                                                                                                                                             separability  features                                                      figure  results semanticrole labeling srl problem                                                        number features increases difﬁculty local classiﬁ                                                        cation problem easier independently learned clas                                                     siﬁers lo perform especially inference used                                                        learning li using inference during training ibt aid perfor                                                    mance learning problem difﬁcult features          accuracy                                         lo                                    li                  amloc                                       ibt              represents leaver represents thing left rep                                    resents benefactor amloc adjunct indicating lo                      training examples                cation action determines verb                                                          model problem  using classiﬁers map                   κ                                                   stituent candidates  different types                                                        fao  fa kingsbury palmer  carreras                                                       marquez  local multiclass decisions                                                        sufﬁcient structural constraints necessary ensure                                                     example arguments overlap embed each                                                        order include structural linguistic constraints                                                        use general inference procedure based integer linear                                                     programming punyakanok et al  use data pro              accuracy                                      vided conll shared task carreras marquez                                                                                                lo                 restrict focus sentences greater                                       li              ﬁve arguments addition simplify problem                                       ibt              assume boundaries constituents given – task                                             mainly assign argument types                      training examples                  experiments clearly show ibt outperforms lo                                                        cally learned lo li local classiﬁers                    κ                      separable difﬁcult learn difﬁculty local learn                                                        ing controlled varying number input features  figure  comparison different learning strategies various features linear classiﬁer expressive  degrees difﬁculties local classiﬁers κ   implies locally learn effectively li outperforms ibt  linearly separability higher κ indicates harder local classiﬁcation features problem difﬁcult ibt outper                                                        forms li figure   question answering goal identify each verb noun phrase labeling  sentence constituents ﬁll semantic role noun phrase identiﬁcation involves identiﬁcation  determine argument types agent patient phrases words participate syntactic relation  instrument adjuncts locative temporal ship speciﬁcally use standard base noun phrases  manner example given sentence “ left pearls np data set ramshaw marcus  taken  daughterinlaw will” goal identify dif wall street journal corpus penn treebank marcus et  ferent arguments verb left yields output al       left  pearls daughterinlaw phrase identiﬁer consists classiﬁers                                                     begin deﬁning growth function measure                                                        effective size hypothesis space                                                        deﬁnition  growth function given hypothesis                                                        class consisting functions  → growth func                                                        tion nhm counts maximum number ways label                                                      data set size                                                           nhm       sup    hx     hxm ∈                                                                                                                                             xxm∈x                                                          wellknown vcstyle generalization bound expresses                                                      expected error  best hypothesis hopt unseen data                                                        following theorem adapted anthony bartlett                                           li                                           ibt          theorem  directly write growth function                                                           bound                                                     separability  features               theorem  suppose set functions set                                                         set growth function nhm let hopt ∈  figure  results noun phrase np identiﬁcation prob hypothesis minimizes sample error sample  lem                                                  size drawn unknown ﬁxed probability distri                                                        bution probability  − δ    detects beginning second detects end           logn   logδ                                                                                                        phrase outcome classiﬁers com  ≤ opt                           bined way satisﬁes structural constraints constraints                      nonoverlapping using efﬁcient constraint satisfac simplicity ﬁrst setting sep  tion mechanism makes use conﬁdence clas arate function learned each ﬁxed number  siﬁers’ outcomes punyakanok roth          output variables section  each example                                                                                                           case li trains each classiﬁer independently components input      xc ∈ ir ×    × ir                                                                                        during evaluation inference used output      yc ∈     hand ibt incorporates inference training given dataset aim learn set linear scor                                                                                                   each sentence each word position processed clas ing functions fixi  wixi wi ∈ ir each  siﬁers outcomes used inference process       lo li setting simple ﬁnd  infer ﬁnal prediction classiﬁers updated set weight vectors each component satisfy  based ﬁnal prediction prediction yiwixi   examples ∈ ibt ﬁnd  fore inference                                                                                                                                   set classiﬁers yiwixi  yiwixi                                                                                                  previous experiment figure  shows perfor  satisfy constraintsp ∈pcy   mance systems varied number features previously noted learning local classiﬁers inde  like previous experiment number features each pendently lo li guarantee convergence  experiment determined frequency occurrence each local problem separable –  frequent features pruned make task difﬁ case global constraints render problems insepa  cult results similar srl task rable lower bound opt optimal  problem difﬁcult ibt outperforms li     error achievable each component separate learning                                                        problem generalization error    bound prediction                                   corollary  set separating hyperplanes                                                             section use standard vcstyle generalization ir   bounds learning theory gain intuition learn                                                                          logemd  logδ  ing locally lo li outperform learning globally    ≤ opt                                  ibt comparing expressivity complexity each                         hypothesis space learning globally possible                                                        proof sketch show ≤ emdd   learn concepts difﬁcult learn locally                                                                         class threshold linear functions dimensions  global constraints available local algorithms                                                        nhm   precisely maximum number continuous  hand global hypothesis space                                    expressive substantially larger representation regions arrangement halfspaces ir                                                               m−                   develop bounds—both linear classiﬁers   ≤  em −     result  stricted problem ﬁrst upper bounds generalization holdsp see  anthony bartlett theorem   error learning locally assuming various degrees sep details  arability second provides improved generalization hand learning collectively ibt  bound globally learned classiﬁers assuming separabil examples consist vector ∈ ircd set  ity expressive global hypothesis space   ting convergence guaranteed course function
