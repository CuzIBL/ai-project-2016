                          kind    graphical   model    brain                                            geoffrey  hinton                                canadian  institute advanced research                            department    science university toronto                                            kings college  road                                         toronto canada  ms                                            hintoncstorontoedu                        abstract                          recent work collaboration simon osindero yee                                                        whye  teh combines tricks surprising way       neurons treated latent variables vi learn hybrid generative model Ô¨Årst proposed       sual systems nonlinear denselyconnected   yeewhye  teh model layers form       graphical models containing billions variables undirected associative memory remaining layers form       thousands billions parameters cur    directed acyclic graph converts representation       rent algorithms difÔ¨Åculty learning  associative memory observable variables       graphical model scale starting  pixels image addition working hybrid       algorithm difÔ¨Åculty learning  model nice features       thousand parameters series       progressively better learning algorithms  learning Ô¨Ånds fairly good model quickly       designed run neuronlike hard        deep directed networks millions parameters       ware latest member series learn      hidden layers optimal performance       deep multilayer belief nets quite rapidly turns slower Ô¨Ånetuning phase required       generic network three hidden layers      learning algorithm builds generative model        million connections good genera      data makes easy distributed       tive model handwritten digits learning     representations deeper layers mind       model gives classiÔ¨Åcation performance       comparable best discriminative methods      inference required forming percept fast                                                             accurate                                                           learning algorithm unsupervised labeled    introduction                                            data learns model generates label  perceptual systems make sense visual input using data  neural network contains  synapses  learning local adjustments synapse strength  debate perceptual abilities depend states presynaptic post  attributed million generations blind evo synaptic neuron  lution million seconds visual experi                                                           communication simple neurons need  ence evolutionary search suffers information bottle                                                             communicate stochastic binary states  neck Ô¨Åtness scalar bet main  contribution evolution endow learning section  describes simple learning algorithm undi  algorithm make use highdimensional gradient rected denselyconnected networks composed stochastic  vectors vectors provide millions bits informa binary variables unobserved section   tion second allowing perform larger shows make simple algorithm efÔ¨Åcient  search lifetime evolution perform stricting architecture network section  introduces  entire evolutionary history                          idea variational approximations learning directed    magic learning algorithm graphical models correct inference intractable  volved attempts answer question using undirected describes ‚Äúwakesleep‚Äù algorithm makes use  graphical models hinton sejnowski  directed variational approximation multilayer directed network  graphical models hinton et al  graphical model stochastic binary variables sections  rumelhart et al  attempts failed safely ignored people familiar ideas  scientiÔ¨Åc theories brain learns sim section  introduces novel idea ‚Äúcomplementary‚Äù  ply work produced prior complementary priors probable father  neat tricks learning undirected models christmas deÔ¨Ånition exactly cancel ‚Äúex  learning directed models paper plaining away‚Äù phenomenon makes inference difÔ¨Åcult indirected models section  includes simple example  complementary prior shows equivalence  stricted boltzmann machines inÔ¨Ånite directed networks  tied weights    section  introduces fast greedy learning algorithm  constructing multilayer directed networks layer  time using variational bound shows each new  layer added overall generative model improves  greedy algorithm resembles boosting repeated use  ‚Äúweak‚Äù learner instead reweighting each data  vector ensure step learns new  rerepresents curiously weak learner used  construct deep directed nets undirected graphical  model    section  shows weights produced efÔ¨Åcient  greedy algorithm Ô¨Ånetuned using ‚Äúupdown‚Äù al figure  boltzmann machine composed stochastic  gorithm contrastive version wakesleep al binary units symmetric connections data  gorithm                                              clamped visible units simple stochastic updating    section  shows pattern recognition performance rule infers conÔ¨Åguration states hidden units  network three hidden layers  million good interpretation data data clamped  weights standard mnist set handwritten digits updating rule used units network  knowledge geometry provided generates visible vectors model  special preprocessing generalization performance  network  errors ofÔ¨Åcial test set beats                                                               Œ±Œ≤   achieved best backpropagation nets si binary state unit global binary  handcrafted particular application conÔ¨Åguration Œ±Œ≤ units chosen random  quite close   achieved best support binary states updated using stochastic activation rule  vector machines                                      eq  boltzmann machine eventually converge    finally section  shows happens mind stationary probability distribution probability  model running constrained visual Ô¨Ånding global state determined energy  input network generative model easy state relative energies global states  look mind ‚Äì simply generate image                                                                                       Œ±Œ≤  highlevel representations                                           Œ±Œ≤      exp‚àíe                                                                                         Œ≥Œ¥                 paper consider nets composed                     Œ≥Œ¥ exp‚àíe    stochastic binary variables ideas generalized  models log probability variable sum conÔ¨Ågurationsp hidden units  additive function states directlyconnected neigh probability thermal equilibrium Ô¨Ånding visible  bours                                                units conÔ¨Åguration Œ±                                                                                 exp‚àíeŒ±Œ≤     boltzmann   machine   learning  algorithm                    Œ±    Œ≤                                                                                                         exp‚àíeŒ≥Œ¥  boltzmann machine hinton sejnowski  net                 pŒ≥Œ¥  work stochastic binary units symmetric connections boltzmann machinepcan viewed generative  usually divided set ‚Äúvisible‚Äù units model assigns probability eq  each possi  datavectors clamped set hidden units ble binary state vector visible units changing  act latent variables Ô¨Ågure  each unit adopts weights biases change probability  ‚Äúon‚Äù state probability logistic function model assigns each possible visible vector  inputs receives units           model set training vectors adjusting weights                                                       biases maximize sum log probabilities nice          psi                                   feature boltzmann machines maximum likeli                       exp‚àíb  ‚àí                                        ij             hood learning rule weights simple local  bi bias unit wij thepweight sym learn locally optimal set weights collecting  metric connection weights biases sets statistics  boltzmann machine deÔ¨Åne energy function global                                                           ‚Ä¢ positive phase clamp training vector                                                   Œ±  conÔ¨Ågurations binary state vectors net using visible units repeatedly update hidden units                                                Œ≤  index conÔ¨Ågurations visible units random order using eq  distribution  conÔ¨Ågurations hidden units                         hidden conÔ¨Ågurations reached ‚Äúthermal‚Äù equi             Œ±Œ≤           Œ±Œ≤       Œ±Œ≤ Œ±Œ≤                     librium clamped datavector sample               ‚àí    bisi ‚àí     si sj wij                                    ij                           hidden state record pairs units                                repeating entire training set                             compute  sisj   average correlation       data clamped visible units     ‚Ä¢ negative phase let network run freely       visible units unclamped states updated       just way hidden units distri       bution global conÔ¨Ågurations reached equilib       rium sample states units record       pairs repeating times                                                        figure  depicts markov chain uses alternating       compute ‚àí average correlation                                                    gibbs sampling step gibbs sampling hid       tween network running freely                                                        den units layer updated parallel apply       producing samples generative model                                                        ing eq  inputs received current states     follow gradient log probability visible units layer visible units  data using simple rule                     updated parallel given current hidden states                                      ‚àí                chain initialized setting binary states visible             ‚àÜwij  sisj  ‚àí  sisj             units datavector correlations   learning rate surprising learn activities visible hidden unit measured  ing rule simple gradient log likeli Ô¨Årst update hidden units end  hood respect weight depends complicated ways chain difference correlations provides  weights backpropagation algorithm learning signal updating weight connection  dependencies computed explicitly backward  pass boltzmann machine show differ  ence local correlations positive nega superscript  indicates correlation mea  tive phases                                          sured start chain datavector clamped    unfortunately simplicity generality boltz visible units indicates correlation mea  mann machine learning algorithm come price sured steps gibbs sampling angle brack  long time network settle thermal equi ets denote expectations choice datavector  librium especially negative phase uncon stochastic updating used gibbs sampling  strained data needs highly multimodal learning rule does follow gradient log like  gradient used learning noisy lihood does closely approximate gradient  difference noisy expectations problems make function contrastive divergence difference  general form algorithm impractical large net kullbackleibler divergences hinton  intu  works hidden units                         itively necessary run chain equilibrium                                                        order data distribution systematically    restricted  boltzmann   machines               distorted model just run chain steps     contrastive  divergence  learning                  lower energy data raise energy                                                        whichever conÔ¨Åguration chain preferred data  willing restrict architecture boltzmann make model likely generate data  machine allowing connections hidden units likely generate alternatives empirical inves  positive phase longer requires settling tigation relationship maximum likelihood  datavector clamped visible units hidden units contrastive divergence learning rules  conditionally independent apply update rule carreiraperpinan hinton   eq  units time unbiased                                                           contrastive divergence learning restricted boltzmann  sample posterior distribution hidden conÔ¨Ågura                                                        machine efÔ¨Åcient practical mayraz hin  tions makes easy measure Ô¨Årst correlation                                                        ton  variations use realvalued units differ  eq                                                         ent sampling schemes described teh et al     prohibit connections visible units quite successful modeling formation  update visible units parallel given hidden pographic maps welling et al  denoising nat  conÔ¨Åguration second correlation eq  ural images roth black  appears  alternating gibbs sampling shown Ô¨Ågure  unfortu efÔ¨Åciency bought high price pos  nately need run alternating gibbs sampling sible deep multilayer nets far  long time markov chain converges equilib long reach conditional equilibrium clamped  rium distribution fortunately start markov chain datavector nets symmetric connections  data distribution learning works causal model data explained terms                                        run chain steps hinton   gives underlying causes  efÔ¨Åcient learning rule                                                           section describes simple learning algorithm                                                  ‚àÜwij  sisj  ‚àí sisj              apparently quite different type network uses directed connections learning algorithm deÔ¨Å  ciencies combined contrastive divergence  learning surprising way produce algorithm  works better signiÔ¨Åcantly similar  real brain      variational  learning  inference directed graphical models use nonlinear  distributed representations difÔ¨Åcult phe  nomenon called ‚Äúexplaining away‚Äù pearl  cre  ates dependencies hidden variables illus  trated Ô¨Ågure  radford neal neal  showed  possible use gibbs sampling perform inference cor  rectly multilayer directed networks composed  type binary stochastic units used boltzmann ma figure  simple logistic belief net containing inde  chines communication required complicated pendent rare causes highly anticorrelated  boltzmann machine addition seeing observe house jumping bias ‚àí earth  binary states ancestors descendants unit needs quake node means absence observation  probability each descendants node times likely earth  turned current states descendant‚Äôs quake node truck node jump node  cestors sample poste total input  means chance  rior distribution conÔ¨Ågurations hidden units better explanation observa  maximum  likelihood learning rule updating directed tion house jumped odds e‚àí apply  connection simple                hidden causes active wasteful                                                        turn hidden causes explain observation                  ‚àÜwji   sjsi ‚àí sÀÜi                                                                   odds happening e‚àí √ó e‚àí  e‚àí   learning rate sÀÜi probability  turned current states ancestors  need ‚Äúnegative phase‚Äù directed models yŒ± conÔ¨Åguration hidden units qyŒ±x  require awkward normalizing term shows probability sender choose use yŒ± order  denominator eq  radford neal showed logistic communicate datavector log pyŒ± cost com  belief nets somewhat easier learn boltzmann ma municating yŒ± receiver model  chines use gibbs sampling samples log pxyŒ± cost communicating receiver  posterior distribution makes tedious learn large model hidden conÔ¨Åguration yŒ± rich  deep nets                                            soon discovered better minimize different    rich zemel realised possible function eventually understood  learn belief net contained layer binary stochas suppose different hidden conÔ¨Ågurations  tic hidden units cost computing pos communication cost datavector  terior distribution sampling prohibitive sender Ô¨Çip coin decide use receiv  stead trying perform maximum likelihood learning ing datavector receiver Ô¨Ågure choices  adopted coding perspective attempted learn model available sender Ô¨Ågure  minimize description length data zemel value random bit produced coin  hinton  idea sender receiver equally good hidden conÔ¨Ågurations sender  access model instead communicating communicate additional bit random bit stream  datavector directly sender Ô¨Årst communicates hid choice conÔ¨Åguration general number extra  den conÔ¨Åguration model costs bits bits equal entropy sender‚Äôs distribution  gives receiver good idea data expect hidden conÔ¨Ågurations extra bits used  given expectations datavector communi communicate bit string need subtract  cated cheaply appears expected cost communication cost datavector  communicating datavector                                                           cx     ‚àí     qyŒ±x log pyŒ±  log pxyŒ±                                                                        Œ±     cx  ‚àí     qyŒ±x log pyŒ±  log pxyŒ±                               Œ±                                                                   ‚àí   ‚àí    qyŒ±x log qyŒ±x           shannon showed using efÔ¨Åcient block coding scheme                                                                            Œ±                     cost communicating discrete value receiver asymp        totically equal negative log probability value sender picks hidden conÔ¨Ågurations true  probability distribution agreed posterior distribution communication cost minimized  sender receiver                              equal negative log probability datavectorunder model hard sample true pos wakesleep algorithm works quite sleep  terior sender use distribution chooses phase exactly following gradient variational  communication cost goes perfectly bound result does wrong thing data  deÔ¨Åned sender example insist using fac vector generated quite different hidden conÔ¨Ågu  torial distribution states hidden units rations instead picking hidden conÔ¨Ågurations  chosen independently communication cost sticking averages conÔ¨Ågurations produce  upper bound negative log probability data vague factorial distribution gives signiÔ¨Åcant probability  der model minimizing communication cost poor conÔ¨Ågurations  push negative log probability data  make bound tighter looseness bound  complementary priors  just kullbackliebler divergence distribu  tion used sender true posterior yŒ±x phenomenon explaining away makes inference difÔ¨Å                                                        cult directed networks comforting                                      qy      ‚àí log px  cx ‚àí   qy  log  Œ±          improve parameters inference incor                               Œ±      py           rectly better Ô¨Ånd way eliminat                         Œ±               Œ±                                                      ing explaining away altogether models hidden    use approximate posterior distribution bound variables highly correlated effects visible vari  log px neal hinton  standard way ables people use directed graphical models regard  learn belief nets inference intractable jordan et impossible  al                                               logistic belief net hidden layer prior dis                                                        tribution hidden variables factorial    wakesleep algorithm                         binary states chosen independently model  simple way make use variational learning multi used generate data nonindependence posterior  layer logistic belief net use set ‚Äúrecognition‚Äù distribution created likelihood term coming  nections compute factorial approximation pos data eliminate explaining away Ô¨Årst  terior distribution layer given binary states hidden layer using extra hidden layers create ‚Äúcom  units layer hinton et al  plementary‚Äù prior exactly opposite correlations  recognition connections general likelihood term likelihood  values corresponding generative connections given term multiplied prior posterior  set recognition weights easy update generative exactly factorial pretty implausible Ô¨Ågure  weights follow gradient description cost eq   shows simple example logistic belief net repli  use datavector set states visible units cated weights priors complementary  use recognition connections compute prob hidden layer net interesting properties  ability each unit Ô¨Årst hidden layer use  probabilities pick independent binary states  inÔ¨Ånite directed model tied weights  units layer repeated each hidden layer  turn sample approximate poste generate data inÔ¨Ånite directed net start  rior given sample learning rule generative ing random conÔ¨Åguration inÔ¨Ånitely deep hidden  topdown weights given eq  ‚Äúwake‚Äù phase layer performing ancestral pass way  wakesleep algorithm                          visible variables clearly distribution    ‚Äúcorrect‚Äù way learn recognition weights visible variables exactly distri  follow derivative cost eq  recognition bution produced markov chain Ô¨Ågure  inÔ¨Å                                                        nite directed net tied weights equivalent restricted  weights affect terms affect terms         derivatives wrt terms messy boltzmann machine  comes outside log make approxima sample true posterior distribution  tion ‚Äúsleep‚Äù phase perform ancestral pass hidden layers inÔ¨Ånite directed net starting  generate sample generative model starting data vector visible units using transposed  layer pick binary states units inde weight matrices infer factorial distributions each  pendent priors pick states units each lower hidden layer turn each hidden layer sample  layer using probabilities computed applying gener factorial posterior computing factorial posterior  ative weights states layer layer exactly process starting  completed ancestral pass visible vector restricted boltzmann machine data letting settle  true hidden causes adjust recognition equilibrium exactly inference  weights better recovering hidden causes procedure used wakesleep algorithm net  states units layer               gives unbiased samples complementary prior                                                        each layer ensures posterior distribution really                  ‚àÜwij   sisj ‚àí sÀÜj             factorial                                                               sÀÜj probability turned interpret undirected model uses gibbs sampling  current states descendants                inÔ¨Ånite directed model tied weights
