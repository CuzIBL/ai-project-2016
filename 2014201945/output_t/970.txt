        state similarity based approach improving performance rl                        ∗                           sertan girgin   faruk polat    reda alhajj    middle east technical university          university calgary             global university     dept engineering          dept science        dept science     sertanpolatcengmetuedutr     girginsalhajjcpscucalgaryca         rhajjguedulb                          abstract                          ically ﬁnding subgoal states generating corre                                                        sponding options solve stolle precup       paper employs state similarity improve rein menache et al  mannor et al  simsek et al      forcement learning performance achieved   identifying frequently occurring patterns      ﬁrst identifying states similar subpolicies state action reward trajectories mcgovern       tree constructed used locat girgin et al  different instances      ing common action sequences states derived  similar subtasks probably different subgoals      possible optimal policies sequences terms state representation subgoal based meth      utilized deﬁning similarity function ods discovered treated separately      states essential reﬂecting updates sequence based methods capable combining different      actionvalue function state similar instances subtask multiple abstractions      states result experience acquired during generated each      learning applied broader context effec                                                          taa based methods try solve solution sharing prob      tiveness method demonstrated empirically                                                        lem inductively based fact states similar pat                                                        terns behavior constitute regions state space corre    introduction                                       sponding different instances similar subtasks notion                                                        state equivalence used direct mechanism  reinforcement learning rl problem faced agent solution sharing reﬂecting experience acquired  learn behavior through trialanderror interactions state similar states connections similar sub  dynamic environment gaining percepts rewards tasks established implicitly fact reduces  world taking actions affect repetition learning consequently improves perfor  realistic complex domains task agent tries mance state equivalence closely related model min  solve composed various subtasks hierar imization markov decision processes mdps var                                                 chical structure formed relations barto ious deﬁnitions exist givan et al  deﬁne equiva                      mahadevan   each subtasks repeats lence states based stochastic bisimilarity  times different regions state space states said equivalent action  stances subtask similar subtasks sequence optimal value equivalent based  identical solutions self guidance agent tion mdp homomorphism ravindran barto   learn solutions independently going through sim  extended equivalence stateaction pairs  ilar learning stages situation affects result allows reductions relations possible case  learning process negative way making difﬁcult bisimilarity furthermore applied stateaction equiva  verge optimal behavior reasonable time         lence options framework derive compact op    main reason problem lack connec tions redundancies called relativized options  tions allow share solutions simi relativized options deﬁned user instances gen  lar subtasks scattered state space pos erated automatically zinkevich balch  ad  sible way build connections use temporally ab dressed symmetries mdps used acceler  stract actions taas generalize primitive actions ate learning employing equivalence relations state  period time sutton et al  diet action pairs particular multiagent case  terich  barto mahadevan  taas permutations features corresponding various agents  included problem deﬁnition requires extensive prominent explicitly formalizing  domain knowledge difﬁcult com  plexity problem increases constructed automat paper following homomorphism notion pro                                                        pose method identify states similar subpolicies    ∗this work supported scientiﬁc technological requiring model mdp equivalence rela  research council turkey grant ehd tions show integrated rl frame                                                    ijcai                                                     work improve learning performance using col                                                                                     sΥ  lected history states actions rewards traces policy               fragments generated translated tree form  efﬁciently identify states similar subpolicy behavior                            based number common actionreward sequences                                            Υ  updates actionvalue function state                              ﬂected similar states expanding inﬂuence new  experiences proposed method treated meta figure  markov decision process homo  heuristic guide underlying rl algorithm demon morphic image  directed edges indicate state transitions  strate effectiveness approach reporting test each edge label denotes action causing transition  sults domains various versions taxi tween connected states associated expected reward  cab room doorway problems proposed  method compared rl algorithms substan                                         tial level improvement observed different test cases s Ψtr  s ∪sΥa ∪aΥ Ψ r                                                                             present performance work compares mdps Υ absorbing state single                                                        admissible action aΥ executed transitions  hierarchical rl algorithms approaches                                  different tests demonstrate applicability effec sΥ certainty surjection Ψ→ Ψ ∪sΥaΥ                                                                                                        tiveness proposed approach                    partial mdp homomorphism pmdph         rest paper organized follows section  states map sΥ stateaction pairs  brieﬂy standard rl framework discrete image block tran                                                                                                     time ﬁnite mdps deﬁne state equivalence based mdp sition behavior expected reward                                                                                                          homomorphisms show used improve der called partial homomorphic image   learning process approach similar states stateaction pairs sa sa said  identiﬁed during learning process presented sec equivalent hsahsa states                                                                                      ρ   →   tion  experimental results reported discussed equivalent exists bijection                                                        ∀a ∈    hs ahs   ρa  section  section  conclusions                                           set stateaction                                                        pairs equivalent set states equivalent    background motivation                          called equivalence classes respectively                                                        ravindran barto  proved com  section brieﬂy overview background necessary plete homomorphism optimal policy π∗  understand material introduced paper start constructed optimal policy  makes  deﬁning mdp related rl problem                possible solve mdp solving homomorphic                    s Ψtr          mdp tuple              ﬁnite set images structurally simpler easier solve                             Ψ ⊆ s×a  states ﬁnite set actions set ad general construction viable case                        Ψ×   →    missible stateaction pairs          state tran pmdphs optimal policies depend                      ∀s ∈ Ψ     s  sition function        s∈s                rewards associated absorbing states  Ψ→ reward function  rs imme   example consider deterministic mdp given  diate expected reward received action executed fig discount factor γ set  optimal                         π Ψ→                                                   state  stationary policy         mapping  policy select action given fig  deﬁnes probability selecting action par partial homomorphic image  easily deter                                        π  πs               ∗                ∗  ticular state value state policy  saand sa∗  cwherec  expected inﬁnite discounted sum rewards agent expected reward executing action aΥ absorbing                                   π                                                         gain starts state follows  particular state accordingly optimal policy select                                         π  agent takes action state andthenfollows canda different  sulting sum called value stateaction pair optimal policy example demonstrates unless  denoted qπs objective agent ﬁnd                ∗                                       reward functions chosen carefully possible  optimal policy π  maximizes state value function solve given mdp employing solutions par  states based experience form sam tial homomorphic images equivalence classes  ple sequences states actions rewards collected stateaction pairs states known used  online simulated trialanderror interactions en speed learning process let sa  vironment rl methods try ﬁnd solution mdp equivalent stateaction pairs given mdp based  approximating optimal value functions detailed discussions partial homomorphic image  surjection  various approaches kaelbling et al                                                                                                 sutton barto  barto mahadevan       hs  fsgsawheref  → ∪ Υ sur                                                                                 rl problems general inherently contain subtasks jection gs  → afss ∈ set state dependent  need solved agent subtasks repre surjections                                                             sented simpler mdps locally preserve state tran formal deﬁnition ravindran barto                                                             ∀ ∈  −  ∗     ∗      −   sition reward dynamics original mdp let      gs   π  πm  gs                                                                                                                      deﬁned fssfssfsfs                                                                                         used instead  possible  sΥ gss ags  agss aΥ                                                    ijcai                                                     suppose learning optimal policy agent se ity states based number common  lects action state takes state imme actionreward sequences                                                                                     diate reward mapped nonabsorbing state given states sletΠsi set                                                                                                hletΩ set states accessible πhistories state length iandςis  calculated                         taking action  probability transition                                                                                                                                                        σ ∈ Π  ∃σ ∈ Π  ar    ar     nonzero mapped state                  sj        σ      σ                    t ∈ Ω                             ςis                   deﬁnition   probability experienc                         Π     ing transition s t taking action a equal                   sj  taking action  sa                   rs arsa            ratio number common actionreward se        equivalent              quences πhistories s length  stateaction pairs expected reward number actionreward sequences πhistories  mapped nonabsorbing state independent                                                                                       length ςis  close  similar  ward assigned absorbing state pol terms state transition reward behavior close  icy state t ∈ Ω  sart                                       qsa          case differ considerably each note  regarded virtual experience tuple  equivalent states actionreward sequences  updated similar qs based observation pre                   a                            s     eventually deviate follow different courses subtask  tending action transitioned agent state ends result larger  state t immediate reward                                                        threshold value ςi inevitably decrease longer    method described assumes equivalence permissible measure state similarity  classes states corresponding pmdphs                                                        trary small values   ςi  known information available prior learn estimate similarity number common  ing restricted class pmdphs possible actionreward sequences high short actionreward  identify states similar each respect sequences optimal value depends  state transition reward behavior based history subtasks problem increase robustness neces  events show section experience gathered sary account actionreward sequences various  state reﬂected similar states improve lengths maximum value weighted average  performance learning                                                                                          ςis  range problem speciﬁc values kmin                                                        kmax used combine results evaluations    finding similar states                             approximately measure degree similarity  rest paper restrict attention states s denoted ςs sonceςs s calcu  direct pmdphs absorbing state lated s regarded equivalent ςs s  associated action action sets mdp given threshold value τsimilarity  likewise restricting  homomorphic image iden  set πhistories start given action  tity mapping action component stateaction tu calculation ςs s similarity stateaction pairs  plesletm     s Ψtr given mdp start  sa measured approximately  ing state sequence states actions rewards set πhistories states available ad  σ   sarrn−sn  each  vance equivalent states identiﬁed prior learn  ai  ≤   ≤   −  chosen following pol  ing general dynamics  icy π called πhistory length arσ  known advance consequently information  ara an−rn− actionreward sequence σ accessible using history observed events  restriction σ Σ ⊆ denoted σΣisthe sequence states actions taken rewards received  longest preﬁx sarri−si σ agent incrementally store πhistories length  sj ∈ Σ si ∈ Σ states kmax during learning enumerate common action  equivalent direct pmdph set images reward sequences states order calculate similarities  πhistories restricted states map nonabsorbing purpose propose auxiliary  states consequently list associated action structure called path tree stores preﬁxes action  reward sequences  property equiva reward sequences πhistories given set states  lent states leads natural approach calculate similar path tree  ne labeled rooted tree                                                         set nodes each node represents unique action     gsaaforeverys maps nonabsorbing state                                                      reward sequence a r ∈ edge    let stateaction pairs equiv                                                       label a r indicating actionreward  alent each based partial homomorphic image                                                     sequence obtained appending  uar  s ∪sΥa  ∪aΥ Ψ   s Ψtr  der direct pmdph consider πhistory state length root node represents action sequence fur                                                                                     s ξ∈s ×r  nandletΣh  ⊆  inverse image hand thermore each node holds list         tuples                                                                                     π  σΣh  sarrk−skk ≤ restriction σ stating state histories starting  Σh image σΣh denoted σΣh  obtained                                                                                                     Σ  mapping each state si counterpart  σΣh  exists πhistory σ image restriction                                                                                           hsarhsark−hsk deﬁnition σΣh  equal σΣh  σΣh σΣh                                                                                                        πhistory hs  equivalent arσ arσΣ                                                                 Σh                                                           ijcai                                                     algorithm  qlearning equivalent state update  ery s pair coexist tuples list node                                                                                            initialize arbitrarily q· ·        level ςs  compared ks κs updated    let path tree initially containing root node greater note eligibility values    repeat                                            stored nodes path tree measure occurrence      let current state                      frequencies corresponding sequences possible ex      repeat                            each step tend similarity function incorporating eligibility val         choose execute using policy derived                                                       ues calculations normalization factors     sufﬁcient exploration observe state                                                        improve quality metric result better dis     append s  observation history                                                      crimination domains high degree nondeterminism         Δqs aαr  γ maxa∈a  qs − qs                                                      likelihood trajectories taken         similar           let t state similar s           consideration order simplify discussion opted                                                       omit extension ς values calculated equiv          Δqt atαr     γ maxa  ∈a  qt at  −                                                                                 ς           τ     qt                                          alent states state pairs greater similarity         end                                      identiﬁed incorporated learning process         s                                       proposed method applied qlearning algorithm pre     terminal state                     sented alg      using observation history generate set πhistories     length kmax add      traverse update eligibility values tuples each  experiments     node prune tuples eligibility value ξthreshold                                                       applied similar state update method described sec     calculate ς determine similar states tion  qlearning compared performance dif             ς τsimilarity                               ferent rl algorithms test domains sixroom maze     clear observation history                      various versions taxi problem fig   termination condition holds                                                        behavior examined various parameter settings                                                        maximum length πhistories eligibility decay rate                                                        test cases initial qvalues set   greedy  action sequence σu ξ eligibility value σu state                                                        action selection mechanism action maximum  representing occurrence frequency incremented                     −    time new πhistory state starting action sequence value selected probability random action                                                        selected used    results  σu added path tree gradually decremented oth                                                        averaged  runs unless stated path tree  erwise πhistory  sar rk−sk added                                                        updated using eligibility decay rate ξdecay   path tree starting root node following edges ac                   ξ          cording label let nˆ denote active node path eligibility threshold threshold andinsim                                   −           ilarity calculations following parameters employed  tree initially root node ifthere                τ          node nˆ connected edge label min  max similarity                                                           fig shows total reward obtained agent  airi ξ tuple sξ incremented                                                        til goal position reached sixroom maze problem  new tuple s  added does exist nˆ                                                        equivalent state update applied qlearning sarsaλ  set new node containing tuple s   created nˆ connected node edge label algorithms based initial testing used learning rate                                                        discount factor α andγ  respectively  airi new node active node                                                        sarsaλ algorithm λ taken  state simi    each episode termination conditions                                                        larities calculated each episode expected  reaching reward peaks case nonepisodic tasks                                                        backward reﬂection received rewards sarsaλconverges  using consecutive fragments observed sequence                                                        faster compared qlearning learning curve  states actions rewards set πhistories length     max generated added path tree using agent’s task randomly chosen position  procedure described eligibility values left room goal location right room  tuples nodes tree decremented factor ing primitive actions agent neighboring   ξdecay   called eligibility decay rate tuples cells each action nondeterministic succeeds probabil  eligibility value given threshold ξthreshold ity  moves agent perpendicular desired direction  removed tree manageable size focus probability  agent receives reward   search recent frequently used actionreward se reaches goal location negative reward −  quences using generated path tree ςs s cal agent tries transport passenger predeﬁned lo  culated incrementally s ∈ traversing cations × grid world using primitive actions  breadthﬁrst order keeping arrays κs ks s adjacent squares pickup dropoff passenger  κs denotes number nodes containing sandks s action cause agent hit wallobstacle posi                                                       tion agent change movement actions non  denotes number nodes containing                                ςs s ks s  κs              deterministic  probability agent perpendic  tuple lists initially            set  ular desired direction episode ends passenger  each level tree tuple lists nodes level                                                                                           successfully transported agent receives reward   processed κs ks  incremented accord immediate negative reward − pickup dropoff  ingly processing level kmin ≤ ≤ kmaxforev actions executed incorrectly − action                                                    ijcai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ab                                                                                     reward                           reward                                                                                                                                                sarsa                    sarsa                                                     equ                           smdpq                                               sarsa equ                   equ                                                                                               cd                                                                                           episode                            episode                                                                                                                                                                                                                                                                                                                                                                                                                                           reward                        reward                       reward                                                                    er                                                                                                    sarsa                     er sarsa                                                  equ                 equ                                                                                                                                                                              episode                           episode                            episode                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        reward                                                                                       avg  tree size                                                   avg  tree size                                                                                                                                                                                                                                                                                       episode                           episode                             episode                                                                                     figure  sixroom maze  ×  taxi problems results sixroom maze problem results  ×    ×  taxi problems experience replay number state updates effect ξdecay kminmax   ×  taxi problem fh reward obtained hi average size path tree    qlearning equivalent state update indicates start symmetries effectively qlearning equivalent  ing early states learning proposed method state update performs better long run learning  effectively utilize state similarities improve perfor curves algorithms equivalent state update reveals  mance considerably convergence attained proposed method successful identifying similar states   episodes result obtained using sarsaλ leads early improvement performance  equivalent state update indistinguishable allows agent learn task efﬁciently results  qlearning equivalent state update           larger  ×  taxi problem presented fig                                                        demonstrate improvement evident    results corresponding experiments  ×  taxi                                                        complexity problem increases  problem showing total reward obtained agent  presented fig initial position taxi agent proposed idea using state similarities  location passenger destination selected ran dating similar states leads stateaction value  domly uniform probability α set andλ value updates experience known remember  taken  sarsaλ algorithm state similarities com ing past experiences reprocessing agent  puted  episodes starting th episode order peatedly experienced experienced  let agent gain experience initial path tree simi called experience replay lin  speed learn  lar sixroom maze problem sarsaλ learns faster ing process accelerating propagation rewards ex  regular qlearning smdp qlearning bradtke duff perience replay results updates experience   addition primitive actions agent select order test gain equivalent state  execute handcoded options agent date method terms learning speed simply  position predeﬁned locations using minimum fact qvalue updates compared  number steps smdp qlearning steep performance experience replay using number  learning curve initial stages utilizing abstractions updates results obtained applying methods                                                    ijcai                                                     
