                   subspace kernel nonlinear feature extraction                                         mingrui wu jason farquhar                max planck institute biological cybernetics  tubingen¨  germany                                  ﬁrstnamelastnametuebingenmpgde                          abstract                          kernel based algorithm essentially applies linear methods                                                                                                                                               mapped data φxi  example      kernel based nonlinear feature extraction kfe                                                                                          kernel principal component analysis kpca algorithm      dimensionality reduction widely used pre                                                        scholkopf¨ smola  pca used extract repre      processing step pattern classiﬁcation data                                                        sentative subspace compared traditional linear      mining tasks given positive deﬁnite kernel func                                                        approaches kernel methods powerful      tion known input data implic                                                        explore nonlinear structures data ﬂexible      itly mapped feature space usually                                                        recover linear algorithms simply using lin      high dimensionality goal kfe ﬁnd                                                        ear kernel kernel based methods      low dimensional subspace feature space                                  retains information needed    usually dimensionality high inﬁ                                                        nite helpful separating different classes data      classiﬁcation data analysis paper pro                                   pose subspace kernel based feature high dimensional space contain      extraction problem transformed kernel pa redundancy irrelevant noisy given clas      rameter learning problem key observation  siﬁcation data mining tasks case fea      projecting data low dimensional  ture extraction input space helpful                                                        classiﬁcation data mining tasks ﬁnd lower dimensional      subspace feature space parameters               used describing subspace  subspace         garded parameters kernel function  kernel based feature extraction kfe approaches                                                                                                              tween projected data current kernel proposed ﬁnd lower dimensional subspace                                                                              parameter learning methods adapted op  feature space  example kpca scholkopf¨      timize parameterized kernel function exper smola  widely used task mentioned      imental results provided validate effec essentially performs linear pca feature space                                                             tiveness proposed approach                   goal ﬁnd directions data vari                                                        ance largest                                                          paper discuss feature extraction methods    introduction                                       focus improving classiﬁcation accuracy  feature extraction dimensionality reduction widely cclass classiﬁcation problem each data point xi associ                                                                                                     used preprocessing step classiﬁcation data mining ated label yi ∈  yi yiyic   tasks extracting proper features reduce effect yik  ≤ ≤ cifxi belongs class  oth  noise remove redundant information data erwise seen kpca effective  irrelevant classiﬁcation data analysis tasks classiﬁcation problems unsupervised feature ex                                              suppose given set data points traction method ignores labels given data                    xi ∈x  ⊂r   input data input space supervised kfe algorithms pro  traditional feature extraction approaches prin posed make use input data corre  ciple component analysis pca linear discriminant sponding labels like kpca perform linear feature  analysis lda linear methods project input extraction linear dimensionality reduction feature  data xi low dimensional subspace input space  space    recently constructing nonlinear algorithms based kernel fisher discriminant analysis kfda mika  kernel methods scholkopf¨ smola  proved et al  aims ﬁnd data projection minimizing  successful given positive deﬁnite kernel function withinclass variance maximizing betweenclass    x×x    →r                  ≤  ≤                   input data         im   variance simultaneously achieving good discrimination  plicitly mapped feature space usually high  dimensionality let φ· denote map                                                            strategies constructing label yi  ≤ ≤         kxi xjφxiφxj   ≤ ≤         possible                                                    ijcai                                                    different classes efﬁcient variant kfda  subspace kernel  based qr decomposition called akdaqr proposed suppose nf dimensional subspace   xiong et al  distinct property akdaqr oon  matrix columns constitute orthog               ondc                                              scales    akdaqr number   onal basis let denote subspace spanned  features extracted ﬁxed number classes  mapped data φxi  ≤ ≤ ninf each oi    partial squares pls algorithm wold  uniquely decomposed parts contained  widely applied domain chemometrics orthogonal complement   like pca algorithm extracts features based                                                                               ⊥  variance input data pls algorithm uses      ok  ok  ok   ≤ ≤ nf  covariance inputs labels guide ex                                                                         ⊥  traction features kernel pls kpls algorithm ok ∈t ok φxi  ≤ ≤  proposed rosipal trejo                 φxi projection computed                                                    orthogonal centroid oc park park  al               oφx  oφx     gorithm linear dimensionality reduction method pre                                       serves cluster structure data algorithm                                                                    onf   given data ﬁrstly clustered projected space                                                             equation  indicates compute projection  spanned centroids clusters orthogonal                                                        φxi consider case  basis subspace computed applying qr decom                                                        subspace  implies vector  position matrix columns consist cluster                                                        expressed linear combination φxi  ≤ ≤  centroids kim et al  method applied                   ∈s                                                            vectors    nf     let denote  dimensionality reduction text classiﬁcation tasks ex     φx φx       hibits good results kernel based nonlinear extension  nf  denote                                                                   written  kernel orthogonal centroid koc algorithm pre                 xw  sented park park  incorporate label                                                                                                                    n×n  formation koc oc algorithm treats input data wik ∈ matrix combination coef  class single cluster number ﬁcients                                                                      extracted features equals number classes  nf linearly independent  method easily extended allowing clusters nf dimensional subspace spanned nf vec  each class                                           tors elements introduce subspace    paper propose subspace kernel based following lemma  nonlinear feature extraction problem trans lemma  projecting data φxi kernel  formed kernel parameter learning problem      matrix projected data computed    rest paper organized follows section                 −        propose basic idea approach formulate          zz                subspace kernel connections related methods       kwwkw−kw                   described section  section  present possible                                                                           n×n  way optimize proposed subspace kernel experimental kij ∈ kernel matrix input  results provided section  conclude paper data kij  kxi xj  section                                                        proof φxi  ≤ ≤ order calculate                                                        projection subspace spanned columns    nonlinear feature extraction kernel             xw need orthogonal basis build     parameter learning                                 follows                                                                               zt    basic idea                                                                                                                                               equation computed follows assume  mentioned given positive deﬁnite kernel im                                              φx   ≤    kz     plicitly introduces mapping given data                                −                                                                               vΛ     ≤ usually high dimensional feature space                                                                                                                ×n  projecting φxi  ≤ ≤ subspace Λ ∈ diagonal matrix eigenvalues                                                                            ×n  kernel function modiﬁed correspondingly matrix kz ∈ matrix columns  feature space changed convenience eigenvectors kz equation  leads  modiﬁed kernel function subspace kernelas                                                                            k−   tt  shown later parameters used describ                                        ing parameters corresponding subspace                                                           precisely result equation  coordinate  kernel current kernel parameter learning methods                                                        projection φxi widely literature  adapted optimize kernel function way feature extraction dimensionality reduction coordinate  ﬁnd discriminating subspace different classes used extracted features classiﬁcation  data separated following explain “kernel matrix projected data” refers ma  idea formulating aforementioned trix elements equal inner product projected data  subspace kernel                                                                                         ijcai                                                                                                      ∗ denotes componentwise product                                                                                                                     kzt                      vectors  θ  θθd                                                                                                                                                       uud  θ ∗ θuθdud  optimizing   unit matrix following equation follows               θ                                        kernel parameter margin maximization radius                                                        margin bound chapelle et al  minimization                                               kzt                      norm norm penalizer θ feature selection                                                        choosing features corresponding large  columns form orthogonal basis subspace                                                        elements optimized θ                                                           feature selection locates discriminating subspace    φxi ∈f  ≤ ≤ projections                                                        input space  similarly approaches  subspace computed                                                        use kernel parameter learning algorithms ﬁnd discrim                                                 xw                       inating subspace paper address                                                        problem feature extraction feature selectionand  xw matrix columns projections subspace want ﬁnd contained feature space  φxi  ≤ ≤                                input space     having obtained projected data xw com  pute inner product points subspace  sparse kernel learning algorithms  following                                                        subspace kernel function given  general                                                                                                             xw xw    uu                      form described each column matrix                                                                                                     xzttzx                                        nf  cf  vector feature space                                                        show kernel relates work wu et al          xzk−xz                                                       special case each column pre          xzzz−xz                       image scholkopf¨ smola  input space                                                                        ∈f                    zˆ ∈x          xxwwxxw−xxw                      each   exists vector                                                         zi  φzˆi subspace spanned          kwwkw−kw                            φzˆ φzˆ                                                                 nf                                                                             zˆ φzˆ φzˆ   used equation  second line equation  convenience let           nf   note  line equation  ﬁfth line equa zˆ  case according  subspace  tions   identical   respectively kernel function  lemma proven                                                 ˆ ˆ  ˆ − ˆ                                                                  kwx φx      zz    φx     proof tells given projection                  −                                                                                 ψzˆxk   ψzˆx            data subspace introduced computed                          zˆ  equation                                             ψ xφxzˆ     kx  zˆ kx zˆ                                                          zˆ                                   nf       let kw· · denote corresponding subspace kernel func                                                             zˆ zˆ  tion according   ∈x zˆ      subspace kernel kw· · computed wu et al  algorithm building sparse                                                        large margin classiﬁers slmc proposed builds                              −          kwx φx      zz     φx          sparse support vector machine svm vapnik                                     −                                                       ψx ww     kw      ψx     expansion vectors given integer wu                                                        et al  pointed building slmc                                                  equivalent building standard svm kernel func                                                     ψxkx  xkx xn               tion computed  slmc algorithm essentially                                                                                      empirical kernel map scholkopf¨ smola  ﬁnds dimensional subspace  spanned                                                        φzˆ φzˆ     equation  illustrates elements      nf  different classes data  subspace described serve kernel parame linearly separated  ters kw· · order ﬁnd discriminating subspace wu et al  kernel function  obtained  different classes data separated lagrange method different  turn optimize corresponding subspace kernel kw adopted kernel function  special                                                        case subspace kernel  seen    connections related work                        based general subspace kernel  useful special                                                        cases derived applications    feature selection kernel parameter       learning                                            optimizing   kw                                           weston et al  chapelle et al   kernel pa         rameter learning approaches adopted feature selection optimize based kerneltarget alignment                                                                                   problem kernel following form considered kta cristianini et al   quantity mea                                                        sure degree ﬁtness kernel given learning task              kθu vkθ   ∗ θ ∗          particular compute solving following kta                                                    ijcai                                                    maximization problem                                   similarly compute ∇aw use following                                                    equations                             k   f        max   aw                                                         c                 n×n                                             ∂k                      ∂k      w∈r              k   f k  f                          ky         yˆ     yˆ                                                                  ∂w                  ∂w                                                                              uv                  uv  · ·f denotes frobenius product ma                                                                                 nf        trices size for equally sized ∂k                 ∂k                                                                       f        xˆj     xˆj      matrices m nf       ij mijnij             ∂w                      ∂w                                                                     uv                  uv  ky ∈  rn×n gram matrix labels deﬁned                                                     wuv  ≤  ≤   ≤ ≤ nf  element                      ky  yy                          inspired   investigate compute                                                                                                                  α ∂k  α α ∈ rn arbitrary vector actually          ∈ rc×n                          ∂wuv                      label performing linear algebra straightforwardly   ≤ ≤                                                                                elements reﬂect similarities labels              ∂k    ky                                                             α       α tuβv                 ij equals belong class                    ∂wuv                             kw      ky  ’aligning’   make        β                         β  similarities data points class higher th element vector  computed  similarities points different classes                  −                                                                        β    kw      kα            maximizing aw ﬁnd subspace  points class closer each  tu uth element vector deﬁned  different classes good classiﬁcation perfor  mance expected data projected sub              kα − kwβ                   space                                                        note given α vectors β need    note subspace kernel allows apply ker                                                        computed according   respectively  nel parameter learning algorithms feature extraction                                                            α ∂k  α calculated   ≤ ≤  problem apart kta choose        ∂wuv                                                         ≤ ≤   approaches compute based          apply                                                           ∇aw  radiusmargin bound chapelle et al  sim     calculated  plicity use kta paper    gradient based algorithms used maximize      experimental results  aw        implementation use conjugate gradient  experimental settings  algorithm solve problem  compute aw uti                                              lize fact  xw xw     empirically investigate performance follow   decompose kw ky follows ing kfe algorithms classiﬁcation tasks kpls koc                                                        akdaqr proposed subspace kernel based feature                           nf                                                     extraction skfe method following scheme                            xˆixˆi               xiong et al  features extracted each kfe al                                                    gorithm input nearest neighbor nn classiﬁer                           c                                                     classiﬁcation performance test data used                            yˆjyˆj               evaluate extracted features reference                                                    port classiﬁcation results nn algorithm using                                                        input data directly kfe       ˆ                                           xi ∈  ≤ ≤ nf  denotes ith column xw mentioned cclass classiﬁcation problem                                                yˆj ∈  ≤ ≤ denotes jth column  number features nf extracted akdaqr    based equations           kocisﬁxedatc compare algorithms                                                        value nf skfe set experiments                             nf                                                       number features extracted skfe             kw ky             xˆyˆ                                               varied kpls three different values nf tried                                                best results reported kpls                                                         f  f                        proposed skfe algorithm function aw                                ˆ ˆ             k   f            xi xj           convex optimization result depends                                                initial choice good initial guess use                                                        subspaces kfe algorithms initializa    equation   computed time com tion experiments efﬁciency use koc algo        oncn       onn                        plexity           respectively   rithm compute initial  small efﬁcient computing                                                             frobenius product directly requires time complexity  values nf tried kpls                                                                                                            ijcai                                                      experiments microarray gene expression          seven text datasets trec collections adopted       data                                             tr tr tr tr la la hitech information                                                        seven datasets available table   subsection seven microarray gene datasets                                                          similar microarray gene data data used text  test various kfe methods brain tumor brain tu                                                        classiﬁcation tasks high dimensionality  mor leukemia leukemia prostate tumor dlbcl                                                        characteristic seven datasets   tumors descriptions datasets presented                                                        highly unbalanced means number data  table  shown table  typical characteristic                                                        tained different classes quite different example  datasets number data smaller                                                        tr dataset  data points contained  data dimensionality                                                        seventh class just  data points ninth class                                                          table  datasets adopted experiments ﬁrst seven each dataset randomly select half data  microarray gene datasets seven text each class form training set use remaining data  datasets each number data di test oc algorithm linear kernel  mensionality number classes provided used set experiments similarly each                                                        dataset experiment repeated independently  times              dataset  type                       average test error standard deviation             btumor gene                        runs reported table              btumor gene                        table  illustrates skfe outperforms kfe meth             leukemia gene                      ods datasets seen table              leukemia gene                    ptumor gene                      cases kfe algorithms obtain bet              dlbcl   gene                       ter performances nn algorithm raw data              tumors gene                   whilst reducing data dimensionality dramatically               tr   text                      nf  nf  cf section  choice nf                tr   text                      tr   text                       skfe compares favorably kfe               tr   text                     methods terms classiﬁcation accuracy compu               la    text                    tational cost higher problems               la    text                   hitech  text                    ported table   ghz pentium pc kpls requires                                                          seconds akdaqr takes                                                          seconds koc requires   seconds    gaussian kernel used experiments                                                        skfe takes   seconds optimization             kx xexp−γ     − x           step skfe implemented im                                                        plemented matlab    fold cross validation conducted parameter selec  tion best cross validation error rate used mea  conclusion  sure performance different algorithms experiment                                                        presented subspace kernel based nonlin  repeated  times independently results ta                                                        ear feature extraction conducted kernel parameter  ble  show mean cross validation error standard                                                        learning connections related work explained  deviation  runs                                                        particular comparison spare large margin    table  observe skfe kpls com classiﬁer slmc wu et al  illustrates useful  pare favorably kfe algorithms particular special cases derived proposed subspace ker  skfe improves results koc algorithm cases nel applications described method  koc used initialize skfe optimize subspace kernel kerneltarget alignment  seen skfe kpls competitive each kta cristianini et al  maximization ker  signiﬁcantly different judged ttest nel parameter learning approaches applied fi  leukemia leukemia dlbcl  tumors kpls   nally experimental results provided validate  better skfe brain tumor skfe outper effectiveness approach  forms kpls brain tumor prostate tumor    experiments text classiﬁcation                references                                                                           subsection investigate different kfe methods chapelle et al  chapelle vapnik bousquet  text classiﬁcation task observed mukherjee choosing multiple parameters sup  usually exist cluster structures text data oc al port vector machines machine learning –  gorithm equivalently koc algorithm linear    kernel structures used dimen cristianini et al  cristianini shawetaylor  sionality reduction text classiﬁcation tasks kim et al elisseeff kandola kerneltarget alignment   exhibits good results                         dietterich becker ghahramani editors                                                           advances neural information processing systems     available httpwwwgemssystemorg     cambridge ma  mit press                                                    ijcai                                                    
