               averagereward decentralized markov decision processes                           marek petrik                              shlomo zilberstein               department science               department science                  university massachusetts                  university massachusetts                     amherst ma                             amherst ma                      petrikcsumassedu                         shlomocsumassedu                        abstract                          problem previously studied decen                                                        tralized settings      formal analysis decentralized decision making                                                          need optimize average reward demon      thriving research area recent years                                                        strated applications including ones reinforce      producing number multiagent extensions                                                        ment learning mahadevan  decentralized robot      markov decision processes                                                        trol tangamchit et al  decentralized monitoring      work focused optimizing discounted cumu                                                        sensor networks networks rosberg       lative reward optimizing average reward                                                        hajek  problems operates      times suitable criterion formalize                                                        extended period time main objective      class problems analyze character                                                        form consistently long run common      istics showing np complete opti                                                        discounted reward criterion usually leads poor longterm      mal policies deterministic analysis lays                                                        performance domains      foundation designing optimal algorithms      experimental results standard problem   motivation using discounted reward models–even      literature illustrate applicability average reward criterion natural–is      lution techniques                                problem easier analyze solve puterman                                                         clear argument prevails decentral                                                        ized settings fact problems average reward    introduction                                       analysis simpler exhibit com  decentralized decision making uncertainty grow plex behavior initially quickly settle simple  ing area artiﬁcial intelligence addressing interaction example automaton used prove optimal  multiple decision makers different goals capabili policy pomdp regular madani   ties information sets markov decision processes mdps optimal discounted policy simple example inﬁnite  proved useful centralized decision making optimal average reward policy simple  stochastic environments leading development contains just single action  effective planning learning algorithms recently case pomdps analysis average  extensions mdps decentralized settings ward case known involved dis  veloped examples include stochastic games sgs com counted reward criterion thorough account  petitive markov decision processes filar vrieze  sues solutions arapostathis et al   decentralized partially observable markov decision pro puterman  decpomdp generalization  cesses decpomdps bernstein et al  sgs concen pomdp expect face difﬁcul  trate mainly observable domains capturing competi ties demonstrate way circumvent  tiveness players decpomdps generalize par difﬁculties focusing subclass problem–the  tially observable mdps multiagent settings modeling case observation transition independent decentralized  operative players different partial knowledge mdps becker et al  results obtain  overall situation                             class problems encouraging lay foundation    paper addresses problem averagereward studying complex models competitive settings  centralized mdps analyze class decpomdp prob main contribution paper formulation  lems dependency interaction agents analysis decmdp problem average reward  deﬁned common reward function agents oth criterion section  deﬁnes problem provides mo  erwise independent each observation tivating example section  show calculating  local mutuallyindependent states similar model gain ﬁxed policy easy ﬁnding optimal policy  previously studied becker et al  ob np complete section  propose practical algo  jective optimizing cumulative reward ﬁnitehorizon rithms solve problem addition algorithms  contrast analyze inﬁnitehorizon average reward show connection centralized average reward mdps                                                    ijcai                                                    help analyze properties optimal policies previous states action selection depends  algorithms based mathematical programming state agents policy stationaryormarkov  mulations finally section  demonstrate algo extend standard terminology structure av  rithms standard problem averagereward erage reward mdps decentralized case decmdp  objective natural results said recurrent unichain local decision pro  provide easily extensible competitive case cesses agents recurrent unichain respectively  clarity mention extensions said multichain puterman   paper                                                  given ﬁxed policy history process repre                                                                                                    ∞                                                        sented sequence random variables xtytt    average reward decmdps                            random variables xt yt represent stateaction pairs                                                        agents stage corresponding sequence  section provides formal deﬁnition problem                       ∞                                                        wards deﬁned rxtyt   seek maximize  shows certain conditions average reward                                                                              expected average reward gain deﬁned follows  expressed compactly leading compact formulation  problem model propose similar deﬁnition  gain markov policy  proposed becker et al  introduce different                                                                                                                             n−  deﬁnition original easily extensi                                                             gss  lim   es      rxtyt                                                                         n→∞           ble inﬁnitehorizon problems simplify discussion                         problem deﬁned agents deﬁni  tion results easily extended expectation subscript denotes initial value ran  arbitrary number decision makers               dom variables sπs sπs                                                        furthermore gain matrix deﬁned gss  deﬁnition  decmdp ntuple                                                        gss actual gain depends agents’ initial dis                                                                                                                                                  tributions αα calculated  α gα  •  × represents actions players  •  × set states                     problem  ﬁts average reward criterion  •  pp  transition probability each pi multiple access broadcast channel mabc rosberg  function si × si × ai → transition probabilities  ooi wornell  problem                 ss → ss given joint action aa    used widely recent studies decentralized control                                          ss ss aaps saps sa   communication devices share single channel  • rsasa ∈ joint reward function need periodically transmit data chan  • ss given initial state agents   nel transmit single message time                                                        agents send messages time inter    process deﬁned transition obser                                                        val leads collision transmission fails  vation independent deﬁned becker et al  tran                                                        memory devices limited need send  sition independence means transition state                                                        messages sooner later adapt model  agent independent states agents                                                        rosberg  particularly suitable  observation independence means observations                                                        sumes sharing local information devices  agent receives independent current states                                                          mabc problem represented decmdp  observations agents                                                        each device represented agent state space    transition independence plausible assumption                                                        each agent represents number messages buffer  domains observation independence somewhat                                                        possible actions send send  strictive prevents kind communication                                                        arriving data modeled random transitions  agents assumption suitable communication                                                        buffer states provide details formulation  prohibitively expensive risky does add                                                        optimal policy section   value process communication use  ful problem observations provides easily  computable lower bound help decide  basic properties  communicate                                          section derive basic properties dec    refer sap sap local pro mdps showing efﬁciently evaluate ﬁxed policies  cesses fact represent mdps reward sim establishing complexity ﬁnding optimal policies  plify analysis assume state set action analysis restricted stationary policies extension  set ﬁnite markov chains agents nonstationary policies mentioned section   induced policy aperiodic markov chain ape average reward mdps gain policy cal  riodic converges stationary distribution culated directly solving set linear equations  limit puterman  discuss relax semble value function equations discounted case  sumptions section                                erman  value optimality functions    policy π function maps history lo used decmdps following theorem states  cal states action each agent prevent need gain expressed using invariant limiting distribu  introducing measuretheoretic concepts assume tion limiting distribution represents average proba  policy depend ﬁnitelybounded number bility each state inﬁnite execution                                                    ijcai                                                    deﬁned                                                                                                           n−                                                                psi  lim        xt  si                                                                        n→∞                                                                                                            limiting distribution calculated initial                     distribution using limiting matrix limiting matrix tran  sition matrix deﬁned                              n−                                                                  ∗                                lim                                 n→∞                               αp ∗ α initial distribu  tion  theorem  suppose   stateaction transition                                                    figure  sketch decmdp created sat  matrices agents ﬁxed policies problem  clauses  variables  reward matrix gain matrix calculated  follows                         ∗   ∗                                             values variable instances satisfy problem    theorem follows immediately fact second assign values variables agents pe  processes aperiodic deﬁnition gain nalized assigning different value actual variable                ∗    calculating practical typically variable instance  quires calculating eigenvectors matrix let vc sat problem conjunctive  idea used calculating gain single agent mal form set variables set  case used follows                     clauses literal instantiation variable  theorem  initial distribution α transition write ∼ construct corresponding decmdp  matrix  limiting distribution fulﬁlls       following way state action sets agents                                                                         −                                      −                                α                          f∪lij ∈     fulﬁlls                                                   ∗                                                                   α                                                                                                      note denotes identity matrix appropriate                                 n  size  implies pi                                                    each state lij ∈ represents jth literal ith  space constraints include proof clause instantiation single variable states  similar derivation puterman                                                         represent variables problem actions rep    examine complexity ﬁnding optimal resent variable value true false main  policy model complexity ﬁnitehorizon dec idea make ﬁrst agent assign arbitrary values vari  mdps shown nexp complete inﬁnite able instances satisfy formula second agent                                                    horizon problems undecidable bernstein et al   determines value assigned multiple variable  model study tractable follow instances  ing theorem shows easier general problem  focus stationary policies truly lack space omit complete deﬁnition transi  optimal policy depend longer history complex tion matrices sketch reduction shown figure   ity justiﬁes use algorithms present later ﬁrst agent follows transitions depicted ac  polynomial algorithm unlikely exist    tion chosen literal true                                                        agent transitions state agents each state  theorem   problem ﬁnding optimal markov small probability   remaining  policy aperiodic decmdp npcomplete     state second agent transition probabilities  proof show ﬁnding optimal policy np independent actions taken notice process  note theorem  states optimal markov policies aperiodic  deterministic calculating gain given ﬁxed policies rewards deﬁned negative reward  possible polynomial time policy size polyno ceived value assigned ﬁrst agent  mial policies checked nondeterministic polyno stance variable different value assigned  mial time                                            variable second agent addition negative reward    np hardness shown reduction satisﬁa received ﬁrst agent transitions state notice  bility sat main idea ﬁrst agent assign happens assigned variable instance values                                                    ijcai                                                    satisfy clause                     solution  used determine opti                −   ∀  ∈       ∈      ∈         mal policy given optimal value program’s variables      fa           aa          procedure determining optimal policy    rlija  − lij ∼     solution dual mdp optimal                                                        policy randomized each recurrent state  cases reward zero                                                                                                                                                                                                     ∗                easy show gain dec mdp  πi sa                  sat problem satisﬁable                                a∈a pis                                                        is welldeﬁned recurrent states    optimal decmdp algorithms                          a∈a pis  fortransient states policy                                                        determined  section present methods ﬁnding optimal                                                                                               ∗            qis   policies decmdps methods based                                                                                                 πi                       mulating problem mathematical program                        a∈a qi  discuss additional approaches solving problem proposition  policy constructed according    previous work similar problem solved using description optimal  iterative method ﬁxes policy agent proof small variation proof opti  computing optimal policy agent nair et al mal policy construction average reward mdps   methods unlikely produce good results example puterman   averagereward case high number local shown averagereward mdp  minima contrast following milp formulation facili exists optimal deterministic policy notice  tates efﬁcient search through set local minima ﬁxed formulation equivalent                                                        dual averagereward linear program optimal    quadratic programming solution                    ∗     ∗                                                        construct deterministic policy follows                                                               ∗  section mathematical program deﬁnes fixing ﬁnd p˜ gain solving  optimal solutions reward deﬁnition based problem mdp following procedure  orem  discuss obtain optimal policy obtain deterministic p˜ following theorem  solution quadratic program           theorem  deterministic markov policy                                                      decmdp optimal gain   maximize   rp   subject ≥                                       mixed integer linear program                                                        formulation offers method used calcu             ≥                                                      late optimal policies quite efﬁciently approach gen   ∀j ∈      pj −        ps ap eralizes approach sandholm et al             a∈a        s∈s a∈a                     based gametheoretic principles works ma   ∀j ∈      pj     qj a−                 trix games approach based lagrange multipliers                                                        analysis problem extended include            a∈a       a∈a                                                        additional linear constraints                      qs ap aαj                                                          clarity derive algorithm unichain             s∈s a∈a                                                     decmdp algorithm multichain case sim   ∀ ∈                −                                              ilar derived way streamline pre            a∈a        s∈s a∈a                     sentation reformulate problem terms matrices   ∀j ∈      pj     qj a−                 follows            a∈a          a∈a                                                                                                                   maximize    rp                      qs ap aαj                                                             subject tp      tp              s∈s a∈a                                                                                                                                                                                      beneﬁt formulation compu                                                                            ≥        ≥   tational efﬁciency solution technique helps  identify key properties optimal policies later vectors ones length  use properties develop efﬁcient formulation respectively matrix representations  shown optimal                    rd th constraint                                                           mixed integer linear program milp formulation  theorem  optimal solution  optimal based application karushkuhntucker kkt  gain                                                 orem bertsekas  lagrangian formu    omit proof lack space correctness lation   constraints follows dual formulation opti lppλρμ                                                    mal average reward shown example puterman                  −       −    theorem  choice objective function      rp  λ       λ                                                                                       follows theorem                                       ρ tp  ρ tp  μ  μ                                                    ijcai                                                      let denote sets active constraints               equalities ≥  ≥  respectively following                        kkt theorem local extremum fulﬁll fol               lowing constraints                                                        figure  sample policy agents each state        −     −    −          −      −    −   rp     λe  ρ   μ      λe   ρ  μ  action represents sending sending states  pi   ∀i ∈          pi    ∀i ∈         –  μi   ∀i∈          μi    ∀i∈                                                        practical application introduced section  experimen  μi ≥  ∀i ∈          μi ≥   ∀i ∈                                                        tal component paper designed illustrate scope                                                        problems solved quickly using new algo  gain local extremum simpliﬁes            rithm                                                          mentioned earlier problem use adaptation            −      −     −     −       rp    λp  ρ  μ    λ        multiple access broadcast channel problem solved                                                                  iwehavepi∨μi      rosberg   order make problem  constraints linear problem ing assume following events occur single  directly solved known messages arrive single message buffer expires  represented binary variables bibi ∈ happens events occur independently                                                                                         determine each constraint ac ﬁxed probabilities α β γ δ   tive following milp  communication devices buffers ﬁxed             −                                          arbitrary length unlike original problem addition   maximize    λ                                       possible actions send send action   subject tp                    tp          send probabilities updated according list stated                                                action send buffer level decreased                                             transition probabilities                                         rp  −λe −  ρ − μ                  action send rosberg  derives simple                  −     −    −                     analytical solution problem applicable                 λe   ρ  μ                  extended problem             μ ≥                      μ ≥             reward associated overﬂowing             μ ≤ mb                  μ ≤ mb       buffers agents respectively agents                ≥                         ≥           decide broadcast message time collision                                                  occurs corresponding reward rewards             ≤ −               ≤ −    negative model cost losing messages                                                        reward zero             bi ∈               bi ∈                                                      ran experiments following message arrival                                                        probabilities     optimal policy                                                                               −      −      −     program sufﬁciently large constants problem rewards         show bounded maximal buffer sizes   depicted figure  op  bias puterman  policies timal policy simple structure case algorithm  aware simple method calculate bounds does rely note choosing different thresholds  practice sufﬁcient choose number close sending messages leads signiﬁcantly lower gains  largest number representable asses computational complexity approach  eliminates solutions representable using lim practicality ran experiments using basic commer  ited precision                       cially available milp solver running pc solver    optimal policy case obtained standard branchandbound algorithm using linear pro  way subsection  notice λ  λ unlike gramming bounds linear programs calculate  competitive situation eliminated bound solved simplex algorithm runs  preceding analysis leads following proposition ﬁxed transition probabilities experimented                                                        sets rewards ﬁrst  −r  −r   proposition  gain optimal policy obtained −                 −     −     −     equivalent                          second                                                                      results summarized figure  show    present formulation cooperative                                                        time required calculate optimal solution   cases easily extended competitive variant                                                        seconds problems size clear results  problem                                                        different reward sets signiﬁcant impact                                                        quickly problem solved quite common    application                                        branchandbound algorithms larger problems  section presents application averagereward beneﬁcial use branchandcut algorithms  decentralized mdp framework simple instance offer good performance large problems                                                    ijcai                                                    
