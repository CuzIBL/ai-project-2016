             learning discontinuities switching local models                  ∗                                   marc toussaint    sethu vijayakumar                                institute perception action behavior                               school informatics university edinburgh                      king’s buildings mayﬁeld road edinburgh eh jz uk                            mtoussaiinfedacuk sethuvijayakumaredacuk      introduction                                       model problem predicting par                                                        ticular model φ responsible given input solved  locally weighted learning techniques particular lwpr                                                                higher level explained section formal  vijayakumar et al   successfully used notations assume mixture model  highdimensional regression problems robustness                                                                             efﬁcient online versions crucial robotic domains                                 instance inverse model articulated dynamic robot yx   −  plocix yi     learned realtime models map high                   dimensional state joint angles velocities                       − φ                                                                   yi  √   exp  −              sired change state required motor signals torques              π σ            σ    typically mappings assumed smooth                                                              uy  real world scenarios interesting cases  uniform distribution accounting                                                                                    functions truly discontinuous ground noise outliers hidden variable spec  examples include contacts objects particular ifying particular model generates datum  ground parts body “joint limits” aim localized models impose locality constraint al                                                                                      fact interesting interactions environment man ready level follows let denote mean input                                                                             φ  ifest through discontinuities sensorimotor center model trained given                                                                 data                                                 input  th model eligible does                                                        exist jth model center “between”    paper show discontinuous switching                                                                                                 precisely  tween local regression models learned general  topic switching models discussed plocix   ⇐⇒ ∃j  hx − cj ci − cji     context state space models ghahramani hin h· ·i scalar product input space  ton  pavlovic et al  multiple inverse mod        eligible eligible    els wolpert kawato  generally question             cj          cj  particular model receives responsibilities given                               input modeled hidden variable generative              mixture model case assume responsibil       ci              ci  ity index predicted input robot state plocix uniform eligible i’s given  inferring model corresponds classifying current family models infer posterior  input domain regions each submodel         responsibility index given datum using bayes    robotic domains local learning crucial pre rule                                                                                vent interference allow online adaptation techniques    iy  yi plocix  propose model responsibility index               self composition local classiﬁers multiple pairwise clas calculating map assignment ˆi allows associate ev  siﬁers concatenated construct complete model ery training datum likely model using  form productofsigmoids capable learning sufﬁcient statistics each local model φi updated fur  complex sharply bounded domains each local model ther data labeled “yet unmodeled”  lieu typical gaussian kernels                 inferred generated uy used generate                                                        new family member following heuristic compare    learning family models                        ransac random datum  selected                                                        modeled data closest neighbors wrt eu                            given training data xk ykk inputs xk outputs clidean input distance chosen initial training data  yk ﬁrst level goal algorithm learn family new model random poisson number  models φ  φn  datum explained mean  input dimensionality finally mod                                                        els receive map responsibilities     ∗the ﬁrst author acknowledges support german research experiments discarded iterative process  foundation dfg emmy noether fellowship  repeated new models generatedfigure  kernels represented product sig  moids      general scheme family learning realized  type models φi experiments  choose φi linear functions learned partial  squares pls regression pls involving intermediate  lowerdimensional projection proven efﬁcient  highdimensional problems vijayakumar et al     products sigmoids switching  second level algorithm goal learn pre  dictive model ix latent responsibility index figure  test function σ  precise uninformed prior plocix given learned switching model  iterations  data easy decide models “po training data points blended switching model                                                                tentially neighbored”—namely exists data yx  βix φix compared lwpr family er  models eligible—based centers ror cf sec  classiﬁcation error  runs ran  each pair ij neighbored models learn sigmoidal dom test functions σ   function ψijx ψij ≡  − ψji product bold line average curves  sigmoids submodel deﬁnes coefﬁcient βix  associate submodel given input                                                           discussion                                          βix        ψijx  ψijx                      presented model addresses problem handling                                  exp−φ                                            ij         discontinuities naturally arise sensorimotor data                                                        during interaction structured environment model  normalizes β indicated represent                                                      extends earlier local learning approaches ways  sigmoids ψ scalar function φ  fig  illustrates           ij                    ij                     responsibility region associated each local model  kind kernels represented products sigmoids                                                        learned product sigmoids ver    sigmoids ψ meant represent likelihood                 ij                                     satile boundary shape compared typical gaussian kernels  model responsible input                                                        problems associated initialization kernel shapes  conditioned responsible product                                                        widths heuristic choice ad hoc number sub  combination comparable voting map la       ˆ                                                models circumvented robust incremental allocation  beling introduced previous section used new models consistently used pls  train sigmoids experiments consider φij underlying regression machinery general model allows  linear functions learned pls          utilize efﬁcient single model learner represent                                                        local models φi classiﬁer functions φij future    experiments                                        work particular investigate nonlinear learners  tested algorithm piecewise linear discontinuous local models boundary classiﬁers  test functions test function  parameters  dimension number linear pieces com references                            σ  posed output noise  localities slopes ghahramani hinton  ghahramani ge  boundaries linear pieces sampled randomly fig hinton variational learning switching statespace  ab display learning results example com models neural computation –   parison lwpr fig cd display error curves  dimensional test functions large input pavlovic et al  vladimir pavlovic james rehg  main −  family error mse best  john maccormick learning switching linear mod                                                           els human motion nips pages –   ﬁtting eligible model φˆi averaged independent test  data set classiﬁcation error counts prod vijayakumar et al  sethu vijayakumar aaron  uct sigmoids correctly predicts φˆi best ﬁtting d’souza tomohiro shibata jorg conradt ste                                       ˆ  model given input argmaxiβi  exper fan schaal statistical learning humanoid robots  iments ﬁnd algorithm reliably generates fam autonomous robot –                                              ily optimal family error noise level σ   wolpert kawato  dm wolpert kawato   dimensions displayed classiﬁcation error multiple paired forward inverse models motor  rapidly converges zero  dimensions classi trol neural networks –   ﬁcation error converges  results  homepagesinfedacukmtoussaiprojectsijcai
