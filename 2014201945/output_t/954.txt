                 sequence prediction exploiting similarity information                        istvan´ b´ıro´ zoltan´ szamonek   csaba szepesvari´            automation research institute hungarian academy sciences                                 kende  budapest  hungary                                         e¨otv¨os lor´and university                              p´azm´any s´et´any budapest  hungary                            ibirosztakihu zszamieltehu szcsabasztakihu                        abstract                          classbased ngram estimation brown et al  greedy                                                        algorithm searches space clustering ﬁnd      data scarce alphabet large     creases data likelihood method extended      smoothing probability estimates                                                                                       handle trigram ueberla   heuristic ran      escapable estimating gram models   domization used speedup clustering process      paper propose method implements form  slight loss performance      smoothing exploiting similarity information      alphabet elements idea view   early works rely  short span relationships      logconditional probability function smooth  bigram trigram tend produce      function deﬁned similarity graph al syntacticallyoriented clusters approach proposed      gorithm propose uses eigenvectors  bellegarda et al  exploits large span relation      similarity graph basis expansion ships words results clusters seman      log conditional probability function tically oriented idea exploit documents      coefﬁcients solving regularized lo thought semantically homogenous set sentences      gistic regression problem experimental results set documents available knowledge      demonstrate superiority method    source exploited create semantically meaningful      similarity graph contains relevant information wordclusters algorithm forms worddocument matrix      whilst method remains competitive  given training data performs singularvalue decomposi      stateoftheart smoothing methods lack tion svd resulting matrix clusters      information                              projections according wellchosen measure                                                          clustering special case constructing similarity infor    introduction                                       mation alphabet elements words                                                        wordforms language dagan et al  pro  statistical language models slm attempt capture                                                        posed use kernelbased smoothing technique  regularities natural language using statistical methods                                                        kernel function built using similarity information derived  construct estimates distributions various linguistic units                                                        wordcooccurrence distributions shown  morphemes words phrases sentences documents                                                         reduction perplexity unseen bigrams com  categorical nature large vocabulary                                                        pared standard backoff schemes toutanova et al   natural languages statistical techniques estimate large                                                        investigated markov chain model exploits apriorisimi  number parameters depend critically avail                                                        larity information stationary distribution used  ability large training data given complex                                                        solving prepositional phrase attachment problems  ity languages training data rarely available  lack sufﬁciently large rich dataset simple ap contribution paper propose method  proaches like straightforward maximum likelihood ml es implements form smoothing exploiting similarity  timation tend produce poor quality models         formation alphabet elements idea view    smoothing context typically used refer tech logconditional probability function smooth function  niques combine various ml probability estimates pro ﬁned similarity graph algorithm pro  duce accurate models enormous number tech pose uses eigenvectors similarity graph basis  niques proposed smoothing ngram mod expansion log conditional probability function  els including discounting recursively backing lower coefﬁcients expansion solving regu  order ngrams linearly interpolating ngrams different larized logistic regression problem experimental results  ders excellent survey techniques demonstrate superiority method similarity  chen goodman              graph contains relevant information whilst method    clustering models make use smoothing ideas remains competitive stateoftheart methods  attempt make use similarities words lack information                                                    ijcai                                                      sequence prediction based similarity            spectrum graph laplacian known close     information                                        lationship global properties graph “dimen                                                        sion” bottlenecks clusters mixing times random walks  consider word prediction task vocabulary vlet  wh                                       ∈v      spectral decomposition method employed moti       denote conditional probability word    vated follows smoothness function  v→  ∈v∗  history constrain size                                                         the graph measured sobolevnorm  histories n− words known ngram model                                                                                                 fg         fv dv         fu − fv wuv  sake simplicity paper deal  v∈v             uv∈e  bigrams methods proposed readily extended ﬁrst term controls size function whilst  higher order ngrams follows notation pyx second controls gradient using norm ob  used denote bigram probabilities      jective exploit similarity information expressed                                      ϕ               desire ﬁnd loglikelihood function gnorm    given  appropriate basis functions   log                                           probabilities bigrams written form small projection function lin                                                       ear function space spanned eigenvectors               log pyx ∝   αiϕix            laplacian smoothest approximation ba                          i∈i                           sis functions sense gnorm inﬂu                                 v×v                  enced ding et al  decomposed using sin  clearly simplest choice let  use                                                            ϕ   ϕ           gular value decomposition  usv  practical pur  called euclidean basis functions  ii   δ δ       δ · ·                            poses truncated svd way            kronecker’s delta func p      p         ·  tion basis function set ﬁtting model   frobenius norm                                                                                   dataset equivalent maximum likelihood training  truncated version  main idea paper given similarity infor column vectors row vectors corresponding  mation possible construct ba largest singular values kept proper  sis function set facilitates generalization unseen cases malization singular vectors uk multiplied  basis functions construct form square root respective singular values dhillon                                                                                   overcomplete representation resort form reg ding et al  uk  uksk   ularization prevent overﬁtting                      basis functions obtained using columns                                                                                 ψ  ψ  ϕ        incorporating similarity information               denoting columns                                                             δiyψi xwhereixy ∈v ∈ksince  let  wwv words vocabulary                                                        similarity information unreliable added  order estimate pyx conditional probability distribu                   ϕ   φ                  ∈w                                    euclidean basis functions    ii   tion        used similarity information δ δ  hidden context assume similarity informa       set obtained way                                                        automatically constructed basis functions useless  tion words represented undirected weighted                                                        method chance recover handle unknown  graph vewwheree       ⊂v×vare edges                                                       words resort nystr¨om’s method baker    →   assigns nonnegative weights word                                                         order extend singular vector ψi new word  pairs mathematical convenience use denote                       ∈v     ×                       th            sufﬁces to know xz set known          weight matrix   entry                        √wxz                                                        words fact x∈v     ψix shown good  weight edge → idea construct                 dxdz  basis functions respect similarity information  approximation  way accomplish use spectral graph cluster experiments used single simi  ing known yield basis functions used larity graph algorithm natural ability  represent square integrable function chung use graphs simply merging set resulting basis   fact spectral graph clustering methods construct functions just like regression use basis  basis functions natural constructing geodesically functions products basis functions  smooth global approximations target function                                                                                words smoothness respects edges graph        yx ∝        αaϕa   case means similar words wordpairs log                                                                                             i∈i  signed similar probabilities left future work deter                                                                                  ab        results presented improved using                αij ϕi yϕj       techniques diffusion wavelets coifman        ab ij∈i  maggioni     particular method chosen basis func                                                                                                           tions computed using singular value decomposition fact second order anova model lin                   −    −                               higher order models products basis func  matrix   wd   hered diagonalvalency                                                    tions used practice rarely  matrix diagonal elements deﬁned  ij  used second order models typically produce sufﬁciently  operator spectrally similar normalized graph                         −           −                good results model sets combined  laplacian operator   −   fact ele number parameters increases rapidly use                                  −     −   mentary algebra yields −   wd   regularized ﬁtting method high importance                                                    ijcai                                                      solving regression problem                   algorithm  similarity based smoothing sbs  assumption basis functions depend input similarity information words training data  through kronecker delta equation  raised output estimated probability distribution  exponent takes form                         build similarity graph words vew                                                                                                                                                                                                                                            −    −                     αy βx               αi βx        calculate normalized matrix  wd   pyβxα         wherezα                                                                                                                                α                                   determine svd decomposition  usv                                                                α     α α                  calculate mapping operator singular vectors  normalizing factor          ma                                                                         α                                                  trix weight parameters learned  contains singular values Φuksk  weight parameters word yandβx vector formed                                                          train logistic regression model weight parameters  basis functions evaluated shall βx                                                            using training data  feature vector associated    let training data sequence vvn   vi ∈v assuming data generated ﬁrst order  markov model transition probabilities mod ikn considered best smoothing techniques  eled multinomial model  data loglikelihood takes goodman   following form                                     order evaluate methods calculated em                                                     pirical crossentropy trained model heldout data                                                              αd           αt β     −                                   ml              vj  j−                                                                                            n                                                                                                                                               hn pˆ−      log pˆwiwi−                           v                                                                                                                                                     −  log    exp αi βvj−  const                                                    pˆwiwi− probability assigned transition  maximum  likelihood ml estimate parameter wi− wi model denotes true distri  vector vector maximizes function order bution synthetic datasets wi ∼ p·wi−  prevent overtraining advisable prefer smooth solu assumption heldout data distribu  tions especially number basis functions big tion training data shannonmcmillanbreiman  words feature vectors β high dimensional theorem know mild assumptions hn close  way enforce smoothness introduce prior dis crossentropy pˆ respect true distribution  tribution parameters αij  choices pos data crossentropy pˆ sum  sible priors work studied behaviour entropy kullbackleibler divergence pˆ  method used laplacian nonnegative quantity smaller crossentropy  prior pαij  ∝ λij exp−λij αij  gaussian closer estimated model true model                       α       α ∝        −  ij                                each test calculated cross entropies  prior    λ  exp λ                ij       ij                              mltrained bigram ikn proposed similarity based    assuming priors training problem smoothing technique case synthetic datasets  comes maximization following objective function                                                        estimated entropy models using true                 n                                     transition probabilities  corresponding points        αd        αt β    −                        graphs labeled ‘model’     map           vj  j−                                                                                             tests synthetic data           v                v k      −            αt β      −        λ  α        model generating synthetic data order        log    exp    j−            ij  ij         velop good understanding possible advantages                                                                                    limitations proposed method tested           penalty term added ml objective function trolled environment data generated using     depending prior wishes designated bigram model order make test  use corresponds laplacian prior whilst  ing decided use “clustered” markov models fol  corresponds gaussian prior                    lowing form probability observing given com    used smlr java implementation krishnapuram puted  et al  implemented laplacian land                                                                 yx        gaussian priors  ﬁnd solution equation                             sketch algorithm shown algorithm       cx cluster symbol word  v→c                                                         idea observation    experimental results                               pends past observation through class cx  compared performance similarity based smoothing past observations x yield identical  sbs synthetic realworld data plain bigram es future distributions cxcx note  timates bi interpolated kneserney smoothing ikn ycδcyc model thought                                                    ijcai                                                    hidden markov model hmm state transitions deﬁned  results        whilst observation probabilities uncon performed sensitivity analysis test effect                                   strained general hmm states equivalent various parameters inﬂuence results particular  model way model speciﬁed studied performance sbs function observa                          ∈  rc×c  matrices    cc                     tion noise γ masks identiﬁability block struc                         v×c  byc  yc ∈    let ture noise similarity matrix   gradually  matrices generated                        creases quality similarity information available    computing matrix start permutation sbs number training sentences ntr cluster  matrix size × perturb random noise structure parameters changed whilst  transition probabilities nonzero ii each kept default values γ   state number “signiﬁcant” transitions lies     ntr   default cluster structure                                                                                                                                         clusters having observations       respec    computing start idealized block tively clusters bigger smaller                                                             structured matrix byc ∝ δcyc perturb  kept  present results  according                                          laplacian gaussian priors corresponding results                                                      labeled ‘sbs gauss’ ‘sbs lapl’ graphs                byc ∝ byc  δ  γzyc  elements z independent random variables sensitivity noise masking blockstructure                      yc                                γ  uniformly distributed − ifδ block  observations used directly infer  structure source clearly identiﬁable based derlying classes estimation problem easier  puts given observation knowing infer blockstructure masking noise increased problem  probability hidden state cy noted harder  model collapses hidden markov model             δ                                                    states nonzero structure blurred                               ikn  experiments used δ  whilst let γ change                              sbs lapl          γ                                                                          sbs gauss  note roughly corresponds noise level                                    model                                                                                          bigram  observations                                              similarity information provided sbs controlled           similarity information provided sbs reﬂects  actual block structure data perfect similar crossentropy    ity information assigns  observations words different  clusters whilst assigns positive value say    observations cluster corresponding matrix                   v×v                                         denoted ∈     sxy  δcxcy                                            disturbed adding noise ran            blockstructure masking noise  dom × matrix created entries inde  pendent uniformly distributed random variables taking val figure  crossentropy models built various al  ues   given noise parameter   ≥  perturbed gorithms function blockstructure masking noise  matrix computed                                 parameter γ           s     −s   −                                                             figure  shows results measurements    denotes component wise product matrices observed proposed method performs signiﬁcantly    denotes matrix entries better considered spectrum γ peers   words increasing   reduces intercluster similarity fact masking noise gets bigger ikn’s performance  increases betweencluster similarity extreme case converges bigram model hand    similarity information lost          sbs able maintain estimation accuracy                                                        range γ experiments sbs  training test datasets training dataset normally gaussian parameterprior performed slightly better sbs  contains ntr   observation sequences laplace prior believe  experiment changing parameter each having ran laplacian prior prefers sparse solutions number  dom length generated drawing random number basis functions high making sparse solu  normal distribution mean  variance  tions preferable  setting length maxl test dataset sepa  rate training dataset  sequences robustness degradation similarity  generated using procedure measurements formation set experiments investigated  repeated  times average values presented sensitivity method quality similarity                                                    ijcai                                                    information gradually increased similarity                                                                                               ikn  information noise parameter   aswehave                                    sbs lapl  discussed   similarity information perfect                         sbs gauss                                                                                           model  whilst similarity information lost                                    bigram                                                                                                                                                                                ikn                                                          sbs lapl                                                                                               sbs gauss                  crossentropy                                 model                                                           bigram                                                                                                                                                                                                                                                                                                                 crossentropy                                                    sentences training corpus                                                                   figure  crossentropy models built various al                                                        gorithms function number sentences train                                                                                                      ing data  tr                   noise similarity information                                                                                  −                                                               clusters             ˆikn      ˆsbs  figure  crossentropy models built various al                  average       stddev  gorithms function noise parameter   governing              quality similarity information                                                                                                                                expected similarity information gets weaker                       performance sbs degrades converges                          mlestimated bigram model notable                           similarity information poor                    sbs performs ikn reason                    experiments did add euclidean basis func                     tions set basis functions expect spectrum           highnoise similar matrix uniform covering xx           spectrum add singular vectors xx           basis model automatically retains power                                                        table  crossentropy decrease sbs compared                                                        ikn number different cluster structures  sparsity training data main promise sbs  using similarity information capable achieving  better performance available training data  tests real data  limited order validate performed set ex task considered predict pos sequences wall  periments number sentences training data street journal wsj corpus extracted weighted di  varied results shown figure  rected sub graph based senses syntactic categories  gaussian laplacian priors sbs outperformed ikn wordnet fellbaum    words based  wide range values ntr interesting note information pos similarity graph pg built  gaussian prior performed better follows using brown corpus ﬁrst collected pos  methods number sentences sible pos categories word created prior  small                                                postag distribution words unique                                                        word brown corpus added outlinks pos                                                        graph according wordnet graph using weight  cluster structure tested sbs large variety clus link wordnet graph weight pos tags  ter setups ranging   clusters vocabulary sides link example arrive  sizes   results summarized ta word live link live → bouncy wordnet graph  ble                                                 weight possible pos tag combinations    clear number clusters small live bouncy link pospairs  words different clusters signiﬁcant creased pxlive · pybouncy · step  structural information similarity notable calculated basis functions outlined  cases sbs able perform ikn  hand signiﬁcant hidden structure penn treebank project httpwwwcisupennedu tree  model sbs greatly outperforms ikn                   bank                                                    ijcai                                                    
