     convergence reinforcement learning general function approximators                                        vassilis apapavassiliou stuart russell                       science division california berkeley ca                                              vassilisrussell csberkeleyedu                                abstract                               tolearning probability  value function rep•                                                                  resented table each state entry         key open problem reinforcement learning           large state spaces compact parametric representa•        assure convergence using compact hy•            tions required representations interested        pothesis class approximate value function          algorithm converge function         standard temporaldifference learning        closest metric true value function form         algorithm shown converge hy•         agnostic learning gordon  proved conveiges         pothesis class linear combination fixed ba•       sense representations called averagers         sis functions diverge general non•        update maxnorm contraction section  tsit        linear hypothesis class paper describes        siklis van roy   proved convergence established         bridge algorithm new method reinforcement          error bounds td linear combinations fixed ba•        learning shows converges approxi•      sis functions         mate global optimum agnostically learnable          nonlinear representations neural networks         hypothesis class convergence demonstrated         observed suboptimal solutions bert        simple example temporaldifference            sekas tsitsiklis  diverge se•       learning fails weak conditions identified un•        rious problem real problems require nonlinearity        der bridge algorithm converges          baird  introduced residual algorithms con•       hypothesis class finally connections be•       vergence proved combined gradient de•       tween complexity reinforcement learning        scent learning method used neural networks        paclearnability hypothesis class             unfortunately error resulting approximation                                                                  arbitrarily large furthermore method requires in•    introduction                                                dependent visits each sampled state                                                                     paper describes bridge algorithm new rl    reinforcement learning rl widely used method       method establish convergence error bounds    learning make decisions complex uncertain environ•     agnostically learnable representation    ments typically rl agent perceives mid acts en•                                                                    section  provides necessary definitions notation    vironment reeei ving rewards provide indication                                                                  section  explains problem nonconvergence pro•  quality actions agents goal maximize                                                                  vides examples section  outlines bridge   sum rewards received rl algorithms work learn•                                                                 algorithm sketches proof convergence shows    ing valueunction describes longterm expected sum                                                                  solves examples fails section  briefly   rewards each state alternatively learn                                                                  covers additional results convergence local optima    qfunction describing value each action each state                                                                  representation use paclearning theory   functions used make decisions                                                                  section  mentions alternative techniques      temporaldifference learning sutton  com•  consider paper necessarily technically tense given    monly used family reinforcement learning methods al•   space restrictions results broad   gorithms operate adjusting value function locally ai machine learning communities   consistent used function approximators   neural networks provide compact parameterized repre•  sentation value function methods solve real     definitions   world problems large state spaces   like know algorithms guaranteed     mdp   work—ie converge return optimal solutions     markov decision process  set      theoretical study rl algorithms usually divides  states set actions transition probability distributions   problem aspects exploration policies guar•           define state distribution given current   antee complete coverage environment value deter   state action reward distributions define   mination value function corresponds given distribution realvalued reward received execut•  policy paper concentrates second aspect prior    ing discount factor  in•  work jaakkola et al   established convergence   terested problem value determination assume        machine learning given fixed policy choice action each state erative process practice   executing fixed policy mdp actually  mapping usually performed exactly   markov chain write transition  access necessary expectation compute   probabilities px reward distributions rx tvx exactly infeasible states   assume able define stationary distribu­ perform approximate mapping using samples   tion π resulting markov chain rewards state sample distribution stationary distri­  lie range                                              bution Π general compute tvar ex­                                                                actly approximate generating samples                                                                 sample distribution     observed random rewards rk distribution rxk   define true value function state ex­  passing learning algorithm joint   pected discounted reward state                 probability density generate samples                                                                 simply combines sample state                                                                 conditional probability density      problem value determination determine true                                                                 generate estimate tvx   value function good approximation classical td so­                                                                  paper focus agnostic learning case   lutions use backup operator takes                                                                 learning operator seeks hypothesis best matches   approximation produces better approximation                                                                 target function typically target func­                                                                tion hypothesis class measure distance us­                                                                ing norm define learning operator                                                                 agnostic learning     operator said contraction factor   norm  •                                       mentioned typical case                                                                 access exact function learned                                                                 draw samples sample distribution     said nonexpansion define      expected value conditional distribution vx                                      vxand normto        addition samples states according what­                                      contraction   distribution used measure distance previous   factor fixed point maxnorm        definition equivalent definition agnostic learning    norm tsitsiklis van roy  repeated     based minimizing risk   application iterative process   converges  use tj represent operator     define risk hypothesis respect   applied times                                              distribution     transition probabilities reward distributions   known possible compute tv directly   definition known   possible compute expectation definition practice approximately performed generating   case observing sequence states rewards samples sample distribution   markov chain form unbiased estimate tv    able estimate risk able output   specifically observe state reward followed  hypothesis  minimal risk respect                                                                 distribution algorithm present assume abil­  state  observed backedup value                                                                 ity compute exactly given hypothesis class   unbiased estimate tvx formally define                                                                 certainly trivial assumption later section   conditional probability density observed backed                                                                briefly discuss extension algorithm case   values state                                                                 pacagnostic learning step ex­                                                                act agnostic learning step   defined random variables               finally let define goal defined best   distributions respectively                      approximation possible using    random variable associated density   ey   similarly defineto                        seek techniques return value function mini­  conditional probability density jstep backedup values ob­ mizes relative absolute error bound   served state    function approximation   state spaces large infinite  nonconvergence td   feasible tabulate value function each state   resort function approximation approximation   section examine nonconvergence problem   scheme consists hypothesis class representable func­ td used nonlinear function approximators   tions learning operator maps arbitrary value      present simple examples reconsider   functions functions                                   bridge algorithm section     standard based approaches use function approx­     mentioned standard td function approxi­  imation essentially compute approximately compute  mation based iterative process                                                                                    papavassiuou russ€ll       nonexpansion norm makes      gence results look examples pre•   contraction composite operator contraction    vious section    process converge error bound relative   main algorithm bridge valuedet determines         example tsitsiklis van roy  consider    value function error bound making repeated    linear hypothesis class simply projection     calls bridgestep invoca•   uses nonlinear hypothesis class iih tion bridgestep    nonexpansion iterative process diverge    metaphorically consists throwing bridge    stuck local minimum arbitrarily far           treacherous terrain hypothesis class      simple examples demonstrating ways            point far side optimal solution bridge    fail used nonlinear hypothesis  lands close aimed able    class consider mdp states probabil     walk productive direction achieve contrac•   ity going state  rewards tion bridge lands far target know    deterministic stationary distribution  isnt hexpressible value function near taiget    each state discount factor  hypothesis class bridge landed error bound                                                                   precise lemma  section                                                                     given old approximation try                                                                   tocreate better approximation vnew webasically                                                                   tools work iih seen figure                                                                    example previous section combine                                                                   operators standard way vnew  h«tv                                                                   stuck local minimum instead use                                                                   creatively guarantee progress establish error bound                                                                                   figure  stuck local minimum                                                                       begin using deter•                                                                  mined main algorithm bridgevaluedet                                                                   calls bridgestep ask question given                                                                   know does tell                                                                   location  turns restricted lie                                                                   hypersphere position defined terms po•                                                                  sitions  precise lemma                                                                    section hypersphere depicted figure                                                                    required lies inside        figure  suboptimal fixed point oscillation        modify rewards slightly rr      — true value function    longer best representation       iih   start     reach suboptimal fixed point  — how•  starting       result repeated applications shown fig•  ure displays different type failure — oscillation be•                                                                                figure  bridge aimed   tween points approaching   pre•  vious exampleis small relative error                     define new operator based iden•  bound large                                                tity operator      bridge algorithm     begin high level description algorithm de•   simply amplifies bellman residual factor   tails appendix followed conver•    seen figure  point far side        machine learning hypersphere operates use   three lengths determine relative position   throw bridge aim bridge bv     respect figure  practice es•  goal ithe true value function  timate true risk empirical risk haussler    lies bv motivation us•       calculate using samples drawn distribution   ing sense jump local minima                                                                         just described single invocation bridgestep     ideally able represent just   represents iteration main algorithm each   standard approach want represent tv     iteration builds new bridge based previous   function likely class representable func• generic iteration begin   tions apply operator map  previous iteration figure  particular input   result  ilhbv shown figure        generic iteration linear com•  bridge supported shown line be•     bination initial approximation previous   tween summary throw bridge aiming    functions final result tall weighted tree   determines point actually lands      leaves insist final result                                                                 apply final mapping end                                                                   just standard td algorithm summarized                                                                                     bridge algorithm essentially                                                                 summarized                   figure  bridge established       practice perform mapping iih generating sam•  ples appropriate distribution passing   learning algorithm particular compute hbv   generate samples according distribution                                                                            figure  generic iteration bridgestep                                                             key feature distribution random variable   associated density ey  bvx                  convergence bridge algorithm     final step walk bridge bridge state main convergence theorem bridge al•  line new approximation vnew      gorithm space limitations allow state   point line figure  point deter• important lemmas used proof begin   mined projecting point   way        useful observation geometric relationship be•  line function input parameters tween tv   just project  using refinement     lemma  let contraction contraction factor   yields better guaranteed effective contraction factor     norm let fixed point                                                                 point                                                                       words given positions av let                                                                              know position                                                                 inside hypersphere radius centered                                                                 figure  hypersphere simply set points                                                                 factor closer av note dis•                                                                tance furthest point hypersphere                figure  new approximation                    apply lemma  using de•                                                                fines hypersphere inside true value function     new approximation vnew necessar•     lie lemma  used mainly prove lemma    ily inh weighted average old approximation characterizes behavior bridgestep provides    calculating weights   average    meat convergence proof   requires ability measure distance risk particular   need measure distance       lemma  given approximation parameters   risk respect distribution       andj   bridgestep returns new approxima                                                                                    papavassiliou russell                                                                  figure  center                                                                   closest hypothesis bv furthermore know                                                                   lie inside small hypersphere figure                                                                    separation separation                                                                   allows prove possible position upper                                                                   bound relative error        tion vnew satisfies following   ditions error bound    defined appendix                                                error bound       intuitively bridge lands close aimed    achieve contraction goal bridge    lands far away prove relative error bound    result key quantity determines    events happens angle formed bridge    line bv  close bv    small bridge lie close hyper   sphere able walk bridge make    progress instead far bv large    walking bridge closer    figure  relative error bound established  large    goal able prove close                                                             noted general know      figure  shows case angle small de•     measure  determine conditions    scribed previously small hypersphere represents set   lemma  vnew satisfies know satisfies    points factor closer    follows applying lemma          lemma  satisfies relative error bound    operator  think bridgestep operator         vnew vnew achieves contraction    takes returns  ask question set        error decreases each successive approxima•   points factor „ input parame•   tion better achieve relative    ter bridgestep closer applying               error bound point subsequent approxima•   lemma  question defines larger hy•      tion achieve bound    persphere depicted figure  center     main result guaranteed conver•   case  arcsin — arcsin note larger hy•           gence relative absolute error bound    persphere completely contains smaller hypersphere    maximum number invocations bridgestep    contains lies inside larger hypersphere   maximum number hypotheses linear combina•   factor closer         tion specified    holds  arcsin — arcsin smaller                                                                   theorem  let     desired relative    achieved contraction better                 absolute error bounds respectively let upper                                                                   bound desired number iterations algorithm                                                                   bridgevaluedetz €on produces approximation                                                                   consisting linear combination nl hypotheses                                                                  satisfies relative error                                                                   bound absolute error bound eo                                                                         proof theorem follows directly lemma                                                                    rewards bounded true value function bounded         figure  contraction achieved small           absolute error initial approximation bounded                                                                   iterations achieve contraction absolute er•     figure  shows case angle large large   ror smaller requested itera•   possible hypothesis  close bv tions failed achieve contraction achieved relative    fact choose  closest hy•               error bound subsequent iterations including   pothesis rest  lie away particular achieve requested relative error bound    lie completely outside big hypersphere depicted      know conditions lemma         machine learning 
