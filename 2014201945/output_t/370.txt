               theoretical framework learning bayesian networks                             parameter inequality constraints          radu stefan niculescu                tom mitchell                    bharat rao       siemens medical solutions        carnegie mellon university        siemens medical solutions           malvern pa usa                  pittsburgh pa usa                 malvern pa usa     stefanniculescusiemenscom        tommitchellcscmuedu           bharatraosiemenscom                        abstract                            learning bayesian networks correctness                                                        learned network course depends train      task learning models realworld   ing data available training data scarce useful      problems requires incorporating domain knowl     employ various forms prior knowledge domain      edge learning algorithms enable accurate improve accuracy learned models example      learning realistic volume training data main expert provide prior knowledge specifying condi      domain knowledge come forms      tional independencies variables constraining      example expert knowledge relevance  fully specifying network structure bayesian net      variables relative certain problem help work addition helping specify network structure      form better feature selection domain knowledge   domain expert provide prior knowledge      conditional independence relationships  values certain parameters conditional probabil      variables help learning bayesian ity tables cpts network knowledge form      network structure                                prior distributions parameters previous      paper considers different type domain   research examined number approaches represent      knowledge constraining parameter estimates    ing utilizing prior knowledge bayesian network      learning bayesian networks particular   parameters type prior knowledge utilized      consider domain knowledge comes    current learning methods remains limited insufﬁ      form inequality constraints subsets pa cient capture types knowledge readily      rameters bayesian network known struc  available experts      ture parameter constraints incorporated  main contribution paper consists deriving      learning procedures bayesian networks closed form maximum likelihood estimators param      formulating task constrained optimization eters bayesian network setting expert      problem main contribution paper main knowledge available form parameter inequal      derivation closed form maximum likelihood pa  ity constraints current methods accommodate      rameter estimators setting          estimators comes theorem describes                                                        formance case domain knowledge represented                                                        inequality constraints entirely accurate    introduction                                         section paper describes related research  probabilistic models increasingly popular constraining parameter estimates bayesian networks  decade ability capture non section  presents task parameter estimation  deterministic relationships variables describing presence general parameter constraints formulates  real world domains models bayesian net constrained optimization problem section  details  works heckerman  received signiﬁcant attention main contribution paper closed form solutions pa  ability compactly encode conditional rameter estimates classes parameter inequality  dependence assumptions random variables constraints formal guarantees estimators  development effective algorithms inference discussed section  conclude brief summary  learning based representations bayesian network research directions future work  consists components structure encodes  assumption variable conditionally independent  related work  nondescendants network given value parents  set parameters each variable main methods represent relationships param  relates probabilistically parents bayesian network eters bayesian network fall main categories  encodes unique joint probability distribution dirichlet priors variants including smoothing tech  easily computed using chain rule                 niques parameter sharing kinds                                                    ijcai                                                       geiger heckerman  shown dirich act estimators altendorf et al  assumes  let priors possible priors discrete bayesian values variables totally ordered feelders  networks provided certain assumptions hold think van der gaag  assumes variables binary  dirichlet prior expert’s guess parameters framework makes assumptions  discrete bayesian network allowing room section place parameter learning inequality  variance guess main problems straints general constrained optimization framework  dirichlet priors related models impossible show closed form learning parameters  represent simple equality constraints pa bayesian networks performed expert knowledge  rameters example constraint θ  θ available form types parameter  θijk  xi   xij parentsxipaik   equality constraints  ing priors hyperparameters dirichelet prior  case marginal likelihood longer com  problem deﬁnition approach  puted closed form expensive approximate methods  required perform parameter estimation second problem deﬁne problem suggest general optimiza  expert’s ability specify tion based approach solve approach  dirichlet prior parameters bayesian network limitations constraints arbitrary    widely used form parameter constraints employed constitutes basis computing maximum likelihood  bayesian networks parameter sharing models estimators classes inequality constraints described  ing different types parameter sharing include dy section  begin describing problem  namic bayesian networks murphy  spe assumptions  cial case hidden markov models rabiner  module  networks segal et al  context speciﬁc indepen  problem  dence models boutilier et al  bayesian multi task perform parameter estimation  networks recursive multinetworks dynamic multinet bayesian network structure known advance  works geiger heckerman  pena et al  accomplish task assume dataset examples  bilmes  probabilistic relational models friedman et available addition set parameter equality andor  al  object oriented bayes nets koller pfeffer inequality constraints provided domain expert   kalman filters welch bishop  bilinear equality constraints form giθfor  ≤ ≤  models tenenbaum freeman  parameter sharing inequality constraints form hjθ ≤   methods constrain parameters share value  ≤ ≤ kwhereθ represents set parameters  capture complicated constraints parame bayesian network  ters inequality constraints constraints sums initially assume domain knowledge provided  parameter values methods restricted shar expert correct later investigate happens  ing parameters level sharing conditional knowledge completely accurate enumer  probability table cpt module networks hmms ate assumptions satisﬁed meth  level sharing conditional probability distribution ods work similar common assumptions  single cpt context speciﬁc independence level learning parameters standard bayesian networks  sharing statetostate transition matrix kalman filters assume examples training dataset  level sharing style matrix bilinear models drawn independently underlying distribution  models allow sharing level gran words examples conditionally independent given  ularity individual parameters niculescu et al  parameters bayesian network second assume  troduces parameter equality constraint framework variables bayesian network  scribes models use parameter sharing module different values safe assumption  networks context speciﬁc independence models hmms   uncertainty random variable  dynamic bayesian networks framework possible value variables bayesian network  showed efﬁcient parameter learning performed deleted arcs nodes  frequentist bayesian point view corresponding variables computing pa  observable partially observable data             rameter estimators additionally assume observed    models described far advantage counts corresponding parameters bayesian network  certain types parameter equality constraints recently strictly positive enforce condition order  altendorf et al  feelders van der gaag  avoid potential divisions zero impact infer  study feasibility incorporating simple inequality ence negatively real world expected  straints learning parameters bayesian networks observed counts zero problem solved  constraints analyzed papers restric using priors parameters essentially ef  tive sense each constraint involve parame fect adding positive quantity observed counts  ters conditional probability table framework essentially create strictly positive virtual counts  allows constraints individual distribution level gran finally functions ggm hhk  ularity additionally altendorf et al  authors twice differentiable continuous second derivatives  employ approximation algorithm derive ex assumption justiﬁes formulation problem                                                    ijcai                                                                 strained maximization problem solved using stan adverbs greater aggregate probability mass              dard optimization methods                            verbs given language formally type                                                                    domain knowledge parameters conditional probabil                general approach                               ity distribution denoted θθn partitioned                                                                                                                                                  θ  ∪    ak ∪   bk ∪c      θi ≤        θi              order solve problem described brieﬂy                 θi∈ak       θi∈bk                                                                            ≤   ≤              suggest approach based existing optimization  let denote nak sum ob              techniques idea formulate problem served counts corresponding parameters ak similar                strained maximization problem objective function deﬁnitions hold nbk nc letn sum              data loglikelihood log dθ constraints observed counts ni corresponding parameters θ              given giθfor    ≤ ≤   hjθ ≤      expert potentially specify different               ≤  ≤  easy applying karush straints conditional probability distributions              kuhntucker conditions theorem kuhn tucker  bayesian network decomposability log              maximum satisfy number likelihood problem computing maximum like              equations variables solve use lihood parameter estimators decomposed set              existing methods example newton independent optimization subproblems each condi              raphson method press et al                  tional probability distribution network                known ﬁnding solution given following theorem              kkt conditions determine opti theorem  ni strictly positive maximum              mum point fortunately objective function concave likelihood estimators parameters θ given              constraints encountered real life                                                                                               linear equalities inequalities ˆ ni · ak  bk      ∈            ≥                                                                      θi     ·n    θi  ak nak   nbk                                                                                     ak              known sufﬁciency criteria guarantee optimality                                                                                         ˆ   ni · ak   bk      ∈            ≥                                                                      θi     ·n    θi  bk nak   nbk                unfortunately methods shortcom              bk              ings general case large number parameters ni                                                                      θˆ    θ ∈ ak ∪ bk na  nb              bayesian network extremely expensive                                                                                                     ˆ   ni      ∈              cause involve potentially multiple runs newton θi  θi              raphson method each run requires expen proof finding maximum likelihood estimators equiva              sive matrix inversions methods ﬁnding solu                                                                    lent maximizing lθ    ni · log θi subject              tions equations employed noted domain knowledge constraints including constraint              press et al  methods limitations                                                                    gθ     θi −  problem contains              case constraints arbitrary nonlinear func equality constraints attempt solve using karush              tions worst case happens exists constraint kuhntucker theorem introduce lagrange multiplier              explicitly uses parameters bayesian network                                                                    λ μk inequality constraint hkθ θ ∈a θi−                method regarded mere suggestion                                                                                                                  θi ≤  optimum θˆ              shortcomings believe compu θi∈bk              tationally feasible arbitrary constraints mentioned solutions              show learning presence parameter                                                                      ⎧                                    constraints formulated general constrained max ⎪ ∇ ˆ −  ·∇    ˆ −       ·∇     ˆ                                                                      ⎪   θ lθ λ    θ gθ   μk   θ hkθ              imization problem general approach provides ⎨⎪               ˆ              starting point ﬁnding closed form maximum likelihood                  gθ                                                                                         ·   ˆ              estimators given particular classes parameter inequal ⎪         μk  hkθ                                                                      ⎪              ity constraints presented section          ⎩⎪                hkθˆ ≤                                                                                          μk ≥                 learning inequality constraints                 ﬁrst equation obtain              section derive closed form maximum likelihood              ⎧                                                                                         ni              estimators parameters discrete bayesian network          ⎨            ∈                                                                                        λμk θi               known structure domain knowledge provided                  ni                                                                                θˆi        θi ∈ bk                                                                                    ⎩  λ−μk              types inequality constraints                         ni                                                                                         λ  θi ∈                                                                                                                             inequalities sums parameters                                    ak                   bk                                                                            θˆi             θˆi                                                                                        θi∈ak     λμk       θi∈bk      λ−μk              brieﬂy type parameter domain knowledge states based constraint tight              sum parameters conditional prob                                                                                         na     nb               na                                                                      •    ˆ                                              ability distribution bounded sum parame θthen λμk  λ−μk  implies λμk                                                                                                               ters distribution bayesian network intu bk      ak   bk                                                                                                   θˆi                                                                          λ−μk        λ                   θi∈ak∪bk              itively think constraint terms parts                                                                          ak   bk                       ·     −              speech language usually adverb comes       λ     case λ nak nbk                                                                            ·                      ≥              verb reasonable assume lan μk nak  nbk sinceμk wealsomusthave                                                                             ≥              guage expert specify aggregate probability mass nak nbk order constraint tight                                                                ijcai                                                                               • hkθˆ  thenμk optimum point algorithm starts set                                                                    ak   bk                             θˆi          inthiscasewealsohave    each step adds ﬁnal tight constraints                    θi∈ak∪bk         λ                           −n                                       expert specify different sets                     ˆ     ak   bk             ˆ                  hkθ      λ    hkθ  wemustalso   straints different conditional probability distributions                  nak nbk                                    network decomposability log                observations allow conclude likelihood decompose task ﬁnding max                                          ≥              straint tight nak nbk  summing imum likelihood estimators set independent opti              parameters conditional probability distribution mization subproblems                                                            theorem   assume observed counts ni strictly                                                                                                        positive assume know set  kkl                                nc     knak  nbk                            θˆi                                    constraints tight point given maxi                                          λ             λ                                                                  mum likelihood estimators θˆ                  gives λ                       ˆ        ni                                                                      θi  αk · θi ∈ ak ∈                                                                                  ak                              ⎧                                          ˆ       −          ·   ni          ∈                                  ni                                  θi       j∈k αj          θi   ak                               ⎨            ∈                                                  m∈k                                 nμk θi                                   ni                                ∈                         θˆi         θi ∈ bk                              ⎩  n−μk                                  ni                                    θi ∈                     proof approach problem ﬁnding maxi                                                                    mum likelihood estimators similar fashion in theo                assume na  ≥  nb   according obser                                                                rem  data loglikelihood given lθ ni ·              vations means constraint tight                                                           log θi maximize respect                ak       bk      ak   bk              nμk    n−μk      ·n    immediately  main knowledge constraints including constraint                                                                           −                    ˆ    ni ·  ak   bk      ∈             ≥         gθ    θi  use karushkuhntucker              derive θi    ·n     θi  ak  nak   nbk                                  ak                                theorem introduce lagrange multiplier λ                        na nb                 ˆ     ·          ∈            ≥                                                    −  ≤              θi     ·n    θi  bk nak   nbk         μ  inequality constraint θ θ ∈ak θi α                                 bk                                                                                      nak nbk   discussed μk  optimum θˆ solutions                          ˆ    ni      ∈    ∪              θi  θi ak   bk nak  nbk                  loglikelihood objective function concave                                                                      ⎧                                    constraints linear inequalities follows θˆ ⎪ ∇ ˆ − ·∇ ˆ −   ·∇     ˆ                                                                      ⎪   θ lθ λ    θ gθ   μk   θ hkθ              set maximum likelihood estimators concludes ⎨⎪              ˆ              proof theorem                                                 gθ                                                                      ⎪               μk · hkθˆ                                                                      ⎪                upper bounds sums parameters                 ⎩⎪                hkθˆ ≤                                                                                             ≥              domain expert provides upper bounds sum                 μk                 parameters conditional probability distri ﬁrst equation obtain              bution bayesian network consider language              example described introduction previous subsec                                                                                   ˆ      ni       ∈              tion expert state aggregate probability       θi         θi ak                                                                                        λ  μk              nouns greater  aggregate probability                                                                                                              ak              verbs greater  aggregate probability θˆi       based                                                                                  θi∈ak      λμk              adjectives greater  combined straint tight                probability mass words equals sum upper                      na                                                                      •    ˆ          ∈                            bounds provided expert greater θie kthen λμk  α  implies              mally type domain knowledge parameters ˆ    ni         ni                                                                        θi  λμ   αk ·                                                                                          ak              conditional probability distribution denoted θθn                                                                      ∪                      ≤       •      ˆ           ∈              partitioned θ  ka θ ∈ak θi α       hkθ   kthenμk                                                                                                           ≤    ≤                                                            ak                  swhereα    given positive constant         θˆi                                                                                   θi∈ak      λ              denote nak sum observed counts cor              responding parameters ak sum summing parameters involved tight              observed counts ni corresponding parameters θifthere constraints                                                                                                                    parameters involved constraints                                                                                                                       j∈k naj              consider belong set ak αk    −    αj           θi                 previous subsection easy way                                     λ                                                                               j∈k      θi∈akk∈k              cide constraint tight optimum point                                                                                                                    type constraints deal able             mp∈k             ˆ                                                                      obtain λ    −     α   θi −              derive simple criterion show simple                 j∈k                                                                                 ni                                                                      j∈k αj ·           θi ∈ ak ∈              linear algorithm computes set tight constraints     m∈k nam                                                                ijcai                                                                                                                                                                                     −            loglikelihood objective function concave  j∈k∪kl α    ﬁrst induction step  constraints linear inequalities follows θˆ proved  set maximum likelihood estimators concludes in sides of inequality  add quantity  −                                                         −         ·              derivation maximum likelihood estimators α   j∈k α    m∈k∪kl  obtain  know advance constraints satisﬁed esti                                  mators                                                 −   −        ·         ≥   −       ·                                                          αkl     αj     nam         αj          nam                                                                j∈k    m∈k           j∈k     m∈k∪k     algorithm ﬁnds set tight                                      constraints                                                           −    −                                                        given  αkl    j∈k αj   equivalent  algorithm  finding set tight constraints λl ≥ λl concludes proof lemma    αj                                                          applying lemma  follows case                                                                 step  start  ∅ each step add constraint αj  algorithm  ends step                                                                                                                 ak                                                                                                                   ak                                                         α    ≥  λj ≥ λl kj ∈ α   λl                                                         kj                                                                                                             mp∈k           ∈ lemma  follows set tight  step   kklletλl  −   α                                        j∈k            constraints case αj  algo  theorem                                        rithm  correct case constraints                                         na               processed left λl compare                                         kl                      ∈                 ≥    step  exists αk     λ let   situation happen step                                                       ∪kl  step  stop  declare set tight constraints                                  na          na                                                                             ks    ≤     ks                                                                         −  proof correctness algorithm start making                  jk αj    αks                                                                                    following observation based proof theorem                                                         αj αj   second                                         ak    •                              ≥              case constraints contradictory happen      tight λμk  α  μ wemust                                                      assume the domain expert provides accurate            ak ≥      αk    λ                                     main knowledge  αj case covered    • hk tight μk   algorithm  obvious constraints                                                           ak                               ak      tight maximum likelihood estimators      hkθˆ     − αk                 λ                               αk       feasible value θ      λ obvious λ ≥  hold      negative parameters                      formal guarantees    just developed criterion test set happen constraints provided  constraints set tight constraints          expert completely accurate methods  lemma  given λ depends computed far assumed constraints correct  theorem  set tight constraints fore errors domain knowledge prove detrimental                          ak                      ak                           performance learned models section investi      ≥ λ ∈   λfor ∈   αk                     αk                            gate relationship true underlying distribution    proving algorithm produces set tight observed data distribution estimated using  constraints let prove useful result      methods based inequality constraints space                                                       reasons omit proofs theorem corollary                                     ≥    ≥  lemma  j αj    λ   λ      presented section  quantity  − j∈k αj strictly positive    suppose true distribution data sam                                                        pled let ∗ closest distribution terms  proof lemma initially  ∅ obvious klp · factorizes according given structure   −          ≥       j∈k αj   obvious λ  let verify obeys expert’s inequality constraints  induction step                                   theorem  inﬁnite data distribu         na                                 kl     α   ≥ λl  − j∈k αj     tion pˆ given maximum likelihood estimators theo           kl                                                        rem  converges ∗ probability                                                                                            corollary  true distribution factorizes according    na  ·  − αk −    αj  ≥ αk ·        nam           kl                                            given structure parameter inequality                   j∈k           m∈k∪kl                                                       straints provided expert completely accurate               −             ≥                                      ˆ    follows    j∈k∪kl α     equality distribution given estimators computed theo  if processed constraints case rem  converges probability        αj obvious that constraints similar results hold inequality constraints described  tight assumed αj  wemusthave subsection                                                     ijcai                                                     
