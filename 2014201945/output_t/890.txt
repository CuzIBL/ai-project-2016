      taming decentralized pomdps efficient policy computation                                                  multiagent settings             nair tambe                           yokoo                      pynadath mar sella           science dept            coop computing research grp            information sciences institute      university southern california            ntt comm sc labs               university southern california            los angeles ca                    kyoto japan                 marina del rey ca             nairtambeuscedu                 yokoocslabkeclnttcojp           pynadath marsellaisiedu                              abstract                                 exceptions effective algorithms                                                                  deriving policies decentralized pomdps         problem deriving joint policies group                                                                  developed significant progress achieved efficient         agents maximize joint reward func•                                                                 singleagent pomdp policy generation algorithms mona       tion modeled decentralized partially                                                                  han  cassandra et al  kaelbling et al          observable markov decision process pomdp                                                                  unlikely research directly car•        despite growing importance applica•                                                                 ried decentralized case finding optimal poli•       tions decentralized pomdp models mul                                                                 cies decentralized pomdps nexpcomplete bern•       tiagents arena algorithms devel•                                                                 stein et al  contrast solving pomdp pspace       oped efficiently deriving joint policies                                                                  complete papadimitriou tsitsiklis  bernstein         models paper presents new class lo•                                                                 et al  note suggests fundamental difference         cally optimal algorithms called joint equilibrium                                                                 nature problems decentralized problem can•       based search policies jesp                                                                  treated separate pomdps individ•        exhaustive version jesp subsequently                                                                  ual policies generated individual agents         novel dynamic programming approach jesp                                                                  possible crossagent interactions reward transition         complexity analysis reveals potential                                                                  observation functions action agent        exponential speedups dynamic program•                                                                 different rewards possible based         ming approach theoretical results ver•                                                                 actions agents domains         ified empirical comparisons jesp                                                                  possibility simplify nature policies considered        versions each globally opti•                                                                 each agents example chades et al         mal bruteforce search algorithm finally prove                                                                  restrict agent policies memoryless reactive poli•       piecewise linear convexity pwlc proper•                                                                 cies approximation define reward        ties taking steps developing algo•                                                                 function transition function observations instead        rithms continuous belief states                                                                  states simplifying problem solving                                                                  multiagent mdp boutilier  xuan et al  de•      introduction                                              scribe derive decentralized mdp pomdp poli•                                                                 cies centralized mdp policy algorithm    multiagent systems research lab crit•                                                                 starts assumption communication grad•   ical applications multisatellite control researchers                                                                  ually relaxed relies instantaneous noise free commu•   need provide highperforming robust multiagent designs                                                                  nication simplifications reduce applicability    nearly optimal feasible end researchers                                                                  approach essentially sidestep question solving de•   increasingly resorted decisiontheoretic models                                                                  centralized pomdps peshkin et al  different    framework formulate evaluate multiagent de•                                                                 approach using gradient descent search local op•   signs given group agents problem deriving sep•                                                                 timum finitecontrollers bounded memory algo•   arate policies maximize joint reward                                                                  rithm finds locally optimal policies limited subset    modeled decentralized pomdp partially observable                                                                  policies infinite planning horizon algo•   markov decision process particular decpomdp                                                                  rithm finds locally optimal policies unrestricted set    decentralized pomdp bernstein et al  mtdp                                                                  possible policies finite planning horizon    markov team decision problem pynadath tambe     generalizations pomdp case         remains critical need new efficient   multiple distributed agents basing actions algorithms generating optimal policies distributed   separate observations frameworks allow vari•    pomdps paper present new class algorithms   ety multiagent analysis particular  solving decentralized pomdps refer joint    allow formulate constitutes optimal policy equilibriumbased search policies jesp jesp iterates   multiagent principle derive policy         through agents finding optimal policy each agent        multiagent systems                                                                                                     assuming policies agents fixed it• tiger problem used illustrating single agent pomdpskael   eration continues improvements joint reward bling et ai  create mtdp    achieved jesp achieves local optimum similar     example modified version agents    nash equilibrium discuss exhaustivejesp     corridor facing doorsleft right    uses exhaustive search best policy each agent door lies hungry tiger lies un•   exhaustive search single agents policy told riches agents know position ei•   expensive present dpjesp im•    ther   slsr indicating door    proves exhaustivejesp using dynamic programming      tiger present agents jointly individually    incrementally derive policy conclude sev•   open door addition agents independently    eral empirical evaluation contrast jesp glob• listen presence tiger  —    ally optimal algorithm derives globally optimal pol• openleft lopenright listen transition func•   icy search space policies finally prove tion specifics time agent opens    piecewise linear convexity pwlc properties tak• doors state reset sl sr equal proba•   ing steps developing algorithms continuous initial bility regardless action agent shown    belief states                                              table  agents listen state remains                                                                unchanged action each agent receives obser•    model                                                     vation new state observation function                                                                  shown table  return hl hr dif•   markov team decision prob•                  ferent probabilities depending joint action taken    lem mtdp pynadath tambe  frame                resulting world state example agents listen    work provide concrete illustra•         tiger left door state sl each agent    tion decentralized pomdp model receives observation hl probability  hr    decentralized pomdp models poten•               probability     tially serve basis bernstein et ai     xuan et al      given team agents mtdp pynadath    tambe  defined tuple     finite set world states                  sets action    agents  joint action represented a„    psiaan sf transition function represents    probability current state sf previous               table  transition function    state si previous joint action oa„               set observations agents      observation function represents   probability joint observation current state     previous joint action agents   receive single immediate joint reward rsa     shared equally      practical analysis using models like mtdp assume   observations each agent independent each   observations observation function expressed   ai  os au a„ ••                    table  observation function each agent        each agent chooses actions based local policy opens door tiger   tj mapping observation history actions present attacked equally tiger sec   time agent perform action table  injury sustained opened                                     refers joint policy door tiger severe open door jointly   team agents important thing note open door similarly receive   model execution distributed planning central• wealth share equally open door   ized agents dont know each observations   riches proportion number agents opened   actions runtime know each policies     door agents incur small cost performing                                                                listen action    example scenario                                             clearly acting jointly beneficial —                                                                 openleft agents receive riches sus•  illustrative purposes useful consider familiar tain damage acting   simple example capable bringing  agents receive independent observations share   key difficulties creating optimal policies mtdps  observations need consider observation histories   end consider multiagent version classic    agent action likely perform                                                                                                 multiagent systems                 table  reward function                       each time step computation performs                                                                  summation possible world states agent                                                                  observations time complexity algorithm     consider consider case reward func•     overall search performs   tion vary penalty jointly opening door   computation each possible joint pol•  tiger table                                     icy each policy specifies different actions pos•                                                                 sible histories observations number possible poli•                                                                 cies individual agent                                                                  number possible joint policies agents                                                                   correspond                                                                  largest individual action observation spaces respec•                                                                 tively agents time complexity find•                                                                 ing optimal joint policy searching space                                                                                       table  reward function                      joint equilibriumbased search policies                                                                 given complexity exhaustively searching op•                                                                timal joint policy clear methods                                                                  successful time generate policy                                                                  restricted section present algorithms                                                                 guaranteed locally optimal joint policy refer    optimal joint policy                                        category algorithms jesp joint equilibrium                                                                based search policies just like solution section                                                                  solution obtained using jesp nash equilibrium   agents share observations                                                                 particular locally optimal solution partially ob•  instead coordinate selecting policies sensitive                                                                 servable identical payoff stochastic gamepoipsg peshkin   teammates possible beliefs each agents en•                                                                et al  key idea policy maxi•  tire history observations provides information                                                                 mizes joint expected reward agent time keep•  problem facing team optimal joint policy                                                                 ing policies agents fixed process   combination individual agent policies produces                                                                 repeated equilibrium reached local optimum   behavior maximizes teams expected reward                                                                 problem optimum agents se•    surefire method finding optimal joint policy   lect multiple local optima encountered   simply search entire space possible joint policies planning centralized   evaluate expected reward each select policy   highest value perform search      exhaustive approachexhaustivejesp   able determine expected reward joint algorithm describes exhaustive approach   policy compute expectation projecting teams  jesp consider cooperative agents   execution possible branches different world states modify policy agent time keeping poli•  different observations present agent ver•  cies   agents fixed function best  sion computation results easily generalize policy returns joint policy maximizes expected   arbitrary team sizes each time step compute   joint reward obtained keeping —  agents policies   expected value joint policy team                  fixed exhaustively searching entire policy space   starting given state given set past observa•  agent policy free each itera•  tions follows                                        tion value modified joint policy       multiagent systems                                                                                                     increase remain unchanged repeated equi• steps section show exploit anal•   librium reached policies agents remains ogous optimality property multiagent case perform    unchanged policy guaranteed local maximum    efficient construction optimal policy    value new joint policy each iteration non jesp algorithm    decreasing                                                     support dynamicprogramming algorithm                                                                  define belief states summarize agents history    algorithm  exhaustivejespq                                  past observations allow agents ignore                                                                  actual history past observations supporting      prev random joint policy conv                                                                   construction optimal policy possible future      conv                                                                    singleagent case belief state stores distri•                                                                         bution  sufficient statistic          fix policy agents                                                                  agent compute optimal policy based          policy space list policies                                                                  having consider actual observation sequence          new bestpolicyipolicyspaceprev          newvalue  prevvalue                           sondik              conv conv                                          multiagent case agent faces complex nor•                                                          mal singleagent pomdp policies agents            prev new conv                                    fixed sufficient agent         conv                                     reason action selection agents            break                                              observation histories agents     return new                                                each time agent reasons tuple —                                                                                                                                best policy remain unchanged       joint observation histories agents   —  iterations convergence reached      treating state agent time define   worst case each joint policy best policy transition function observation function single   iteration algorithm agent pomdp agent follows   worst case complexity exhaustive search globally   optimal policy better practice   illustrated section  solution   algorithm local optimum adequate   applications techniques like random restarts simulated   annealing applied perturb solution   settles different higher value      exhaustive approach steps     exhaustivejesp algorithm enumerates searches   entire policy space single agent     osuch policies evaluating each incurs                     joint policy                                                                  agents    time complexity using ex•                                                                   define novel multiagent belief state   haustive approach incurs overall time complexity steps                                                                  agent given distribution initial state bs         incur                            prs                                                                                                                             complexity cost each pass through jesp   algorithm faster means performing bestpolicy           words reasoning agents policy   function step  produce big payoff overall  context agents maintain distribution   efficiency dynamic programming alternative      simply current state figure  shows different   exhaustive approach doing jesp               belief states  agent  tiger domain                                                                  instance shows probability distributions       dynamic programming dpjesp                                  sl hr hr history agent observa•  examine singleagent pomdp literature inspi•     tions sl current state section  demonstrates   ration algorithms exploit dynamic programming    use multiagent belief state construct dy•  incrementally construct best policy simply namic program incrementally constructs optimal pol•  search entire policy space monahan  cassandra et    icy agent   al  kaelbling et  algorithms rely   principle optimality states each subpolicy  dynamic programming algorithm   overall optimal policy optimal words   following model singleagent valueiteration al•  tstep optimal policy given history    gorithm dynamic program centers value func•  steps portion policy covers tion tstep finite horizon readability sec•  — steps optimal remaining  tion presents derivation dynamic program                                                                                                    multiagent systems                                                               updated                                                                 obtained using equations   bayes rule given                                                                 follows                                                                                                                                                                                              treat denominator equation                 figure  trace tiger scenario                pr normalizing constant bring                                                                 sum numerator  result   twoagent case results easily generalize nagent  enters computation future expected reward   case having fixed policy agent  value function second term equation  compute                                                                 agents new belief state future expected reward   vtbt represents expected reward team re•  ceive agent  follows optimal policy tth   overall value function turn using agents   step onwards starting current belief state       current belief state primitive elements given   start end time horizon  mtdp model having computed overall value function   work way beginning way con•    vt extract form optimal policy   struct optimal policy maximizing value function    maps observation histories actions required   possible action choices                                 equations                                                                     algorithm  presents pseudocode overall dy•                                                                namic programming algorithm lines  generate                                                                 belief states reachable given initial belief state   define action value function recursively                              possibly unique belief                                                                 state seauence actions observations agent                                                                  reachable belief states                                                                 reachability analysis uses belief update procedure algo•                                                                rithm  time complexity                                                                 invoked belief state time over•  term equation  refers expected immediate reachability analysis phase time complexity   reward second term refers expected future re• lines  perform heart   ward belief state updated performing action     dynamic programming algorithm                                                                 time complexity   lines    observing  base case — future                                                                 translate resulting value function agent policy   reward  leaving                                                                 defined observation sequences required al•                                                             gorithm argument phase lower                                                                 time space complexity    calculation expected immediate reward breaks      phases considers optimal actions   follows                                                   agent  overall time complexity algorithm                                                                  space complexity                                                                 resulting value function policy essentially product                                                                 number reachable belief states size   compute immediate reward using      belief state representation    agents current belief state primitive elements  piecewise linearity convexity value   given mtdp model section                                                                        function     computation expected future reward second   term equation  depends ability update agent    algorithm  computes value function be•                                                                lief states reachable given initial belief state    belief state bt light new observa•                                                                subset possible probability distributions   tion example figure  belief state                                                                      use dynamic programming entire set   updated performing action     receiving obser•                                                                   show chosen value function piecewise   vation  derive algorithm performing                                                                 linear convex pwlc each agent faced solving   update computing remaining                                                                 single agent pomdp policies agents   term equation  initial belief state based                                                                 fixed shown section  sondik  showed   distribution initial state                                                                  value function single agent pomdp pwlc                                                              value function equation  pwlc addition       multiagent systems                                                                                                   
