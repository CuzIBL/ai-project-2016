                        does new simple gaussian weighting approach                                    perform text categorization                         giorgio maria di nunzio                               alessandro micarelii                 dip di ingegneria dellinformazione                 dip di informatica automazione                   universita degli studi di padova                   universita degli studi roma tre              gradenigo  padova  italia della vasca navale  roma  italia                         dinunziodeiunipdit micarcldiauniromait                             abstract                               expert human problem classifying new                                                                  unknown documents        new approach text categorization prob•                                                                   starting idea simplicity decided design        lem presented called gaussian weight•                                                                 training algorithm corresponds action finding        ing supervised learning algorithm                                                                  information parameters given set categories        during training phase estimates sim•                                                                 documents each term category calcu•       ple easily computable statistics                                                                  late presence percent documents        presence term  present cate•                                                                 category term  appears        gory expressiveness  present                                                                  expressiveness term  absent        outside rest domain                                                                  categories domain model gaus•       learned information gaussian function                                                                  sian function parameters value        shaped each term category order as•                                                                 abscissae equal  used weight term        sign term weight estimates level                                                                   called learning approach gaussian weighting        importance particular category tested                                                                  important stress fact dont use stemming        learning method task singlelabel clas•                                                                 algorithm feature selection function yang peder       sification using reuters benchmark                                                                  sen  eventual reduction number features        outcome result quite impressive dif•                                                                 category using thresholds named thresp        ferent experimental setups reached micro                                                                 threse relative parameters        averaged flmeasure  peak         macroaveraged recall precision             make evaluation gaussian weighting comparable        calculated reported  lat•      published results tc chose reuters       ter  results reach state         corpus benchmark singlelabel classifica•       oftheart techniques machine learning applied         tion task three different tests performed        text categorization demonstrating new       complete automated learning approach run        weighting scheme does perform partic•        use gaussian function simply giv•       ular task                                                ing weight proportional second                                                                  test runs gaussian weighting ap•                                                                 proach proposed       introduction                                                outcome results quite surprising through•  consider problem automated classification text doc•  different experimental setups repeatedly reached   uments problem great importance accessible  microaveraged fl measure  peak   textual information increases volume online texts    microaverage known highly   available through internet expands rapidly solution   influenced frequent categories yang liu    categorize documents according topics before•   decided compute macroaveraged recall pre•  hand real time                                          cision results reached satisfactory macro     number different machine learning methods         recall  macroprecision    applied text categorization tc including prob•  results reach stateoftheart techniques ma  abilistic classifiers decision trees regression methods neu• chine learning applied text categorization dumais   ral networks support vector machines sebastiani     et al  demonstrating new weighting scheme    methodologies share common factor      does perform particular task low   high level complexity makes classifier design computational cost algorithm propose makes   difficult understand                                       approach particularly preferable computing power     question addressed paper possible low   simple learning algorithm uses naive human com•  prehensible parameters way act like non        wwwdaviddlewiscomresourcestestcollcctionreuters       learning                                                                                                                   algorithm                                             locality presence expressiveness    section present supervised algorithm approach uses weighting method terms each    analytically define parameters presence   class sense local type weightingmodeling    expressiveness explain training al•  scheme classes idea similar local lsi rep•   gorithm works classifies new documents     resentation hull  order characterize    runtime                                                  term space singular value decomposition applied                                                                  matrix consisting known relevant documents       presence expressiveness                         case documents belonging class    central key naive approach computation  substantial differences weight   easy human understandable parameters pres•      ing scheme counts number occurrences term     ence expressiveness captures    document counts documents  ap   term appears documents belonging     pears computes number documents    category estimates       appears collection    term does appear documents cat•     calculates according class computing    egories given set categories                            weights partial averaged presence domain        each category having number documents                                        following        learning gaussian weights gw    definitions                                                  paragraph explain shape gaussian func•                                                                 tion using values starting definition        number documents    ith category                                                                generic gaussian function                                                                                                                                  number documents category  term   estimate parameters follows         present                                                                                                                                                                                                                                                                                                                     gaussian weighting gw term cate•                                                                 gory defined substituting       note use symbol  denote presence    term inside category document    confused symbol used conditional                                                              probability pxy      presence term ith category   gw calculated   returns real value belonging    using                                               interval particular maximum weight                                                                  reached presence term equal                                                                   say appears each document category                                                                  figure  shows example gw      expressiveness term ith category                                                                    continuous      calculated using                                                                      dashed terms presence                                                                  higher expressiveness higher weight                                                                  computing parameter   important stress term different cat• algorithm compute following   egories different values expressiveness fact                                                                     each category   better explained table  term present three   categories presence shown row sec•           ond row expressiveness term each             each document   category note  category  greater expressive•                                                                    ness given relatively smaller presence term   rest domain                                                   each unique term                                                                                                                                                                                                                            add                                                                         end   table  numerical example presence expressive•  ness                                                              end                                                                     each term                                                                                                                  learning                                                                   each                                                                     output                                                                        each term                                                                         test                                                                                                                                              output                                                                      nterm  nterm                                                                         end                                                                                                                                       end                                                                    assign document highest output                                                                   each category algorithm calculates mean                                                                  activated gaussian functions terms                                                                  greater fixed thresholds respectively thresp    figure  example gw continuous line gw        threse computational cost algorithm       £   dashed line gw        higher expressiveness    term possesses bigger output gw gives                  experimental setup                                                                   test collection       end                                                 make evaluation gw comparable                                                                  published results tc chose reuters corpus     end                                                   benchmark during years corpus    assume use hashmap hm store results     used standard benchmark tc methods    update search hm con•        evaluated results dif•  stant cost worst case algorithm compute   ficult compare slightly different version used   parameter computational cost                        paper used modapte split reuters   computing parameter                                       stories  used training doc•                                                                 uments build classifier remaining     algorithm compute following                                                                  test accuracy singlelabel classifier       each category                                        reuters known quite unbalanced distri•     sum                                                     bution stories category  poten•                                                                 tial topics categories  frequent used         each term                                                                   categories account  training          each category £                        instances remainder distributed            tin                                               table  shows number training test samples                                                                  each category            sum • sum           end               end     end   assume hashmap constant   cost access search computational cost   second algorithm  quadratic cost   respect number categories      categorizing new document   trained parameters   each term domain possible                                                                 table  number training test documents   feed unknown document according                                                                 frequent categories reuters modapte split   value couple optional thresholds thresp   threse classify following algorithm       learning                                                                                                                 parameters setting                                       yang pedersen  decided calculate   performed three different test runs          macrorecall mre macroprecision mpr                                                                       test run didnt use gaus•       sian function tried run        simple function use       gw test run uses gw function defined equa•       tion       simple gw test run simplified gaussian        function variance equal                            results   tests maintained threse equal  say  test run   term category expressivity value greater   test weight each term  category com•   value permitted achieve highest perfor•  puted   mance during test phase thresp varied   range  terms categories used    terms present  training    test run results shown figure    documents used     number terms category according value   reported table                                                                      figure  test run xaxis represents                                                                 values presence     table  table shows number terms category   according threshold presence row reports   average features category       stoplist  terms used remove   frequent words english language stemming                  table  test run   feature selection performed     order evaluate accuracy classifier    particular test did expect remarkable   computed standard measures recall pre•    performance considering weight term proportional   cision following averaging measures microrecall    parameters possible solu•         microprecision microaveraged fl mea•             tion investigate performance   sure microfl                                              influenced significantly threshold thresp                                                                 set   worth stressing test per•                                                                formed better terms belonging                                                                 stoplist used                                                                    gw test run                                                                 results run shown figure  numerical                                                                 values reported table                                                                     test performed using equation  calcu•                                                                late weight each term category best results   particular reported values flmeasure   obtained thres macrorecall   order directly compare results ones macroprecision quite high indicate sys•  literature microaveraged measure known    tem works category performance   highly influenced frequent category    starts decrease sensibly thres                                                                                                                   learning                                                                           table  simple gw test run                                                                  category decreases   sharp                                                                decay influences macroaveraged performance sys•                                                               tem fact macrorecall retains values                                                                macroprecision cutoff  fact in•                                                               dicates incorrectly assigning stories   figure  gw test run test function  used                                                                categories domain biggest ones   weight each term                                                                thresp gets bigger performance sys•                                                               tem shows classic behavior recall tends                                                                                                    precision     micro                       test run didnt expect good re•   macro                     sults simple proportional weight assignment performed    macro pr                     approaches performance                                                                decays slightly rapidly test important   table  gw test run each column reports values  confirm idea using simplified learning approach   micro macro recall macro precision each thresp  solve problem tc                                                                  second test runs show    simple gw test run                                   values oneperthousand differences con•                                                               sidered relevant moment draw con•  results second run shown figure  numer•                                                               clusion three learning approaches   ical values reported table                                                                 perform better general plan investigate mat•                                                               ter testing complete reuters test                                                                collections                                                                  comparative results                                                                dumais et al tested number inductive learning algo•                                                               rithms reuters modapte split used                                                                dumais et ai  results arc briefly summi                                                               rized table   frequent categories       figure  simple gw test run test variance   gw equal        test performed using simplified version table  microavcraged flmeasure  frequent   equation  calculate weight each term cat• categories reuters reported dumais   egory variance gaussian function set         best results obtained thresp        findsim method variant rocchios method rele•      does particular difference vance feedbackrocchio  naive bayes classifier   gw test run cases thresp equals      constructed using training data estimate prob•  macro precision sensibly lower                 ability each category given document feature values                                                                new instance discussion indipendence assump•   discussion                                               tion naive bayes classifier lewis    results reported satisfactory situa•  bayesnets bayesian network uses dependence   tions reached accuracy comparable ones  bayesian sahami  example bayes   stateoftheart systems aspects need dis• nets classification tree decision tree approach de•  cussed performance test   scribed chickering et al  linear   runs decades thresp exceeds  say    svm linear hyperplane separates set positive   looking table  number features   examples set negative ones joachims        learning 
