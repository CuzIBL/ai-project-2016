     avoidance model reinduction            svmbased feature selection text                                             categorization                        aleksander kołcz                             abdur chowdhury               microsoft research live labs              illinois institute technology                      arkmicrosoftcom                              abduririitedu                          abstract                          features certain count small                                                        important able estimate point perfor      searching feature space subset yielding mance curve classiﬁer measured number      optimum performance tends expensive es    informative features achieves maximum      pecially applications cardinality “levels off” unfortunately search optimum feature      feature space high text categorization settings timeconsuming repetitive model      particularly true massive datasets training      learning algorithms worse linear scaling  work investigate alternatives svm model      factors linear support vector machines svms    induction during feature selection able demon      performers text classiﬁ strate proposed techniques result substan      cation domain work best rich  tial gains terms computational efﬁciency actually      feature representations beneﬁt lead accurate solutions typically lead equiva      reducing number features   lent results feature selection criterion focus      large extent work propose alterna feature ranking induced svm      tives exact reinduction svm models during  text categorization domain compares fa      search optimum feature subset ap vorably mladenic et al  mainstream feature      proximations offer substantial beneﬁts terms selection criteria information gain rogati yang      computational efﬁciency able demon          strate signiﬁcant compromises terms      model quality      cases gains accuracy achieved             modeldriven feature selection svms                                                        linear svm creates classiﬁcation model attempting                                                        separate elements different classes maximum    introduction                                       margin vapnik  problems linearly separa  linear support vector machines svms vapnik   ble results identifying subsets positive  best performers tasks negative examples lie exactly margin —  involving text categorization joachims lewis et al called support vectors svs quite common   richness natural language text cat problem linearly separable case  egorization problems characterized large num support vector set enriched training examples  bers features infrequent features removed classiﬁed model correctly softmargin  dimensionality attribute space tends high svm cortes vapnik  balances margin  ≈ learners poses computa total loss training data case weight  tional challenges andor leads overﬁtting training data vector svm solution derived convex optimization  case linear svms good performance given linear combination svs output  cases achieved little feature selection joachims svm input vector expressed  lewis et al mladenic et al                        ⎛          ⎞  best performance using features means guaran                                                                                                          ⎝       j⎠  teed shown gabrilovich markovitch   wixi           αjdi  xi     relatively easy identify text classiﬁcation prob                  lems best accuracy achieved aggressive feature  selection                                            di value corresponding feature train    optimum performance achieved selec ing example wi weight assigned ith feature  tion dependence classiﬁcation accuracy svm αj lagrange multiplier associated                                                                                  number features used exhibits saturation value αj  unless support vector   improvement accuracy increasing number wise αj   softmargin svms lagrange multipliers                                                    ijcai                                                                              satisfy  αj ≤ ∈−  class label datasets rich feature representations                 associated di                                   common text categorization applications identifying    svms proven quite robust dealing large optimum quite expensive  numbers features overﬁtting tends published results svmbased feature selection indicate  sue compared learners applied text classiﬁ svm good feature ranking text applications  cation domain feature selection needs applied interesting question “how stable original  svms reported perform established solution subject subsequent feature selection dif  feature selection approaches ig χ bns ferently interested original solution sig  man                                            niﬁcantly reoptimized restricted domain using    recent work mladenic et al  investigated efﬁ topn features  cacy using feature ranking imposed trained svm linear svms quality solution primarily  ranking according absolute fea termined orientation hyperplane normal given  ture weight values outperforms established alternatives direction vector ﬁnal position hyper  use justiﬁed mladenic et al  fact plane controlled bias term  adjusted  sensitivity margin separating classes deter satisfy particular utility function misclassiﬁca  mined linear svm changes jth feature di tion cost precision recall  rectly dependent sensitivity norm weight feature subset obtain projection  vector changes expressed mladenic original hyperplane normal reduced feature space  et al                                          obtained ignoring “masking coordinates                                                      corresponding removed features contribution                                                      δ                             address following questions                          w ∝                                                     •               i∈trn set δxi                                direction projected hyperplane normal sub                                                            stantially different direction normal  features receiving low absolute weights ex duced reduced feature representation  pected little effect orientation optimum • does classiﬁcation performance solution  hyperplane                                               associated projected hyperplane normal com                           mladenic et al  proposed apply criterion pare solution induced reduced feature                                ﬁlter fashion john et al   ranking representation  obtained just subsequent attempts identify op                                                          •  timum subset use topn features according initial membership support vector set affected  ordering guyon et al  followed alternative wrap reducing dimensionality training data                             approach john et al  each subset  normal vector feature masking  relevant features removed svm model trained  remaining ones provides new ranking let consider simple procedure uses original  used identify subset features remove svm model derive operates reduced                                                                    discussed hardin et al  svm assign space features approximation original  low weights features redundant given presence model unchanged weight vector                                                                                           relevant features recursive approach guyon components intersect feature subset  et al  likely successful compen retained corresponding feature masking transformation  sating effect especially desired identify applied documents output masked model                                                        expressed  fairly small set relevant features                                     ⎛              ⎞    text domain optimum number features                            tends large differences quality mod                          ⎝           ⎠                                                        fmask     miwixib            αjy midi   xib  els utilizing recursive nonrecursive approaches                      j∈sv  small li yang kalousis et al  work focus exclusively ﬁlter variant svm                                                                                               ∈m  based feature selection proposed mladenic et al                                                                                                i∈m    avoiding model reinduction svmbased            set features masked     feature selection                                    gain insight properties masking let                                                        start original model assume feature  traditionally investigation efﬁcacy different feature masked index given original solution  subsets relies inducing model each particular choice optimal linearly separable problem minimizes  subset comparing results achieved depending lagrangian  type learner repetitive model reinduction                                          carry signiﬁcant cost terms compu α · −    α yj  · dj  −                                                                                        tational time required case linear svms cost                  j∈trn set  quadratic terms number training instances                                                                                         yj · dj  −  ≥       linear terms number features used massive subject                                                                           ijcai                                                     satisﬁes ﬁrstorder local minimum conditions note renormalization typically applied              δl                                       default reinducing svm model using topn fea                  −         α yjdj               δw                                   tures                       j∈trn set              δl                               α yj                          experimental setup              δb                              j∈trn set                            used linear svm induced features baseline  let mask nth feature vector each source feature ranking given original fea                 dj                                     ture set methodology examine quality mod  training vector  keeping lagrange multipliers           unchanged consider optimization problem els using features asses extent impor  −  dimensional space notice derivatives tance differences compared effectiveness  new lagrangian  respect remain exact model reinduction proposed alternatives   original solution projected lower following document collections  dimensional space meets necessary optimality conditions trecap dataset represents collection ap  respect true solution newswire articles trainingtest split  space feature ignored needs maxi scribed lewis et al  used collection  mize respect α meet constraints  consisted  training  test documents  constraints  met originally equality divided  categories each document belonged  sv set strong inequality training category  vectors sake argument let assume  reutersrcv  hold points outside sv set true svs           collection represents                                                            recent largest corpus used research involving text  constraints violated feature                        actually present given sparsity text categorization   training documents                                                                                              small fraction sv set        test documents described lewis et                                                                      violation each sv                        al  ways grouping docu                                                            ments restricted topic ontology   yj · dj  −  − yj w−n · dj  −   yjdj                                                        using topic categories represented train                                                         ing portion data  categories total  seen small value wn  leads small                            −n                              document belong topic  departure optimum represents vector  certain cases hierarchical relationship  nth feature masked                             topic considered subtopic    based expect keeping                                                         techtc  twoclass problems based  values lagrange multipliers ﬁxed masking low                          weight feature achieve solution lies close open directory project generated purposefully rep  optimum reduceddimensionality space va  resent different challenges svms far feature                                                                                   lidity assumption increase features selection concerned gabrilovich markovitch                                                                  infrequent inequality constraints fewer training    points affected features assigned low ab  document representation  solute weights departure optimality likely  small                                            documents preprocessed removing markup                                                        punctuation converting characters lower case ex    feature masking document normalization       tracting feature tokens sequences alphanumerics  text domain transforms document feature limited whitespace case rcv  vectors unit norm according compensate techtc collections used pretokenized feature  document length variability joachims dumais et ﬁles provided corpus websites indocument term fre  al leopold kindermann  transfor quency ignored each feature received weight  mation introduces feature weighting uniform fea appeared document —  tures document varied documents weight zero binary representation experience  normalbased feature masking preserves original feature performs tfidf coupled document  weighting relevant features removed length normalization importantly context  counters extent original length normalization work binary representation magnitude pertur  alternative consider retain set svs bations  conditions  depended primarily  associated lagrange multipliers renormalize train svm weights independently derived factors  ing vectors each set features removed output tfidf length normalization applied  model given assuming length normal beneﬁcial text classiﬁcation joachims  ization                ⎛                       ⎞                  httpwwwdaviddlewiscomresourcestestcollectionstrecap                             dj                          httpwwwdaviddlewiscomresourcestestcollectionsrcv                ⎝                     ⎠    fsv         αj                   mixi          httptechtccstechnionaciltechtctechtchtml                                                                               mk · dk · dk                 httpwwwdmozorg                                                    ijcai                                                     dumais et al leopold kindermann     words occurred just training corpus        eliminated                                                                      experimental procedure                                                                    multiclass categorization problems broken  series oneagainsttherest tasks each class          treated turn target remaining ones playing  avg    role antitarget techtc dataset natu                                        trecap                      rally consisted  twoclass tasks unlike                                         exact  rcv twoclass problems fairly                                  sv set                                                                                        mask  balanced comparable numbers training test ex                                                                    amples available target antitarget                                        classiﬁcation performance measured terms                   percentage features used  area receiver operating characteristic roc figure  average bestf function topn features  curve auc best attainable value tuned trecap  decisionthreshold adjustment metrics estimated  test data reported terms macro  average best microaverage auc        categories                                                                        each twoclass task svm model induced using  features ranked according ab         solute weight values features represented sv set        removed considered using just fraction                                                                   avg  best  auc  ranking features respect nonzero weight       ones values                 addition computing average performance ﬁgures                  mask  svset exact  each fraction provided average best  sults category basis acknowledges op figure  averagebest auc  twoclass prob  timum number features according given performance lems techtc according onesided paired  metric change twoclass problem test difference exact masking approaches    labeling results exact reinduction approach signiﬁcant pvalue  − difference  noted exact normalbased feature masking denoted exact sv set approaches pvalue   mask weight recomputation using masked  renormalized sv set labeled sv set                                                        chosen lower feature counts differences    results                                            pronounced                                                                    techtc    accuracy effects                                                collection consisted prob                                                        lems balanced numbers positive negative ex  table  compares average auc best results amples reporting classiﬁcation accuracy limited  trecap reutersrcv    collections selves average auc metric following gabrilovich  best feature selection results determined cate markovitch  fold crossvalidation used  gory basis seen best results according estimate accuracy onesided paired ttest applied  metrics numerically close each estimate signiﬁcance differences classiﬁcation  each dataset using three model feature selection ap formance  proaches according pvalues differences average best auc results using features  exact approximate approaches considered three feature selection methods shown figure  note  statistically insigniﬁcant used macro ttest outlined approaches optimize feature set substantially  yang liu  bestf measure better using features differences sta  normalbased masking method rcvoverthetwo   tistically signiﬁcant pvalues ≈  pvalues show  collections best auc performance achieved sv set approach statistically equivalent exact  features best performance normalbased masking underperformed  varied illustrated figure  case trecap case given techtc speciﬁcally designed  apparent reducing number features illustrate beneﬁts feature selection svm  beneﬁcial effect exempliﬁes fact opti surprising big gains auc shown figure   mality feature selection dependent performance  criterion note high fraction ranking fea correlation effects  tures retained   essentially difference aside measuring impact feature selection  exact approximate feature selection method classiﬁcation performance interesting investigate                                                    ijcai                                                     table  average best auc results exact approximate methods feature selection trecap  reutersrcv   collections pvalues onesided pairwise ttest macro given determining point  differences approximate variants exact approach considered signiﬁcant                               best auc                               best                     exact     maskpval     sv setpval  exact     maskpval      sv setpval          trecap                         rcv                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                support set overlapcontainmentsupport  set                weight  vector  correlationcosine  sim pearson coeff                       sv overlap                                                                                             cosine similarity                                      sv containment                                                                                                                                                                   percentage features used                           percentage features used                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      pearson coeff                      overlapcontainmentsupport  set                weight  vector  correlationcosine  sim                                     sv overlap                              cosine similarity                                    sv containment                                                                                                                                                                   percentage features used                           percentage features used    figure  pearson correlation coefﬁcient cosine similarity figure  overlap original set support vec  masked original weight vector weight tors set obtained training reduced feature  vector obtained svm retraining using topn features representation trecap rcv  trecap rcv                  fraction containment svs corresponding original                                                        sv set shown  similarity weight vectors assigned origi  nal svm svm obtained using reduced feature rep svs tends decrease fewer features used  resentation                                          show fraction svs corresponding original sv set    figure  shows dependence weightvector simi sv set obtained using reduced feature  larity according pearson correlation coefﬁcient representation seen sv set overlap quite high  cosine similarity number topranking features used exceeds  long sizable fraction original  trecapandrcv datasets weight vec features used  smaller values nthe  tors close each fraction overlap goes mainly decrease  topn features used small direction hyperplane number svs cases fraction orig  normal vector weakly dependent inal svs used remains consistently high overlap  relevant features projecting original weight vector containment sv sets approach  frac  subspace ranking features obtains tion features used approaches   direction oriented close optimum    fraction deﬁned respect set features    figure  shows set average overlap origi ceived nonzero weights original svm run  nal set svs obtained using topn fea discarding zeroweight ones apparent  tures trecap rcv overall number use features deemed irrelevant did fact                                                    ijcai                                                     
