          optimizing classiﬁer performance word sense disambiguation                                redeﬁning word sense classes                                   upali kohomban        wee sun lee                                      department science                                      national university singapore                                     science drive  singapore                                  upalileewscompnusedusg                        abstract                          johnson  used contextual features classify                                                        known nouns wordnet lfs      learning word sense classes shown                                                          use wordnet lfs begs question      useful ﬁnegrained word sense disambiguation   better design sense classes speciﬁcally ﬁne                                   kohomban lee   com       grained wsd answer question afﬁrmative      mon choice sense classes wordnet lexicogra                                                        using clustering techniques automatically derive sense      pher ﬁles designed machine learning  classes show sense classes constructed way      based word sense disambiguation work signiﬁcantly outperform wordnet lfs used      explore use clustering techniques effort                                                        kohomban lee’s  method word ﬁnegrained      construct sense classes suitable wsd interesting result inevitable      word sense disambiguation endtask results   losses caused multiple ﬁnegrained senses falling      show classes signiﬁcantly improve                                                        sense class dramatically smaller      classiﬁer performance state art                                                        number sense classes kept num      sults unrestricted word sense disambiguation  ber wordnet lfs additionally method applied                                                        parts speech nouns verbs wordnet    introduction                                       lfs effectively used resulting wsd  problem faced research word yields state art results senseval  english  sense disambiguation wsd acquiring labeled training allwords task evaluation datasets result senseval  data supervised learning crucial problem data best aware  unsupervised learning shown comparable  results supervised systems labeling data  generic word sense classes motivation  wsd laborintensive                               work borrows kohomban lee  crestan    way overcoming problem reduce speci et al  major idea classify word instances  ﬁcity senses focusing dominant senses mo coarsegrained set word sense classesandifwe  hammad hirst  addition max know ﬁnegrained senses fall classes  imize use knowledge gathers available la use ﬁnegrained wsd  beled data identifying common ‘classes’ word senses replacing resulting coarsegrained classes  depending similarities usage         frequent ﬁnegrained sense each class    wordnet lexicographer ﬁles lfs sense groups way lose senses accuracy kohom  created manually during construction wordnet ban lee  argued loss affordable  fellbaum  provide rough classiﬁcation given classiﬁer gain coarser granularity  senses instance ﬁrst senses nouns cat dog fall coarser classes reduce data sparsity results  lf animal ﬁrst sense verb dance falls paper show properly designed classes loss  motion wordnet  lfs deﬁned nouns    far smaller loss wordnet lfs previ  verbs lfs intuitive choice semantic ously used  classes ‘supersenses’ words reasons working principle uses ﬁnetocoarse  cluding popularity wordnet lexical resource sense mapping convert available data labeled  availability data labeled respect wordnet senses ﬁnegrained senses training examples coarsegrained  works discuss use lfs ﬁne grained wsd  classes classiﬁer uses training examples different  crestan et al  classiﬁed word instances wordnet words label word instance class containing  lfs senseval evaluation exercise kohomban lee senses ﬁne grained sense assigned   proposed training examples different words manner described needs scheme  utilized learn wordnet lfs work mapping ﬁnegrained senses coarse set  used ﬁnegrained wsd nouns verbs ciaramita generic words previous work used wordnet lfs                                                    ijcai                                                      wordnet lfs designed work  senses fall class showing similar  generic set classes wsd thinking wsd ap age patterns labeled corpus show consistent behav  plication setting identify issues hinder ior basic reasoning inductive  wsd performance lfs used sense classes learning                                                          address issue senseloss coarse grain na  feature coherence                                                        ture wordnet lfs having larger number classes  commonly used features wsd available wordnet does lfs  text collocations speech best                                                          argued wordnet hierarchy encodes  knowledge proven evidence wordnet lfs                                                        human handcrafted knowledge  form cohesive classes term features counter exam                                                        tained possible construction coarse  ples                                                        grained classes tested idea partitioning word    wierzbicka  instance provides examples                                                        net hierarchy segments ﬁner wordnet  show closely related wordhypernym pairs                                                        lfs retaining wordnet hierarchical relationships  share usage patterns word pairs applefruit                                                        each partition result shows method does  ﬁshanimal insectanimal readily interchangeable                                                        work reducing sense loss  practical language usage cohesive parts                                                        ﬁnal classiﬁer performance  taxonomy “there animal collar” sounds                                                          section clustering  odd insect animal wordnet terms                                                        schemes purely feature based wordnethierarchy  linguistic usage different assuming examples                                                        constrained section  analyze managed  class merely introduce noise                                                        reach design goals feature coherence sense granu  hand contextually similar usages                                                        larity using feature based sense clustering section   furtheraway wordnet taxonomies semantic reasons                                                        present framework set evaluating perfor  making impractical differentiate senses using                                                        mance classes real endtask ﬁne grained wsd  textual features semantically close word                                                        section  discusses results show improve  senses assigned totally different lfs instance iono                                                        ment state art possible com  spherelocation stratosphereobject                                                        paring previously published results    problem wordnet lfs lfs  subsumed food instance subset sub  stance create confusion features learning  clustering schemes  lfs arguably close meanings cog section describes implementation proposal  nition feelingandmotive hard differentiate automatic generation classes based features  using contextual features possibly better control experiment used similar techniques ob  group single class                       tain classes constrained wordnet hierarchies    lfs adjectives relate underlying referred fb wn respectively  concept contextual features applicable syntactic lexical features text necessarily cor  generic semantic classes                             relate reason decided test different clus  loss senses                                        tering arrangements respectively based local  coarser classes greater chance given context speech features labeled data  class includes sense given word ﬁne sections   details data features features  sense class mapping described beginning sec represented binary vector local context feature vec  tion manytoone mapping lose senses tors large dimension sparse used  each word reverse mapping resulting errors ﬁne singular value decomposition reduce feature space dimen  grained wsd granularity senses ﬁnegrained wsd sion discard elements singular values smaller  attain wordnet lfs poor having   largest data obtained used fb wn  classes nouns  verbs                   schemes each scheme class arrangements based                                                        local context pos features    clustering solution  order address issues suggest direct  purely featurebased classes fb  taskoriented approach                               section discuss clustering senses independently    using features text ﬁnd common groups original wordnet hierarchy target better  senses based context shown useful pre featureclass coherence idea long cor  viously lin pantel  magnini cavaglia  pus behavior senses remain possible  use idea generic sense classes using clustering assume hypothetical generic ‘class’  techniques try generate automatically set ‘classes’ regardless able understand label exact  word senses based lexical syntactic features semantics class ﬁnd classes using la  classes directly based features beled data possible use wsd  unlike wordnet lfs expect easier learn place wordnet lfs described section   ing features new linguistic assumption sense represented average vectors labeled  assumption classes instances corpus sense omitted senses                                                    ijcai                                                    absent labeled corpus wsd task                                                                                                  wns  section  considered classes                                     wns                                                                                          wnsc    clustering algorithm inspired kmeans al                                  fbs                                                                                              fbs                                                                                              fbsc  gorithm guan et al  instead initializing clus   ters randomly chose base original wordnet  lfs instead iterating ﬁxed number clusters       used method growing new clusters outliers  existing clusters each iteration kmeans algorithm     calculate variance clusters formed check     squared distance each point cluster cen  troid ratio distance variance larger   given constant point isolated new cluster                     reaching convergence reﬁnement  cluster smaller number members figure  proportional ‘loss’ senses vs number classes  desirable merge nearest cluster chosen nouns fb wn sc senseval   simplelinkage condition cluster cj ‘nearest’ allwords task data semcor optimal clustering high  cluster ci  jifcj node shortest pos lighted leftmost points wn correspond wordnet  sible distance node ci allows nonspherical lfs featurebased classes consistently yield better sense  clusters preserving size clusters certain separation smaller numbers classes  limit                                                                     clusters formed straightforward cre                               wns  ate sense mapping used classiﬁer                                wns                                                                                          wnsc  discussed section  applied method                               fbs                                                                                              fbs                                                                  nouns verbs adjectives                                                            fbsc                                                                      classes constrained wordnet wn                                                                   build trees wordnet hierarchy senses                                                                  nodes hypernyms respective parent nodes trees  belong lf connected root        node feature coordinates earlier each sense                                                                  added tree respective node given tree                      segment centroid coordinates calculated av  eraging sense coordinates segment average figure  proportional ‘loss’ senses vs number classes  square distance centroid measure cohesiveness verbs details ﬁgure   tree consider each node tree candidate break  ing point decide break checking split  gives largest reduction total variance  sense resolution  partitioning proceeds greedy manner selecting each compared ‘sense loss’ fb classes  run node gives best overall improvement wn classes    earlier smaller clusters removed merging recall section  sense loss occurs losses  pick geometrically nearest clus counted errors ﬁnegrained wsd  ter merge distort wordnet hierarchy minimizing losses desirable quality classes given  sistency requirement cluster merged class mapping labeled corpus assess loss  point originally detached          counting proportion labeled instances belong    adjectives adverbs organized proper ‘lost’ senses figure   shows loss labeled data  tree forms hypernyms method sets fb wn different numbers classes wn  limited nouns verbs                  starting points graph original wordnet lfs                                                          schemes beneﬁt larger num    effects clustering                              bers classes additional gain fb did                                                        anticipate achieves good resolution smaller  section analyze empirically basic effects numbers classes number classes  clustering schemes discussing effectively man wordnet lfs possible obtain  reduc  aged obtain properties desired design goals tion sense loss recall fb split clusters                                                        reorganize points order reduce variance wn    constant chosen slightly maximum split reorganizing violate hierarchy  distancevariance ratio ﬁrst iteration means fb theory achieve better optimization    lexical ﬁle arrangement adjectives semantically based number clusters work prac  clusters initialized three available tice added advantage smaller clusters                                                    ijcai                                                                       local context     pos              lfs used original work evaluating                     nouns  verbs  nouns   verbs        performance resulting wsd       wordnet lfs                 wn                             data       fb                           use labeled data semcor corpus fellbaum                                                         chapter  training data determine global classiﬁer          table  average information gain values      settings randomly selected  instances                                                        each speech heldout validation data set  reducing sense loss undesirable prop wordlevel validation employed randomly picked  erty including fewer number senses limits word instances  training data kept  number training examples class                aside evaluation senseval senseval    actual wsd task used cross validation guess edmonds cotton  snyder palmer  en  best number classes each clustering glish allwords task data sets tests use wordnet   shown highlighted graphs                      senses    feature coherence                                  features  observed given fb class include group features used learning implemented way  ings different semantics instance small noun described kohomban lee   class dominated three distinct ‘themes’  local context −n symmetric window  kneecap forearm palm coattail words sides word consideration  coat shirtsleeveandhomeland motherlandbut size window ∈   chosen cross valida  mix does pose problem similarity weighting tion words converted lower case punctuation  instances section  lessen inﬂuence unre marks excluded  lated words training instances similarity measure based speech similar local context window  wordnet hierarchical proximity introduces hi using rules considering parts speech  erarchy information classiﬁcation ensuring punctuations exceeding sentence boundaries  contextual taxonomical coherence                 grammatical relations included basic syntactic    empirically evaluate classes sepa lations subjectverb verbobject prepo  rated features endtask classiﬁer calculated fea sitional phrases ‘sound bells’ word sound  ture information gain values token poslocal context  window complete semcor data set information gain  classiﬁer  feature set values vi given           obtain clustering process mapping                                                       ﬁne grained senses respective class number each            wi  hc  −     · hcv            ﬁnegrained sense labeled instance training set used                         v∈vi                           classiﬁer example particular class using                                                       class mapping training data each word limited    hc−      c∈c clogp entropy                                                      instances belonging classes include  class distribution  provides measure senses word classiﬁer used timbl memory  given set classes separated given feature                                                    based learner daelemans et al  essentially  class distribution feature values each knn classiﬁer  feature available semcor straightforward apply                                                        kohomban lee   showed noise  formula obtain given feature table  shows using examples different words reduced  information gain measures nouns verbs semcor examples weighted according relatedness  data local context pos features averaged exampleinstance word word labeled em  positions context windows seen wn ploy method nouns verbs experiment  fb clusters improve gain smaller class sizes using relatedness measure weighting proposed  fb clusters yield best gain                      jiang conrath jcn  measure takes                                                        account proximity senses wordnet    wsd endtask evaluation framework                  hierarchy information content nodes  order empirically validate effect prop path links  erties classes thought critical wsd instance weighting implemented modifying dis  endtask used originally described kohom tance Δx  training instance testing  ban lee  ﬁnegrained wsd measure stance  following way  ‘quality’ classes using instead wordnet                                                                                  Δx                                                                        Δ      improvements kohomban                     sxy     lee  published reran experiments reported  used wordnet lfs sense classes new results implementation uses information content data compiled  correspond ‘wordnet lfs’ entries tables    ted pedersen et al httpwnsimilaritysourceforgenet                                                    ijcai                                                    sxy jcn similarity frequent                  senseval  senseval  sense falls class  small baseline                  stant avoid division zero                              senseval best                separate classiﬁer used each three feature crestan et al                 type described three classiﬁers classiﬁer  predicts wordnet ﬁrst sense output par    table  baseline previous results  ticipated simple majority voting case tie ﬁrst  sense chosen                                                      noun    verb    adj  combined  weighted majority voting kohomban lee       baseline                   reported weighted majority algorithm littlestone war wordnet lfs             muth  increase classiﬁer performance simple wn pos                  majority algorithm ﬁnal combination used wn lc                    validation data pick best clustering scheme pos lo fb pos              cal context nouns adjectives did local fb lc               context based clusters verbs did pos based baseline                  clusters                                                wordnet lfs                final results                                            wn pos                    instance classiﬁed particular class wn lc                 decide ﬁnegrained sense belongs fb pos                  sense falls given class men fb lc                tioned section  use heuristic picking sense  smallest wordnet sense number class table  results different original wordnet lfs wn  motivated fact wordnet senses supposedly fb clustering schemes simple majority voting senseval  come descending order frequency           senseval data pos lc clus    each clustering words senses fall terings based pos local context features section   multiple classes classiﬁed described section  words assigned wordnet sense                                                           table  shows baseline wordnet ﬁrst sense  check multiword phrases separate wordnet en                                                        performance best systems reported senseval mi  tries phrases assigned sense  usually                                                        halcea  decadt et al aswellasthatof  sense adjective clustering applicable                                                        crestan et al  used wordnet lfs  wn mentioned earlier fb method used adjec                                                          table  shows results clustering schemes dis  tives wn adverbs                                                        cussed using simple majority voting section   cases resorted wordnet ﬁrst sense                                                        rerun original wordnet lfs  adjective similarity                                  sults given three parts speech combined  jcn similarity measure depends hypernym hierarchy described ‘final results’ wn clusters’  applicable adjectives hierar formance signiﬁcantly different original  chical organization kohomban lee  reported wordnet lfs constraining hi  jcn similarity measure help classiﬁers outper erarchy type localization effect original  form baseline addition adjective lfs three achieved jcn weighting yielding  number suitable wsd framework additional information preventing informative  fb clustering scheme applied adjectives examples used supports idea  group adjective senses smaller classes obtain good performance merely splitting lf  addition context vectors svd provide way ﬁner set classes  calculating intersense similarity coordinate vectors hand featurebased fb classes provide  resulting svd gives smoothed measure average contextuallybased information given hi  behavior sense idea successfully used erarchy complemented information  wsd previously strapparava et al  use jcn similarity measure based hierarchy  measure similarity using dot product words clustering independently hierarchy  coordinate vectors similarity senses  better way utilize different sources information    general used measure classiﬁer pro semantic taxonomical hierarchy lexicalsyntactic  cess yielded results outperformed baseline linguistic usage patterns  measure outperform jcn limited seen fb pos based clustering  use adjectives                               formed verbs local context based clustering                                                        did nouns adjectives rough explanation    results                                            verbs generally beneﬁt syntactic features  evaluated clustering schemes fb wn available through pos effect consistent  framework described section  clustering schemes  based pos local context features        numbers shown recall values using ofﬁcial scorer                                                    ijcai                                                    
