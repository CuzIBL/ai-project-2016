          exploiting known taxonomies learning overlapping concepts                                      lijuan cai   thomas hofmann                                      department science                                  brown university providence ri usa                                          ljcai thcsbrownedu                          abstract                          tween categories perceptron learning svm clas                                                        siﬁcation architecture rest paper organized      realworld classiﬁcation problems involve    follows section  introduces ways representing tax      large numbers overlapping categories ar onomy knowledge derive pairwise similarities      ranged hierarchy taxonomy propose  categories based relative locations taxonomy      incorporate prior knowledge category taxonomy  order tie learning categories second adapt      directly learning architecture present standard  loss function weigh misclassiﬁcation errors      concrete multilabel classiﬁcation methods accordance taxonomy structure section       generalized version perceptron hierarchi mulate hierarchical learning problem terms joint      cal multilabel svm learning method works    large margin problem derive efﬁcient train      arbitrary necessarily singly connected tax ing algorithm section  propose hierarchical percep      onomies applied generally     tron algorithm exploits taxonomies similar fashion      settings categories characterized section  examines related work section  presents exper      tributes relations necessarily imental results show new hierarchical algo      duced taxonomy  experimental results     rithms bring signiﬁcant improvements metrics      wipoalpha collection show hierarchical  clusions future work discussed section       methods bring signiﬁcant performance improve      ment                                                           utilizing known taxonomies    introduction                                         problem setting  realworld classiﬁcation tasks involve large numbers assume patterns documents represented  overlapping categories prominent examples include vectors ∈x ⊆rd mapped higher  international patent classiﬁcation scheme approx  dimensional feature space φx denote set cat  patent groups open directory project approx  egories  category ∈yanda  categories web pages gene ontology ap label set ∈ py py power set  prox  terms gene products cases taxonomy directed acyclic graph ve nodes  instances assigned category cat v⊇ysuch set terminal nodes equals formally  egories rarely mutually exclusive leads large  ∈v v ∈v  yv ∈ note  scale multilabel classiﬁcation problems categories assume taxonomy singly connected tree  typically organized hierarchies taxonomies com est allow converging nodes cases wants  monly introducing superordinate concepts relating express items belong supercategory  categories ‘isa’ relationships multiply connected tax terminal categories suggest model  onomies uncommon context             formally adding terminal node each inner node repre    believe taxonomies encode valuable domain   senting “miscellaneous” category avoids problem  knowledge learning methods able capital partial paths  ize particular number training examples multilabel learning aim ﬁnding mapping   individual classes small dealing tens x→py based sample training pairs xiyii  thousands classes potential loss valuable n⊆x×py popular approach suggested  information ignoring class hierarchies pointed instance schapire singer  actually  led number approaches employ learn ranking function categories each pattern  different ways exploit hierarchies mccallum et al   x→sqwheresq set permutations ranks   wang et al  dumais chen             order unique subset labels needs    paper present approach systematically address additional question select number  incorporating domain knowledge relationships categories pattern assigned                                                    ijcai                                                       common deﬁne ranking function implic assigning irrelevant item label set   itly scoring function  x×y →r     deﬁne ancy  ≡v ∈v   ∃y ∈ yv ∈  ancynowwe  gxy gxy xy fxy cate quantify loss following manner  gories higher values appear earlier ranking                                                                                              ˆ    −               −   −      ease presentation ignore ties notation gy used yy    sv     sv       sv    clear context                                           v∈ancy  v∈ancyˆ        v∈ancy                                                                                                  ∩ancyˆ     class attributes                                 note nodes symmetric difference ancy     following cai hofmann  suggest use scor ancyˆ  contribute loss following sim                                                                                          −      ing functions linear joint feature repre plify presentation assuming    sentation Φ inputs categories xy ≡ setting sv ∀v ∈v gets  yyˆ   w Φ                      weight vector following   ancy    ancyˆ  intuitively means colors                                               Φ  tsochantardis et al  cai hofmann  nodes path node color say  chosen form Λy ⊗ φxwhereΛy                                     ˆ                                                    blue nodes paths nodes color  λyλsy ∈ refers attribute vector repre                     ⊗                                  say yellow nodes colors blueyellowgreen  senting categories kronecker product correct blue nodes ones missed  interpret terms stacked vector individual weight               wt      wt                           yellow nodes ones incorrectly selected  vectors   s   leading additive types mistakes contribute loss proportional             xy    λ yw  x  composition                     general  volume  idea notion class attributes allow general during training loss function difﬁcult deal  ization place similar categories just directly involves sets labels like  training examples belonging category work pairwise contributions involving terms  absence information set  deﬁne                                                                                             λryδry  leads xywy x                          yy   ancy  ancy              going translate taxonomy information singly connected taxonomies eq  equivalent  attributes categories idea treat nodes length undirected shortest path connecting nodes  taxonomy properties formally deﬁne           y                                                                                      suggested wang et al   order relate                         ∈ ancy               state following proposition               λ                                                                    proposition  yyˆ ⊆ysatisfying ⊆ yˆ                                                        yˆ ⊆   tv ≥  attribute value node simplest                   case tv set constant like  denote ancy ancy    ancyˆ ≤ ancy   ancˆy    set ancestor nodes taxonomy including                     y∈y −yˆ  notational convenience leads intuitive                yˆ∈yˆ −y  decomposition scoring function contributions learning ranking functions translate  nodes paths root node speciﬁc                 terminal node                                                 yg            ancy   ancˆy                                                                            y∈yyˆ∈y−y    loss functions                                                      gygˆy  standard loss function multilabel case use look pair categories incorrect cate  symmetric difference predicted ac gory comes correct category order deﬁned  tual label set count number correct categories count symmetric difference respective ancestor  missed plus number incorrect categories sets corresponding loss  assigned  yyˆ  ≡y   yˆ     applications actual loss predicted la  hierarchical support vector machines  bel set relative true set category labels depend  multilabel classiﬁcation  relationship categories motivation generalize multiclass svm formulation crammer  consider generic setting routing items based singer  multilabel formulation similar  membership nodes taxonomy instance news elisseff weston  given set correct  routing setting readers signup speciﬁc topics se                                                        categories yi denote complement y¯i  y−yi  lecting appropriate node terminal following elisseff weston  approximate  node taxonomy category “soccer” separation margin respect ith example  ner node supercategory “sports” note  assume items assigned terminal nodes  taxonomy customers prefer signup  γiw ≡   min  Φxiy − Φxi y¯ w                                                                         ∈   ¯∈ ¯  categories en bloc selecting appropriate supercategory       yiy yi    assume relative signup volume sv ≥ formulation aims maximizing margin                             −   each node costs missing relevant item training set maxww mini γiwthisis                                                    ijcai                                                     equivalent minimizing norm weight vector algorithm  hierarchical multilabel svm  constraining functional margins greater                           ≥                                                          inputs training data iyi tolerance    equal  generalized softmargin svm formulation  initialize si  ∅ αiyy¯  ∀ ∈ yi y¯ ∈ y¯i  obtained introducing slack variables ξi’s  repeat  penalty scaled proportional loss associated       ˆ                                                                    select argmaxi ψi  violation respective category ordering mech          ˆ                 ¯                                                            select ˆyy¯  argmaxy∈yˆy¯∈yˆ gˆ ¯  anism suggested cf tsochantardis et al                              iyy                                                                               sˆ  sˆ ∪ˆyy¯ˆ  cai hofmann  putting ideas yields  expand working set                                                                                  αˆ  yy¯ ∈ sˆ  convex quadratic program qp                         solve qp subspace iyy¯                                                                   reduce working set sˆ  sˆ −yy¯αˆ                  n                                                                          iyy¯                                                        ψˆ ≤   min   w       ξi                                          wξ                                             ξi                            note minimize upper bound loss   st wδΦiyy¯≥ −         ∀i ∈ yi y¯ ∈ y¯i                          yy¯                        eq  simply assign slack variable ξiyy¯                                                        triplet instance positive label negative label      ξi ≥   ∀i                                                         leads dual program similar eq  sec       δΦ  yy¯ ≡ Φx − Φx  y¯                                            αiyy¯ ≤  ∀    ∈     ∈ ¯                                           ond set constraints yy¯ yy¯     formulation similar used schapire explore direction  singer  generalizes rankingbased multilabel  svm  formulation elisseff weston  follow  optimization algorithm  ing crammer singer  included bias derived qp large employ  terms categories efforts correctly efﬁcient optimization algorithm inspired smo  dering each pair positivenegative labels use algorithm platt  performs sequence sub  size prediction mechanism elisseff space ascents dual using smallest possible subsets  weston  convert category ranking actual variables coupled remaining variables  multilabel classiﬁcation                            through constraints algorithm successively optimizes    dual qp                           subspaces spanned αiyy¯  ∈ yi y¯ ∈ y¯i              n                                       selected instance additional variable selec  max  Θα          αiyy¯                          tion performed each subspace strategy   α                                                                                                            y∈yi                                 known column generation demiriz et al  deﬁne                    ¯                  y¯∈yi                                          n                                                                                 wα ≡             αiyy¯δΦiyy¯                       −              αiyy¯αjrr¯δΦiyy¯δΦj r¯                                                                         ¯                                                                  y∈yiy¯∈yi               ij y∈yi r∈yj                  y¯∈y¯ ¯                     r¯∈yj                                   ≡          −           α                                                          giyy¯    yy¯  δΦiyy¯                                                  αiyy¯                                                   st αiyy¯ ≥ ∀i ∈ yi y¯ ∈ y¯i    ≤ ∀i                                      yy¯                  li ≡ max     max   giyy¯                                             y∈yi                                            ¯                                 ¯                                     iy∈yiy¯∈yi                               y¯∈yi                             ⎧                                                                 ⎪            ¯                                                                 ⎨min  iy∈yi y¯∈yi giyy¯      ζi   note                                                                                                                                 ≡       αiyy¯                                                                            ui  ⎪         δΦiyy¯δΦjr r¯                                  ⎩                 ¯                                                                   min  min iy∈yi y¯∈yi giyy¯   ζi                                                                                            Λy −  Λ¯y Λr − Λ¯rφx φx                         αiyy¯                                                                                                                                                                            simply replace inner products corre                       αiyy¯                                                        ζi  −    ∈   ¯∈ ¯     deﬁneψi   ≡ li − ui  sponding kernel functions straightforward observe           yiy yi yy¯     ξ yields upper bound training loss derivation similar cai hofmann                                                                         ∀  resulting classiﬁer measured   following sense shown ψi  necessary sufﬁcient  deﬁne maximum loss                             condition feasible solution optimal                                                        score ψi used selecting subspaces giyy¯ used           xyg  ≡      max        yy¯         select new variables expand active set each subspace                      ∈  ¯∈ ¯   ≥ ¯                     yy                    resulting algorithm depicted algorithm   maximal loss set examples deﬁned    details convergence sparseness gen                                                                                                                    n                           eral class algorithms tsochantardis et al                                                                ≡         gx                                                                                  hierarchical perceptron                          ξˆ  proposition  denote  ˆ   feasible solution svm competitive generating highquality                    ˆ  qp thenn   ξi upper bound empiri classiﬁers computationally expensive percep                             cal maximal loss xf  ˆ  iyi               tron algorithm rosenblatt  known simplicity                                                    ijcai                                                     algorithm  hierarchical minover perceptron algorithm misclassiﬁcations penalized ancestors miss                                                  relevant patterns include irrelevant ones hloss    inputs training data iyi desired margin    initialize αiyy¯  ∀ ∈ yi y¯ ∈ y¯i        punishment occurs node descendents    repeat                                            penalized addition loss function works                                                        arbitrary taxonomy just trees     ˆi yˆ y¯ˆ  argmin ∈ ¯∈ ¯ wαδΦiyy¯                       iy yiy yi                        rousu et al  applies maximummargin markov     wαδΦˆˆyy¯ˆ                                                      networks taskar et al  hierarchical classiﬁcation       terminate satisfactory solution                                                        taxonomy regarded markov networks                                                            propose simpliﬁed version hloss decomposes         αˆ   ← αˆ     ˆyy¯ˆ         iyˆy¯ˆ iyˆy¯ˆ                                contributions edges marginalize exponential     end                                           sized problem polynomial methods learn   maximal number iterations performed  ing occurs taxonomy nodes instead edges view                                                        taxonomy dependency graph “isa” relation                                                          cai hofmann  proposes hierarchical svm  speed section propose hierarchical decomposes discriminant functions contributions  ceptron algorithm  using minimumoverlap minover different levels hierarchy way  learning rule krauth m´ezard              work compared cai hofmann     minover perceptron uses instance violates stricted multiclass classiﬁcation deal  desired margin worst update separating hyperplane additional challenge posed overlapping categories  deal instances sequentially truly multilabel problem employ category rank  online fashion using minimum overlap selection rule ing approach proposed schapire singer   effectively speeds convergence yields sparser summary major contributions  formulate  lutions                                              multilabel classiﬁcation global joint learning problem    using taxonomybased class attribute scheme eq  taxonomy information account  exploit  tv  simple update rule step  algorithm  taxonomy directly encoding structure scoring func  decomposed                                tion used rank categories  propose novel taxonomy                                                        based loss function overlapping categories    wv ←  wv   yy¯φxi  ∀v  ∈ ancy − anc¯y     ←   −             ∀    ∈       −             motivated real applications  derive sparse optimiza             yy¯φ   anc¯y ancy      tion algorithm efﬁciently solve joint svm formula    weight vectors nodes predeces tion compared multiclass classiﬁcation sparseness  sors y¯ updated nodes important constraints  left intact strategy used dekel et al  dual variables  present hierarchical percep  online multiclass classiﬁcation severe loss tron algorithm takes advantage proposed methods  incurred dramatic update step encoding known taxonomies   updates scoring functions classes  question spread impact classes sharing  experiments  affected ancestors                           experimental setup                                                        section compare hierarchical approaches    related work                                       ﬂat counterparts wipoalpha data set com  approaches hierarchical classiﬁcation use decision prising patent documents  tree like architecture associating each inner node taxonomyderived attributes employed hierar  taxonomy classiﬁer learns discriminate chical approaches√ comparison purpose tv eq   children dumais chen  cesabianchi et al  set  depth depth ≡ maxy ancysothat  offers advantages terms modularity local maxy Λy ﬂat hierarchical mod  optimization partial classiﬁers inner node els experiments hierarchical loss equals half value  able reﬂect global objective               eq  historical reason hierarchical learning em    cesabianchi et al  introduces loss function ploys hierarchical loss ﬂat employs −  called hloss speciﬁcally designed deal loss used linear kernel set  each instance  case partial overlapping paths treestructured tax normalized norm  experiments test  onomies cesabianchi et al  proposed bsvm performance evaluated crossvalidation macro  uses hloss uses decoding scheme averaging folds  explicitly computes bayesoptimal label assignment measures used include oneaccuracy average pre  based hloss certain conditional independence cision ranking loss maximal lossandparent oneaccuracy  sumptions label paths loss function proposed ﬁrst three standard metrics multilabel classiﬁca  eq  exploits taxonomy different way tion problem schapire singer  elisseff  loss partly convert partial path categories ston  oneaccuracy acc measures empirical  complete path ones loss function inspired real probability topranked label relevant doc  applications like routing subscription taxonomy ument average precision prec measures quality la                                                    ijcai                                                                section cat   doc  cat    acc     prec     xloss    rloss    pacc                                   doc  ﬂat   hier  ﬂat   hier ﬂat    hier ﬂat   hier ﬂat    hier                                                                                                                                                                                                                                                                                                                                                     table  svm experiments wipoalpha corpus each row categories speciﬁed level node section results  random fold crossvalidation better performance marked bold face “catdoc” refers average number categories  document “ﬂat” ﬂat svm “hier” hierarchical svm                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            flat prec                                                                                                                                                                                        hier prec                                                                                                  flat rloss                                                                           flat acc               hier rloss                                                                             hier acc                                                           performance    flat pacc performance                                                                                                                                                                 hier pacc                                                                                                                                                                         figure  columns left right depict accuracy sample number category sample number category  ﬂat hierarchical perceptron average precision ﬂat  hierarchical perceptron                          figure  flat hierarchical svm section data vary                                                       ing training set size small number documents sampled     section  acc   prec    xloss  pacc       each category training purpose learned classiﬁers              ﬂat hier ﬂat hier ﬂat hier  ﬂat hier      tested remaining documents repeated  times                      each sampling number bars depict sample standard deviation                                         ument labeled primary category                      number secondary categories types categories                      used form multilabel corpus performed                      dependent experiments taxonomies  toplevel                      sections                                                                             document parsing performed lemur toolkit   table  svm experiments wipoalpha corpus subsam stop words removed stemming performed word  pling three documents sampled each category counts title claim ﬁelds used document fea                                                        tures table  summarizes svm performance  bel rankings precision calculated each position sections hierarchical svm signiﬁcantly outperforms  positive label occurred labels ranked higher ﬂat svm terms  xloss ranking loss parent ac  including predicted relevant precision curacy each individual setting attributed  values averaged obtain average precision rank fact hierarchical approach explicitly op  ing loss rloss measures average fraction positive label timizes upper bound  xloss spe  negative label pairs misordered metrics ciﬁc hierarchical form discriminant function  described details schapire singer  hierarchical svm produces higher classiﬁcation    maximal loss denoted  introduced eq  accuracy average precision gains mod  measured hierarchical loss function eval erate improvement statistically signiﬁcant  uate parent oneaccuracy pacc measures conducted fold crossvalidation section  accuracy category’s parent nodes level        paired permutation test achieved level signiﬁcance                                                         accuracy     experiments wipoalpha collection             measures  wipoalpha collection comprises patent documents released figure  depicts performance perceptron algorithm  world intellectual property organization wipo  setting allow perceptron run  classiﬁed ipc categories ipc level hier convergence takes signiﬁcantly time svm  archy consisting sections classes subclasses groups reaches lower performance observe hierarchical  categories experiments refer main groups ceptron performs better cases  leaves depth hierarchy each doc addition randomly sampled  documents each      wwwwipointibisdatasets                            wwwlemurprojectorg                                                    ijcai                                                     
