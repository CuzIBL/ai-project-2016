                generalizing bias term support vector machines                            wenye li    kwongsak leung       kinhong lee                             department science engineering                                   chinese university hong kong                                  wyli ksleung khleecsecuhkeduhk                          abstract                          ﬁnd solution regularized minimization problem      based study generalized form rep                                                                         m      resenter theorem speciﬁc trick construct                                             ing kernels generic learning model proposed           min        yifxi  γ fk                                                                        f∈hk      applied support vector machines algo                         rithm obtained naturally generalizes      bias term svm unlike solution standard                                                        hk γ expression represents tradeoff      svm  consists linear expansion ker                                                        empirical error calculated loss func      nel functions bias term generalized algo                                                        tion  ”smoothness” solution represented      rithm maps predeﬁned features hilbert space                                                        norm reproducing kernel hilbert space rkhs      takes special consideration                                                                                               leaving space unregularized seek  induced kernel γ called      ing solution space empirical evaluations regularizer svm partially derived frame                                                                                                    ≡      conﬁrmed effectiveness general work choice hinge loss function yf                                                        max  −          ization classiﬁcation tasks                            yf      incompatibility comes                                                        bias term makes combined model  gener                                                        ally hk    introduction                                                          paper follow regularization point view  support vector machines svm shown based study generalized regularizer leaves  form machine learning applications based hypothesis space unregularized speciﬁc trick  vapnik’s seminal work statistical learning theoryvap constructing kernels propose new learning scheme  nik  algorithm starts ideas separating                                                      allows user predeﬁne features uses fea  hyperplanes margins given data xi yii                                                      tures kernel expansions derive solution hy  xi ∈r    yi ∈   − searches lin pothesis space instead considering kernel expansions  ear hyperplane separates positive negative sam existing kernelbased methods idea applied  ples largest margin distance hyper svm obtain algorithm naturally generalizes  plane nearest data point nonseparable case bias term svm generalized term linearly com  soft margin method used choose hyperplane bines predeﬁned features instead constant  splits examples cleanly possible using kernel                                                      different empirical results existence  kx    positive deﬁnite function extend bias term does make signiﬁcance practicerifkin  algorithm nonlinear case optimal hyperplane  show term trivial  simple representation linear expansion generalized appropriate choices term improve  kernel functions constant                    applicability algorithm                        m                    ∗                                     paper organized follows section                         cikxi                                                                       vestigate generalized regularizer regularized learning                                                                               framework study associated solution applied  each ci real number called bias term svm section  introducing kernel construc    case nonseparability meanings mar tion trick predeﬁned features mapped  gin motivation lost girosi  evgeniou et al  rkhs clarify mathematical details deriv  poggio smale  suggest different view svm ing svm new algorithm proposed section  based  based framework championed poggio   previous discussions empirical results demonstrated  researcherspoggio girosi implic section  finally section  presents discussions  itly treats learning approximation problem tries conclusions                                                    ijcai                                                       generalized regularized learning                   equation holds ∈hk  speciﬁcally letting                                                            ·  suppose hk direct sum subspaces hk  h⊕ kx  gives                    ···                 ≤                               m        span  ϕ   ϕ spanned                         ∗                                                                           pf          αiyikxi  linearly independent features consider generalized                  γ  regularized learning minimizing                                                             m                                    reproducing property kernels           min                                         yif     γ  pf                                                  f∈hk                                                                                                                                     ∗     ∗      ∗                                                                        − pf         αiyikxi                                                                                     pf orthogonal projection                              γ                 γ  called generalized regularizer ∗       ∗                            ∗  model applied svm consider hinge loss − pf  orthogonal projection hand                                                                                 function mentioned introducing slake variables ξi represented λpϕpsowehave  corresponding empirical error point xi problem                                                                           m                                                                     ∗                                                                                    λpϕp      cikxi                         min       ξi  γ pfpf                                                     f∈h                                                                                                      ∈r            satisfying                                            λp     ci  γ αiyi                                                          derived minimizer  satisﬁes reproduction                  yif xi ≥    − ξi                                                                                                 property suppose xi yi comes model                           ≥                                                                        ξi                               perfectly linearly related ϕ ···ϕ desirable  · ·k denotes inner product hk wederivethe solution independent features ev  dual quadratic program using technique lagrange ident result  property satisﬁed parameters  multipliers                                          ···cm  zero makes regularizer                 m                                      equal zero shown experiments                                                       property effect stabilizing results dif                 ξi  γ pfpfk                                                               ferent choices kernels regularization parameters prac                m                      m              tically              −    αi yif xi − ξi −   ζiξi                                                   mapping predeﬁned features rkhs                                           want minimize respect ξ maxi decomposing hypothesis space hk studying gen  mize wrt αi ζi subject constraints primal eralized regularizer proposed generalized regular  problem nonnegativity constraints αi ζitaking ized learning model derived associated solution  derivative wrt ξi setting zero  consists linear expansion kernel functions prede                 ∂l                                    ﬁned features section introduce kernel                        − αi − ζi                              ∂ξi                                  struction trick maps kernel functions predeﬁned                                                        features simultaneously hilbert space  substituting                                                      show mathematical details deriving svm gener                                                      alized bias term        γ pfpfk −     αiyif xi   αi                                                  ∗                                              kernel construction trick  suppose minimizer  ∈hk let      ∗           ∈r        ∈h                        given features ϕ ···ϕ strictly positive deﬁnite      δg δ       wehave                      Φ                      ∗           ∗                     function  let consider following reproducing kernel             γ pf  δpgpf   δpgk                  m                    m                                                                     ∗                                                                −    αiyi  δgxi    αi                                    ϕp    ϕp                                                                                      taking derivative wrt δwehave                                                    m      ∂l          ∗         γ  pf   δpgpg  −     αiyig xi                                                   ∂δ                                                                                                                              ∂l                                                            ∗                                                    Φx           ϕp ϕq yΦxp xq  minimizer ∂δ δ   problem                                                                                   m                                                                        ∗                                               −      xΦx   −      yΦx           γ pf pgk −     αiyig xi                          ϕp               ϕq                                                                                                                                          ijcai                                                                                                                                    ···                                                    ˜     ˜     ˜                               ϕ   ϕ  deﬁnes linear transformation prede λ  λ ··· λ  α    α     ···αm   ﬁned features ϕ ···ϕ wrt ···x                                                              ···                          ···                                    −             α         α   α    α         α   αm        ϕ      ϕ  ···  ϕ x      ϕ                   ···                  ···                                                                   diag  y         diag  y  ym       ···          ···         ···          ···                                        m                                                          ϕp xi           ϕp xi              ϕ       ϕ ···  ϕ x      ϕ                     pi               pi                                                    vector ones appropriate size taking derivative                                                             ˜  satisﬁes                                         wrt λ obtain                           ≤  ≤             ϕ  xp                                             eyα   eyα                                        ≤   ≤                                                            trick studied light wayne  pro                                                                                          vide alternative basis radial basis functions ﬁrst      γ˜c h˜c − α yh˜c  α          used fast rbf interpolation algorithmbeatson et al   sketch properties peripheral concerns taking derivative wrt ˜c setting zero  cludes                                                                       γh˜c − hyα                          kxp  ϕp                                                                                                                                                    strictly positive deﬁnite invertible                ϕpϕq                                                                                                                                           yα                                                                          ˜c                                             hxp                                                   γ                                                hxi ϕp                                                                       substituting                                                 hxi hxj                                                                                                              ≤ ≤ and   ≤ ≤ use           α hyα  −    α hyα     α                                                               γ                γ  ful property matrix xi xj                                                                                      ij                      strictly positive deﬁnite used following                                                                                       −       α hyα  − γ  α  computations                                                    γ       property  predeﬁned features                                                        problem  ϕ ···ϕ explicitly mapped hk                               subspace  span ϕ ···ϕspan ϕ ···ϕby                                                                                                                        ···                                              min  α hyα   − γ α           property  ϕ ϕ  forms orthonormal basis            α                                                           satisfying    computation                                                                                                                 ≤   ≤                     using kernel deﬁned   properties   α   mand      α      α            minimizer  rewritten                                                     ﬁrst box constraint comes  nonnegativ                                                                                              ∗                                         ity α requiring  ≤ αi ≤  ≤ ≤ second                     λpϕp      cikxi                                                                                                                                                                              equality constraint comes  hy strictly                    ···                                positive deﬁnite matrix problem standard                              m                      quadratic programqp comparing svm’s qpvapnik                        ˜                                                         λpϕp       c˜ihxi                new problem requires  equality constraints                           i                    stead equality constraint burden        ˜     ˜                                         computations actually major computations  λ ··· λ c˜ ··· c˜m parameters solving qp come box constraint equality  termined furthermore orthogonal property                                                        constraints affect computations                                                                                                  ˜  ϕp wehave                              solution ˜c obtained α  λ comes                          m                            linear program                      ∗                            ˜i                                                                                      m                         i                                                                                                                              min       ξi                                                                                            λ˜  property                                                                         ∗    ∗   ˜ct h˜c                 pf  pf                            satisfying       ˜c ˜    ··· ˜                                     ⎛                      ⎞      c    cm  substituting  ob              m  tain matrix representation                             ⎝     ˜          ˜    ⎠   ≥    −                                                            yi     λpϕp         cjhxj             ξi                   ˜        ˜                             j    γ˜c h˜c − α ye λ − α λ  h˜c   α                                                                                     ξi  ≥                                                       ijcai                                                       generalized algorithm    algorithm  based discussions algorithm general  izes bias term svm proposed follows                            start data xi yii     ≤ predeﬁned linearly independent features                                               ϕ ···ϕ data deﬁne ϕ ···ϕ according      equation     choose symmetric strictly positive deﬁnite function                                                    Φx Φx   continuous ×r       hx according equation                rd →r                                   figure  image classiﬁcation accuracies using coil dataset    deﬁne                                        different number training images predeﬁned                              m                      features gaussian kernel Φ used svm                     ˜                                 gb Φ  used svm kernel regularization             λpϕp       c˜ihxi                             i                    parameters selected cross validation oneversusone                                                        strategy used multiclassiﬁcation       solution c˜ ··· c˜m comes      quadratic program  equation       ˜      ˜                                             experiments      λ ··· λ obtained solving linear program                                                         evaluate effectiveness brought generalized bias    virtual samples                                  term groups empirical results reported ﬁrst  new algorithm needs construction orthonormal group experiments focused reproduction property                         ···                             ···     generalized bias term used columbia university im  basis ϕ   ϕ  predeﬁned features ϕ   ϕ                                wrt samples linear transformation al age library coil dataset image classiﬁcation                                                                          ···  gorithms search linear hyperplanes separate dataset  images    evenly distributed                                                          data invariant linear transformations pro classes preprocessing each image represented                                                             ×     vide invariance consider alternative approach       dimensional vector each compo                                                                                        avoids transformation                            nent having value    indicating gray                              ···                   level each pixel experiments performed each    introduce  virtual points   v satisfying                                                       experiment used ﬁrst     images respectively                            ≤  ≤                each class training rest testing            ϕp xvq                                                   ≤   ≤                 using recently developed nonlinear dimensionality reduc                                                        tion technqiuestenenbaum et al  roweis saul                                               label yvq each virtual point vq deﬁned  images actually form manifold  study following problem                          lower dimension original   dimen                                                        sions prior knowledge features ϕ ···ϕ                      min  lv                                      f∈hk                              predeﬁned follows                                                                                                                                              nn  xi ∩ trainp                                                                 ϕp xi                                                                                                                     lv            − yvq xvq                             ≤  ≤   ≤  ≤                                                                             nn     deﬁnes three                                                    nearest neighbors geodesic distance xandtrainp                   m                                                      set training images class label geodesic dis                      − yif xi  γ pf          tance computed graph shortest path algorithm                                                                                             tenenbaum et al  deﬁning features way                      m                                implicitly used knn classiﬁer generalized bias term                                                                      − yif xi  γ pfk    similar ideas generalized combining different ap                                                proaches knn                                                          figure  compares classiﬁcation accuracies svm    easily seen ϕ ···ϕ formed                                                        knn new algorithm denoted svmgb  thonormal basis wrt virtual points                                                      sightful study results ﬁnd  predeﬁned  need compute ϕ  ···ϕ new regularized                                                      features dominated learned svmgb solution  minimization problem  equivalent original                                                        reproduction property performance   hinge loss introduction virtual points                                                        algorithm similar knn shows better performance  does change scope classiﬁcation problem                                                        standard svm dataset  having avoided linear transformation construct  thonormal basis                                         httpwwwcscolumbiaeducavesoftware                                                    ijcai                                                                                                           gb better results svm bow representation                                                        experiments gives belief embedding                                                        plsa features improves accuracy                                                             discussion conclusion                                                        idea regularized learning traced modern                                                        regularization theorytikhonov arsenin  morozov                                                         states existence regularizer helps                                                        provide unique stable solution illposed problems                                                        paper considered usage generalized regular                                                        izer kernelbased learning straightforward reason                                                        hypothesis space left unregularized new  figure  image classiﬁcation accuracies ﬁxing regularizer wellaccepted fact ﬁnite dimen  groups kernel regularization parameterslogscale sional problems wellposedbertero et al                                                         vito et al  need regularization                                                        case learning problems deﬁned subspace    reproduction property helps lessen sensitiv spanned number predeﬁned features similar idea  ity algorithm choices kernels regular explored spline smoothingwahba  leaves  ization parameter γ illustrate effect experi polynomial features unregularized kernelbased learn                        ×  ment conducted using    training images instead ing similar work semiparametric svm  using cross validation parameter selection groups modelsmola et al  combines set unreg  kernel regularization parameters ﬁxed figure  ularized basis functions svm knowl  picts results generalized bias term makes svmgb edge algorithms applications investigated  stable variations kernel regularization pa machine learning tasks uniﬁed rkhs regulariza  rameters property especially useful concerning tion viewpoint general way  practical situations arbitrariness exists designing new regularizer speciﬁc trick construct  kernels applications                        ing kernels predeﬁned features explicitly mapped    study performance new algorithm hilbert space taken special consideration during  second group experiments conducted text catego                                                       learning differentiates work standard  rization tasks using newsgroups dataset  dataset col kernelbased approaches consider kernel expansions  lects usenet postings newsgroups each group constant learning projecting predeﬁned fea                  messages experimented tures rkhs idea conditionally positive def  major subsets ﬁrst subset ﬁve groups comp inite functionmicchelli  lurking background  second groups rec groups sci goes discussion paper  groups talk                          applying idea svm developed new                                          dataset removed  words algorithm regarded having generalized  highest mutual information class variable rain bias term svm domain speciﬁc knowledge pre  bow packagemccallum  each document repre deﬁning features generalization makes bias term  sented bagofwords bow treats individual words trivial conﬁrmed experimental results                                       features linear kernel coupled predeﬁned fea correct choices features help improve accuracy          ···  tures ϕ ϕ obtained probabilistic la make algorithm stable terms sensitivity  tent semantic analysis plsa hofmann  used selection kernels regularization parameters  input svmgb experiments carried dif               ˜   ferent number        training documents each acknowledgments  subset experiment consisted runs average  accuracy reported each run data separated research partially supported rgc earmarked  xvalprep utility accompanied package grant hong kong sar    comparison svm tested linear kernel rgc research grant direct allocation chinese uni  commonly used method text categorizations versity hong kong  completeness bow plsa representations  documents experimented seen ﬁgure  references  svm plsa representation performs                                                        beatson et al  rk beatson wa light  number training samples small approach loses                                                           billings fast solution radial basis function  advantage training set increases moderate                                                           interpolation equations domain decomposition methods  size svmgb reports best performance                                                           siam sci comput –    training documents shown svm                                                        bertero et al  bertero mol er pike    httpwwwcscmuedu˜textlearningdatasetshtml     linear inverse problems discrete data ii stability    httpwwwrulequestcompersonalcrtargz       regularisation inverse probl –                                                     ijcai                                                     
