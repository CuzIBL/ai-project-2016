                 general model online probabilistic plan recognition                                                          hung bui                                                 department computing                                             curtin university technology                                       po box perth wa  australia                                        url httpwwwcscurtineduaurbuihh                                              email buihhcscurtineduau                             abstract                               network making exact inference intractable                                                                  dbn sparse structure online plan recognition        present new general framework online                                                                  algorithms based exact inference run problems        probabilistic plan recognition called abstract                                                                  belief state large unable        hidden markov memory model ahmem                                                                  scale larger detailed plan hierarchies        new model extension existing abstract                                                                    previous work proposed framework        hidden markov model allow policy                                                                  online probabilistic plan recognition based abstract        internal memory updated markov                                                                  hidden markov models ahmm bui et         fashion show ahmem repre­                                                                 ahmm stochastic model representing execution        sent richer class probabilistic plans                                                                  hierarchy contingent plans termed policies scal­       time derive efficient algorithm plan                                                                  ability policy recognition ahmm achieved        recognition ahmem based rao                                                                 using approximate inference scheme known rao       blackwellised particle filter approximate inference                                                                  blackwellised particle filter rbpf doucet et al         method                                                                  shown algorithm scales wrt                                                                  number levels plan hierarchy    introduction                                                   despite computational attractiveness current   ability perform plan recognition useful  ahmm limited expressiveness particular in­  wide range applications monitoring surveil­   ability represent uninterrupted sequence plans   lance decision supports team work plan      actions fact each policy ahmm   recognizing agents task usually complicated uncer­  purely reactive current state memory   tainty plan refinement process outcomes ac­  type memoryless policies represent unin­  tions agents observations plan dealing    terrupted sequence subplans way   issues plan recognition challenging task es­ remembering subplan sequence currently   pecially recognition online    executed words decision choose   observer react actors plan realtime       subplan dependent current state      uncertainty problem addressed sem­      subplans chosen past   inal work charniak goldman  phrases           models plan recognition probabilistic state   plan recognition problem inference problem       dependent grammar psdg pynadath wellman    bayesian network representing process executing       pynadath  expressive   actors plan recent work considered dy­          limitation unfortunately existing exact inference method   namic models performing plan recognition online py       psdg pynadath  flawed   nadath wellman   goldmand et          inadequate bui    huber et ai  albrecht et  offers   main motivation paper extend existing   coherent way modelling dealing various sources     ahmm framework allow policies memories   uncertainty plan execution model computational  considered propose extension ahmm called   complexity scalability inference main issue es­ abstract hidden markov memory model ahmem   pecially dynamic models                                   expressiveness new model encompasses      inference dynamic models dynamic             psdg pynadath wellman  new model   bayesian networks dbn nicholson brady          removes current restriction ahmm impor­  difficult static model inference static   tantly show rbpf approximate inference method   network utilizes sparse structure graphical model   used ahmm extended general   make tractable dynamic case dbn belief      ahmem ensuring new generalized model   state need maintain usually does preserve   remains computationally attractive best knowl­  conditional independence properties single timeslice   edge provide scalable inference method       user modeling                                                                                                          general type hierarchical probabilistic plan hierar• tion rules form yx allowed    chy                                                          rule models adoption lower level policy      paper structured follows section  provides      higher level policy second rule models    detailed discussion ahmm psdg re•          termination policy psdg model considered    lated models online probabilistic plan recognition   pynadath wellman allows general rules    ahmem introduced section  algorithms       form recursion symbol    plan recognition presented section  experimental re• located end expansion psdg    sults prototype provided section  fi•  policy expanded sequence policies    nally conclude discuss directions future work  lower level executed   section                                                      control returned higher level policy                                                                    expressive ahmm existing    related models online probabilistic                      computational method inference psdg remains        plan recognition                                          inadequate pynadath proposed exact method updat•                                                                 ing belief state psdg compact closed form   ahmm bui et al   agents probabilis•    proposed algorithm seemingly gets exponen•  tic plan modeled abstract markov policy amp     tial blow size belief state unfortunately   amp extension policy markov decision pro•        derivation algorithm based flawed assumption   cesses mdp defined subset environment state  higher levels belief state independent   space select refined amps     lower levels given current level details   form hierarchy policies amp simi•      flaw inference algorithm psdg interested read•  lar contingent plan prescribes subplan ers referred bui    invoked each applicable state world noisy      ahmm psdg proposed ahmem re•  observation environment state modelled     lated hierarchical abstract machines ham parr   making state hidden similar hidden state   used abstract probabilistic planning model   hidden markov models rabiner  stochastic pro•      policy represented stochastic finite automaton   cess resulting execution amp termed ab•  automata lower level despite   stract hidden markov model intuitively ahmm models       representational similarity computational techniques   amp causes adoption policies ac•       ahmem related models intended plan recogni•  tions different levels abstraction turn generate tion ham model used speeding pro•  sequence states observations plan recogni•    cess finding optimal policy mdp   tion task observer given ahmm corresponding         ignore state dependency dbn structure   actors plan hierarchy asked infer cur• ahmem psdg similar structure hierar•  rent policy executed actor levels   chical hidden markov model hhmm fine et al    hierarchy taking account sequence observations    murphy pashkin  hhmm   currently available problem termed policy recogni•    type probabilistic context free grammar pcfg   tion bui et al                                         ahmem psdg state dependency      scalability policy recognition ahmm            model   achieved using hybrid inference method variant   raoblackwellised particle filter rbpf doucet         abstract hidden markov memory   et al  applied dbn inference rao  blackwellisation casella robert  splits net•        models   work sets variables set variables need                                                                  section introduces abstract hidden markov mem•  sampled termed raoblackwellising rb vari•                                                                 ory models ahmem extension ahmm   ables set remaining variables belief state                                                                  policy internal memory main aim   conditioned rb variables need maintained                                                                  construct general model plan recognition expres•  exact inference termed raoblackwellised rb belief                                                                  siveness encompasses current ahmm psdg   state rbpf allows combine samplingbased                                                                  models retaining computational attractiveness   approximate inference exact inference achieve effi•                                                                 ahmm framework define ahmem sub  ciency improve accuracy                                                                  section  dbn structure new model given     probabilistic statedependent grammar psdg py        subsection    nadath  pynadath wellman  described   probabilistic context free grammar pcfg jelinek                                                                   model   et al  augmented state space state transi•  tion probability table each terminal symbol pcfg   consider mdplike model representing states   addition probability each production rule   environment representing set primitive   state dependent result terminal symbol acts     actions available agent each action € defined   like primitive actions nonterminal symbol chooses     transition probability current state   expansion depending current state ahmm         state oass set abstract policies include   equivalent special class psdg produc•    primitive actions furthermore ahmm bui et al                                                                                                          user modeling  defines higher level abstract policies set distributions    policies follows                                           land                                                                     ampe executed state                                                                  initialises memory value according distribution                                                                           policy lower level selected                                                                  according distribution policy                                                                  executed terminates state new                                                                  state policy terminate probability                                                                            does terminate memory variable                                                                  given new value according transition probabil­                                                                 ity new policy lower level                                                                  selected probability                                                                    using ampes construct hierarchy abstract                                                                  policies way ahmm start set      amp defined purely reactive sense    primitive actions    build set   selects policy lower level based new ampes toplevel policy   current state restricts set behaviours     executed invokes sequence levelkl policies   amp represent example able      each invokes sequence leveik policies   represent plan consisting subplans followed   level policy invoke sequence primitive   regardless state sequence represent actions leads sequence states in­  kind plans agent needs form inter­     troduce hidden states model noisy observation   nal memory remember current stage execution let     state observation model   set possible internal memory states ex­    dynamic process executing toplevel ampe termed   tend definition policy include memory variable abstract hidden markov memory model ahmem   takes values updated each stage    special cases ahmem worth mention­  execution policy                                      ing ahmm special ahmem   definition  abstract markov policy memory               memoryless policies ahmm equivalent am­  ampe let ii set ampes ampe  ii       pes dependency memory variable ignored   defined tuple singleton set class psdg con­                                                                 sidered pynadath  easily converted                        set applicable states                                                                  ahmem terminal symbols psdg equiv­                        set destination states        alent primitive actions each nonterminal symbol                                terminating proba    equivalent memoryless policy addition each sequence                          probability policy                        encountered rhs production        stop current state current mem­                                           equivalent        ory value                                          policy memory taking values                                                                   simply selects memoryless policy                                     policy selection prob­                                                                   note ahmem definition assume balanced        ability a m probability selects                                                                  policy hierarchy ease presentation actions        policy  state memory value                                                                   appear level                              initial distribution specify unbalanced hierarchy introducing dummy        memory values probability ini­           policies arc equivalent primitive actions higher        tial memory  commences state              levels hierarchy                                         memory transi­       tion probability probability              dbn representation ahmem        memory value given current memory        value current state      subsequently drop subscript   confusion policy context note   states  called terminal states                             policy selection memory   initial transition probability proper probability        argue wc incorporate memory vari­  able environment state gain extra represen­  tational power just introducing memory variables   incorporating memory variables state variable blow   size state space defeat purpose keeping   model computationally feasible       user modeling                                                                                                                                                                       start through starting state starting                                                                  time conditional independence theorem poli­                                                                 cies ahmm holds general setting                                                                  state theorem ahmem proof                                                                  ahmm bui et ai  directly extended                                                                  general case using context specific independence proper­                                                                 ties described previous subsection                                                                         let note knowingis equivalent                                                                  knowing precisely each policies starts ends                                                                  given starting time state                                                                  current policy known following corollary                                                                  direct consequence theorem                                                                     corollary  let ct represent conditional joint distribu•                                                                 tion ct following    policy termination selection                              bayesian network factorization    policy termination selection model ahmem    essentially ahmm depen­   dency value current memory variable    note context specific independence boutilier et     properties ahmm hold example   ct following undirected network repre­           independent             sentation form set cliques     remaining parents variable consider variable                                        note set                independent                 cliques cok form chain cliques order    remaining parents                                 term ct policyclique chain extends concept                           independent                  policy chain memoryless case ahmm bui                                                                  et al  ct factored product poten   memory update    consider variable parents node    act like context variables     policy lower level terminated memory   network factorization potentials said canoni•   updated time be­                    cal form potential representation clique chain    comes independent remaining parents         canonicalized perform message passing exact in­                                                                 ference compute marginal each clique canoni­                                                                 cal form computed directly marginals                                                                  later use undirected representation ct                                                                  exact inference canonical form directed representa­                                                                 tion ct obtaining samples joint distribution                                                                  using simple forward sampling                                                                   approximate inference ahmem                                                                  section look online inference prob­                                                                 lem ahmem assume time    independence properties ahmem                       sequence observations environment state    ampes expressive memoryless                  need compute be­   policies remain autonomous sense     lief state dbn joint distribution    higher layers influence state ampe     current variables given observation sequence   during execution way higher layers influ­                                 answer   ence current state ampe through conditions    various queries current status plan execution                                                                                                          user modeling example marginal probability tells   current policy actor executing level   probability prtells current stage ex­  ecution policy probability pr tells      end current time     compact closed form representation   belief state exact inference structure   ahmem intractable large how­  theorem  suggests apply rao  blackwellised particle filter rbpf problem sim­  ilar way ahmm bui et al  us­  ing rt raoblackwellising rb variables rao  blackwellised rb belief state similar origi­  nal belief state ahmem   known  pr note   rb belief state obtained directly   policyclique chain ct adding network represent­  ing conditional distribution ot  —   pr proistctseefig     main steps rbpf procedure  updating            figure  time slices rb belief state   rb belief state using exact inference  sampling   rb variable rt current rb belief state                                                                 rbpf procedure ahmm bui et al     updating rb belief state                              each time step algorithm maintains set sam­  fig  shows modified timeslice dbn rb        ples each consists value sti para­  variables known note                      metric representation ct differences details   nodes level  remain unchanged      obtain new samples update rb belief state   links time slices level  re­      obtain each new sample stlt need canoni  moved greatly simplifies network structure allow­  calize potentials ct assuming ct   ing updating operations performed efficiently      canonical form canonicalize potentials                                                                 ct level  level   complexity     bt obtained directly ct                                                                 complexity sampling step onl   compute ct procedure updating   usual stages  absorbing new evidence complexity updating step onl overall                                                                 complexity algorithm each time step onl fur­  ct need compute — pr     projecting new time slice thermore distribution usually decays exponentially                                                                 average complexity   need compute ct     principle apply simplified version junction  time estimation pr   tree algorithm uensen  undirected network rep­   required need compute  each sam­  resentation ct perform update step fact ple involves performing message passing entire   generalization arcreversal procedure operates chain ct complexity time step nk   directed network representation policy chain types queries possible long proba­  ahmm bui et al  algorithm updating        bility required computed rb belief state                                                                 example ask question actor currently   rb belief state given fig  absorbing step involves                                                                 executing current stage execution   simply incorporating evidence likelihood poten­                                                                policy answer query need compute con­  tials ct obtain potentials ct projecting                                                                 ditional probability easily   step involves adding timeslice   ct fig                                                                  achieved replacing ft function algorithm   marginalizing redundant variables   old time slice marginalization perform­  ing message passing cliques timeslice   network shown fig                                        experimental results       potentials ct level    level stay implemented algorithm surveillance   unchanged complexity algorithm   domain demonstrate working practical application   highest level termination furthermore environment consists spatial area sep­  potentials canonical form remains canonical form arate rooms corridor monitored set  cameras   updating procedure                                 fig  monitored area divided grid cells                                                                 cell coordinates constitute overall state space     rbpf ahmem                                            coordinates returned cameras modelled   rbpf algorithm policy recognition         noisy observations true coordinates tracked per­  ahmem provided fig  general structure      son time provide sequence observations       user modeling                                                                                                       
