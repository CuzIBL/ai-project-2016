              sequential genetic search ensemble feature selection              alexey tsymbal              mykola pechenizkiy      pádraig cunningham         dept science          dept cs  iss         dept science           trinity college dublin      university jyväskylä     trinity college dublin               dublin  ireland         po box  finland         dublin  ireland                tsymbalocstcdie        mpechencsjyufi    padraigcunninghamcstcdie                       abstract                      sets used generate base classifiers possible                                                   promote diversity produce base classifiers tend      ensemble learning constitutes main di                                                  err different subareas instance space      rections machine learning data mining en feature selection algorithms including ensemble feature      sembles allow achieve higher accuracy selection typically composed following compo     achievable single models                                                   nents aha bankert  opitz   search      technique proved effective strategy searches through space feature subsets      structing ensemble diverse classifiers  fitness function inputs feature subset     use feature subsets different approaches                                                   puts numeric evaluation search strategy’s goal      ensemble feature selection genetic search feature subset maximizing function      shown perform best domains reasonable include fitness function explic     paper new strategy gassefs genetic algo                                                  itly implicitly accuracy diversity measure      rithmbased sequential search ensemble fea fitness proposed opitz  defines      ture selection introduced instead genetic fitness fitness  classifier corresponding feature subset      process employs series processes goal                                                      proportional classification accuracy acci diver     each build base classifier ex sity div  classifier      periments  data sets conducted comparing     new strategy previously considered ge       fitnessi  acci  α ⋅ divi        netic strategy different ensemble sizes                                                   α  reflects influence diversity diversity divi      different ensemble integration methods contribution classifier total ensemble diver     experiments show gassefs sity measured average pairwise diver     timeconsuming builds better ensem sity pairs classifiers including fitness      bles especially data sets larger numbers function used experiments tsymbal et al      features                                      used experiments paper                                                     tsymbal et al  genetic searchbased strategy    introduction                                  ga introduced uses genetic search evolving   popular method creating accurate model set initial population built random subspacing ga   training data construct set ensemble classifiers shown perform best average respect   shown ensemble accurate three strategies diversity measures best   single classifiers integration classifi ga considered measures kappa statistic   ers currently active research area machine learn failnonfail disagreement   ing neural networks communities dietterich  paper introduce new genetic searchbased    theoretical empirical research demon strategy ensemble feature selection gassefs   strated good ensemble include diverse base instead maintaining set feature subsets each gen  classifiers important issue creating effective eration like ga consists applying series genetic   ensemble choice function combining processes each base classifier sequentially   predictions base classifiers shown increas paper organized follows section  task   ing ensemble diversity ensure increased ensemble feature selection considered section    accuracy – integration method does properly util present strategies genetic ensemble feature selec  ize ensemble diversity benefit arises tion ga gassefs diversity measures   integration brodley lane            section  different methods ensemble integration    effective approach generating ensemble di reviewed section  present experiments   verse classifiers use feature subsets ensemble genetic strategies conclude section   feature selection opitz  varying feature sub summary assessment research topics     ensemble feature selection random        cluding fitness function penalty terms accounting                                                    number features use individual accuracy       subspacing                                   diversity  alternative solution problem    task using ensemble models broken motivation alternative fact   basic questions  set models fitting level base classifiers desirable    generated  predictions overfitting ensemble shown recently    models integrated dietterich    studies ensemble overfitted members     effective approach ensemble generation use better ensemble nonoverfitted   different subsets features each model finding set members example street kim  pruning   feature subsets constructing ensemble trees resulted decreased ensemble accuracy   known ensemble feature selection opitz  accuracy trees increased   traditional feature selection algorithms goal genetic algorithm ensemble feature selection   finding best feature subset suitable ga tsymbal et al  based gefs strategy   learning problem learning algorithm task opitz  gefs genetic algorithm   ensemble feature selection additional goal finding ensemble feature selection explicitly used diversity   set feature subsets promote diversity fitness function ga begins creating initial popu  base classifiers opitz               lation rs new candidate classifiers produced     ho  shown simple random selection fea crossover mutation producing certain num  tures effective technique ensemble feature ber individuals process continues selecting new   selection lack accuracy ensemble subset candidates randomly probability propor  members compensated diversity tech tional fitness called roulettewheel selection   nique called random subspace method simply ran process creating new classifiers selecting subset   dom subspacing rs                             generation continues certain number times     rs solve small sample size problem predefined number generations fittest indi  training sample size relatively increases ran viduals make population comprises en  dom subspaces ho  shows semble representation each individual constant  classification methods suffer curse dimensional length string bits each bit corresponds particu  ity method does rs common lar feature crossover operator uses uniform crossover   bagging skurichina duin  instead sam each feature children takes randomly   pling instances samples features like bagging rs value parents mutation operator ran  parallel learning algorithm generation each base domly toggles number bits individual   classifier independent makes suitable parallel instead maintaining set feature subsets each   implementation desirable practical applica generation genetic process gassefs genetic al  tions shown like bagging accuracy gorithmbased sequential search ensemble feature se  increased addition new members lection uses series genetic processes each base   ensemble complexity grew ho     classifier sequentially pseudocode gassefs given     rs used base number ensemble feature se figure  each genetic process base classifier   lection strategies gefs genetic ensemble feature selected ensemble gassefs uses fitness   selection opitz  hc hill climbing cunning function  diversity calculated base classi  ham carney                            fiers formed previous genetic processes instead                                                    members current population ga proc     strategies genetic ensemble fea     ess fitness function use accuracy gas                                                   sefs uses genetic operators ga       ture selection                                                    ensemblesize                                                       populationjrsmfeatures     ga gassefs                               generations                                                         calculatefitnesspopulationk    use genetic search important direction         feature selection research genetic algorithms     randomly proportional logfitness    shown effective global optimization techniques         lmselectpopulation                                                            offspringskcrossoverlm    use genetic algorithms ensemble feature selection     endfork    proposed kuncheva  elaborated        mutateoffspringsk                                                                mutateoffspringsk    kuncheva jain  fitness function       calculatefitnessoffspringsk    kuncheva  kuncheva jain  ensemble randomly proportional fitness    accuracy used instead accuracy base classi populationselectpopulationoffsprings                                                      endforj    fiers fitness function biased   according fitness    particular integration method simple voting baseclassifieriselectpopulation      shown kuncheva  endfori    design prone overfitting additional preven figure  pseudocode gassefs algorithm    tive measures needed taken avoid ga number peculiarities use                                                                        gassefs feature sets allowed rs   ∑  ii                                                                                            crossover operator produce feature subset indi Θ   Θ  ⎛ ⋅ ⎞                                                                              ∑⎜   ⎟   viduals crossover selected randomly proportional             i⎝     ⎠   logfitness instead just fitness adds di number classes total number   versity new population generation children instances Θ  estimates probability classifi  identical parents prohibited provide better                                                    ers agree Θ correction term estimates   diversity length feature subsets different muta probability classifiers agree simply chance   tion operators used mutate mutate case each classifier chooses assign class    deletes features randomly given prob                                                   label randomly pairwise diversity divkappaij   ability – adds features         fined follows     parameter settings implementation ga                                                                           Θ − Θ   gassefs include mutation rate  population      div  kappaij        size  search length  feature subsets number          − Θ   new individuals produced crossover mutation normalize measure vary       offsprings current population  clas   sifiers generated crossover  mutated  integration ensemble models    springs  each mutation operator  generations                                                    challenging problem integration decide    individuals produced pilot studies shown                                                    classifiers select combine results pro   cases configuration ensemble                                                    duced base classifiers number selection    accuracy does improve  generations                                                   combination approaches proposed    fitting training data                                                      popular simplest techniques used     complexity ga does depend number                                                    combine results base classifiers simple voting    features os′⋅   s′  number                       gen                          called majority voting bauer kohavi    individuals generation  number                                 gen                voting output each base classifier considered    generations tsymbal et al  complexity gas                                                   vote particular class value class value    sefs os ⋅ s′⋅   number base                   gen                              receives biggest number votes selected final    classifiers experiments average ga gas                                                   classification weighted voting wv each vote    sefs look through   feature subsets cor                                                   weight proportional estimated generalization   respondingly given number base classifiers                                                    formance corresponding classifier works usually bet    number individuals generation                                                     ter simple voting bauer kohavi     number generations                                                     number selection techniques proposed                                                    solve integration problem popular     diversity measures used fitness function simplest selection techniques crossvalidation ma   failnonfail disagreement measure kappa statis jority cvm called single best simply    tic shown provide best performance ga static selection ss experiments schaffer     tsymbal et al                          cvm crossvalidation accuracy each base classi    failnonfail disagreement measure defined fier estimated classifier highest   skalak  percentage test instances accuracy selected   classifiers make different predictions described approaches static select   correct                              model data space combine models                                                uniformly dynamic integration each new instance                                               classified taken account usually better results           div  disij                                                 achieved integration dynamic          ab                                          consider experiments three dynamic techniques    number instances classified correctly based local error estimates dynamic selection    incorrectly classifier correctly ds dynamic voting dv dynamic voting   incorrectly classifier denominator  selection dvs tsymbal puuronen    equal total number instances failnonfail dis tain main phases learning phase local    agreement varies                     classification errors each base classifier each instance     kappa statistic introduced cohen  training set estimated according  loss   let nij number instances recognized class function using cross validation learning phase finishes    classifier class second ni training base classifiers training set   number instances recognized classifier application phase begins determining knearest   ni number instances recognized neighbours new instance using given distance metric   second classifier define Θ Θ     weighted nearest neighbour regression used pre                                                   dict local classification errors each base classifier                                                    new instance   ds simply selects classifier pre stances divided sets approximately equal   dicted local classification error dv each base classifier size validation set test set  test runs   receives weight proportional estimated local each data set each search strategy diversity    accuracy final classification produced wv different ensemble sizes tested      dvs base classifiers highest local classifica  ensemble size did exceed    tion errors discarded classifiers errors fall main reasons  limitation computational resources   upper half error interval locally  shown experiments guided ensemble   weighted voting dv applied remaining classifiers construction genetic search biggest gain                                                    achieved  base classifiers     experimental investigations                   classifiers needed unguided ensemble                                                   struction rs bagging      experimental setup                            each run algorithm accuracies types    experiments conducted  data sets taken ensemble integration collected static selection ss    uci machine learning repository blake et al  weighted voting wv dynamic selection ds dynamic   data sets include realworld synthetic problems voting dv dynamic voting selection dvs   vary characteristics previously investigated collected ensemble characteristics num  researchers main characteristics data sets bers generations       presented table                           reduce number possible combinations pa                                                   rameters conducted separate series preliminary ex           table  data sets characteristics periments using wrapper approach based cross vali                                                   dation select best diversity coefficient α                                     features         data set instances classes                 number nearest neighbors dynamic integration                                 categ num        tsymbal et al  experimented seven         balance                              values α         seven values         breast cancer                                                  car                                       −    used        diabetes                              experimental results best value      glass recognition                       depended integration method used       heart disease                        ionosphere                           data set best α ’s varied search strat       iris plants                           egy integration method data set used           led                                  experiments repeated selected         led                              values α  data used       liver disorders                      lymphography                          selection later experiments believe        monk                                did lead overfitting small number        monk                                possible values α         monk                               soybean                                 test environment implemented         thyroid                              mlc framework machine learning library       tictactoe                            kohavi et al  multiplicative factor  used         vehicle            voting                              laplace correction sb domingos paz         zoo                                zani  numeric features discretized                                                    equallength intervals observed value whichever    tsymbal et al   use simple bayes domingos pazzani     sb base classifier ensembles approach slightly accu   cently shown experimentally theoretically sb rate sophisticated ones advantage    optimal “naïve” featureindependence simplicity sufficient comparing different ensem   assumption violated wide margin domingos bles sb classifiers each     pazzani  second sb applied sub   problems lower dimensionalities error bias  experimental results    bayesian probability estimates caused feature validate findings divided data sets    independence assumption smaller eas groups  features  data sets group     ily handle missing feature values advantages greater equal  features  data sets group    terms simplicity learning speed classification speed  checked characteristics groups    storage space believe dependencies ensemble accuracies nearly diver   clusions presented paper depend learn sity measures failnonfail disagreement    ing algorithm used similar known slightly better average present results di   learning algorithms                            versity measure figure  ensemble accura    evaluate ga gassefs used stratified cies strategies ga gassefs groups    randomsampling cross validation  percent data sets ensemble sizes shown averaged    stances training set remaining  percent data sets best integration method dvs  seen figure gassefs builds  interesting findings    accurate ensembles ga especially group     including data sets larger numbers features accu selected values α  different different data sets    racy grows ensemble size growth flattens supporting findings tsymbal et al   gen   number base classifiers increases    eral strategies α  dynamic integration                                                    methods bigger static ones  vs  av                                                   erage gassefs needs slightly higher values α                                                ga  vs  average explained                                                    fact gassefs starts classifier                                               based accuracy subsequent classifiers need                                                    diversity accuracy                                                 number selected features falls ensemble size                                                   grows especially clear gassefs                                                                                                       base classifiers need diversity rule features                                                  needed static integration methods dy                                              namic ones achieve better accuracy gassefs results                                                    slightly smaller feature subsets average  vs                                                features dynamic integration strategies                                                      reported tsymbal et al  se                                              lected kneighbourhood values dynamic integration           gagr  gassefsgr gagr gassefsgr change integration method ds needs higher values    figure  ensemble accuracies strategies groups explained fact prediction   data sets ensemble sizes               based classifier selected                                                    unstable higher values provide stability    figure  ensemble accuracies shown ds average selected equal  ds    strategies integration methods ensemble sizes  dv dvs hybrid strategy   tictactoe data set representative group  tween  selected values change signifi   including  instances  features figure supports cantly change search strategy ensem   previous findings seen dy ble size     namic integration expectedly outperforms static integration experimental results ga gassefs show    ga gassefs accuracy grows static integration methods ss wv dy   ensemble size growth greater best integra namic ds start overfit validation set     tion methods ds dvs case         generations show lower accuracies accura                                                   cies dv dvs continue grow  generations                                                shows importance selection appropriate                                                    integration method genetic strategies                                                       conclusions                                                                                                paper considered genetic search strate                                                      gies ensemble feature selection new strategy gas                                                 sefs consists employing series genetic search proc                                                       esses each base classifier shown experi                                               ments gassefs results better ensembles having                                                    greater accuracy domains especially data sets                                                relatively larger numbers features gassefs         ss  wv  ds dv dvs ss  wv ds  dv dvs        significantly timeconsuming ga   figure  ensemble accuracies ga left gassefs right easily parallelized multiprocessor setting   integration methods ensemble sizes tic processor used each offspring current   tactoe data set                                 generation                                                      reasons success gassefs   difference strategies clearer fact each core ga processes leads significant   best integration methods dependencies overfitting corresponding ensemble member   data sets lesser difference shown ensemble overfitted members   tween integration methods data sets dv better ensemble nonoverfitted members   performs ds supports previous findings oliveira et al  shown use   behaviour integration methods tsymbal puu weights combine number objectives fitness   ronen  tsymbal et al               function genetic algorithms use α  case                                                    common approach gives better results 
