                       feature    selection   based     shapley    value                                     shay  cohen    eytan  ruppin                                       school   sciences                                     telaviv university telaviv  israel                                                                        cshayruppin      ¡ posttauacil                                                 gideon   dror                                      department   science                        academic   college telaviv yaffo telaviv  israel                                          gideonmtaacil                              abstract                          containing iid sampled instances ¢¤£¥§© form                                                                                                                              ¥ available train validation test represent          present study contributionselection al ing training set validation set test set respectively                                                                                                              gorithm csa novel algorithm feature se                                                                                                                                          ¥¥ ¢©                                                        given induction algorithm set          lection algorithm based multi     stands classiﬁer constructed training set      perturbation shapley analysis framework  ing induction algorithm input variables nar      relies game theory estimate usefulness                                                            rowed ones  ¢© labels each                                                                                                               algorithm iteratively estimates usefulness                                     abdc                                                                                  ¤ © ¤                                                            stance form ¢¥                features selects accordingly using                                                        value domain §  task feature selection      forward selection backward elimination empir                                                        choose subset  input variables maxi      ical comparison existing feature mize performance classiﬁer test set      selection methods shows backward elimi  follows shall focus optimizing accuracy classi      nation variant csa leads accurate  ﬁer easily optimize performance      classiﬁcation results array datasets    measure area roc curve balanced error                                                        rate    introduction                                         rest paper organized follows section                                                         troduces necessary background game theory  feature selection refers problem selecting input vari detailed description csa algorithm section  provides  ables called features relevant predicting empirical comparison csa feature  target value each instance dataset feature selection selection methods accompanied analysis results  potential beneﬁts defying curse dimension section  discusses empirical results provides  ality enhance prediction performance reducing mea insights success backward elimination version  surement storage requirements reducing training csa algorithm  prediction times paper focuses ﬁrst issue  selecting input variables attempt maximize perfor  mance classiﬁer previously unseen data          classiﬁcation  coalitional game    paper suggest recast problem feature cooperative game theory introduces concept “coali  selection context coalitional games notion tional games” set players associated  game theory perspective yields iterative algorithm payoff real function denotes beneﬁt achieved      feature selection contributionselection algorithm different subcoalitions game formally coalitional                                                                                                                                                                                                  ei     ¥jjk¥      csa intent optimizing performance classiﬁer game deﬁned pair ¢ef¥gh©                                                                                       ne  unseen data algorithm combines ﬁlter set players g¢lm©   real number      wrapper approaches unlike ﬁlter methods features associating worth coalition   game theory  reranked each step using classiﬁer black pursues question representing contribution each  box ranking based shapley value shapley player game constructing value function   known concept game theory estimate assigns realvalue each player values correspond  importance each feature task hand speciﬁ contribution players achieving high payoff  cally taking account interactions features contribution value calculation based shapley    paper use following notations value shapley  intuitive example potential      distribution dataset instances drawn rep use shapley value provided academic set                                     £   ¢¥¥©  resented variables ¢¤£¦¥¨§ ©            ting assume professor running lab      represents input variables vector features § decided distribute yearly bonus      represents discrete target value class £  three sets students fair manner reﬂects actual contribution each student academic success lab case shapley value feature measures  during year students form spontaneous “coalitions” tribution combined performance measure just  groups students each group works publishes sum corresponding shapley values linearity      paper summarizing work coalitions shapley value consequence property                                                                                                           assembled professor paper gets rank payoff function multiplied real number                                                                                               §         §                                                                                                                                                                                                           ¢gh©   ¢gh©  impact factor composing “payoff function” based shapley values scaled    annual data students’ coalitions associ words multiplying performance measure  ated payoffs shapley value provides fair efﬁcient constant does change ranking features vital  way distribute bonus each individual student accord property scheme ranks features ’impor  ing contribution year                tance’        shapley value deﬁned follows let marginal                                          ¡        importance player  coalition         estimating features contribution  using                                                     ¢                                             msa                                                g¢l¤£  ©¦¥ g¢lm©                  ¢¤m©m                                                                                   calculation shapley value requires summing  shapley value deﬁned payoff       possible subsets players impractical                                                               case keinan et al  presented unbiased esti                                                             ¢                    §                                       mator shapley value uniformly sampling permu                         ¢gh©                                    ¢ ©©                                   ¨¢¤                                                                                                             ©¨                                                           tations  estimator considers large                                                                                         small features sets calculate contribution values                                                                                              ¢ ©     set permutations  feature selection algorithm use shapley value heuris      set players appearing  th player permutation tically estimate contribution value feature        shapley value player weighted mean task feature selection realistic cases    marginal value averaged possible subsets players sume size  signiﬁcant interactions fea      transforming game theory concepts arena tures smaller number features    feature selection attempts estimating limit calculating contribution value    tribution each feature generating classiﬁer players mutations sampled set players                                                           ing bound permutation size notice ﬁl            mapped features dataset payoff                                                                                                                                                                   ¢¤ ©  represented realvalued function  measures ter methods equivalent using inter      performance classiﬁer generated using set fea actions taken account feature selection using ran                                                                                                                                                                                                   tures   finally usage shapley value feature dom forests breiman  equivalent       selection justiﬁed axiomatic qualities bounded estimated contribution value                                                                                                                                                                                                                                                             ¢      axiom  normalization pareto optimality game                                                                                                                                                                                          ¨¢¤ ¨¢ ©©                                                                      ¨¢g ©                            §                                                                                 ¡                                     ¢e¥g ©                                ¢g ©m  ¢e¦©            holds                                                                                                                                                                              context feature selection axiom implies  set sampled permutations subsets    performance dataset divided fully size   usage bounded sets coupled method  different features                                   shapley value estimation yields efﬁcient ro                                                        bust way estimate contribution feature task      axiom  permutation invariance symmetry  classiﬁcation detailed discussion msa frame                                                §                                                            ¢e¥g ©                                                                ©               permutation      holds  ¢g      work theoretical background keinan et al       §                  ¢ ©                                                                    contributionselection algorithm  axiom implies value altered arbitrarily  renaming reordering features                  contributionselection algorithm csa described                                                        figure  iterative nature adopt      axiom  preservation carrier dummyproperty                                                                                           forward selection backward elimination approach                            ¢¤£  ©m  ¢¤m©          game ¢ef¥gh©                             §                                        ing subroutine contribution ranks each feature accord                                       ¢gh©m       holds                                       ing contribution value selects  features                                                        highest contribution values forward selection using  axiom implies dummy feature does inﬂu                   ence classiﬁer’s performance receives contribu subroutine selection  eliminates  features  tion value                                          lowest contribution values backward elimination using                                                        elimination repeats phases calculating contri      axiom  additivity aggregation games bution values remaining features given                                §           §      §          ©    ¢e¥ ©          ¢g¡  ©   ¢g © ¨¢ ©      ¢e¥g     holds                         selected eliminated selecting eliminating new             ©j¢¤m©  ¢¤m© ¢lm©  ¢g                                             features contribution values candidate features    axiom applies combination different payoffs alternatively selection subroutine use forward selec      based set features classiﬁcation task tion technique instead   features added ascending order  example accuracy area roc contribution values long classiﬁer’s performance  curve false positive rate false negative rate improves                                                                                            ¢                                        ¥  ¥   contributionselectionalgorithm                            classes  features train size test size                                                             reuters                                        ¢¡¢£¥¤   ¦                                                                        reuters                                              § ©¨  ¢¡¢£¥¤          each                                        arrhythmia                                                             ¢¡¢£¥¤               contribution                       internet ads                                                ¢                                      dexter                                                                                                                                                                            ¢          arcene                                             ¢¡¢£¥¤    ¢¡¢£¥¤  ¡£                           selection                                                     goto                                                  table  description datasets used                    return selected                              higher  likely features redundant                                                                                                                                                                                              figure  contributionselection algorithm forward se tribution selected  minimizes                                                              redundancy dependencies features increasing  accel                                            lection version  input set features contribution                                                       erates algorithm’s convergence algorithm’s halting      value threshold maximal permutation size calculating       ¢      contribution values  number features selected each criterion depends  designates tradeoff  phase contribution routine calculates contribution value number selected features performance        feature  pay function described section classiﬁer validation set forward selection                                                                           ¢                                                                                                             selection routine selects features highest contribu version choosing   means csa selects features    tion values exceed   backward elimination version long exists feature likely improve clas      selection subroutine replaced elimination subroutine siﬁer’s performance selects smaller sets features ¢                                                                            ¢  eliminates  features each phase halting criterion increased increasing opposite effect size  changed accordingly                                                        ﬁnal set features intuitive halting crite                                                        rion stop performance gain achieved  exceed contribution threshold ¢ forward selection restrictive csa’s halting criterion enables se  fall contribution threshold ¢ backward elimina lection features proved useful later stages veriﬁed  tion                                                empirically datasets    algorithm speciﬁcation contri  bution subroutine generalization ﬁlter methods  results  main idea algorithm contribution  data benchmark algorithms  subroutine unlike common ﬁlter methods returns contri  bution value each feature according assistance im test csa empirically ran number experiments  proving classiﬁer’s performance generated using seven realworld datasets number features rang  speciﬁc induction algorithm conjunction ing   table  reuters dataset  features using notation section  assuming reuters dataset constructed following koller  optimizes accuracy level classiﬁer contribution sahami  using reuters document collection  subroutine forward selection calculates contribution arrhythmia database uci repository perkins                                                            et al   internet advertisements database                                           ¢¤ ©  values using following payoff function                                                             uci repository blake merz  collected               £  ¢¡¢£¥¤                                                         research identifying advertisements web pages                                                          ¢ ©    generate classiﬁer  training set train dexter text categorization dataset arcene cancer                                                            dataset nips  workshop feature selec                                   ¢©    evaluate     examples validation set                                                        tion guyon  microarray colon cancer      validation                                                            dataset alon et al                                                             return accuracy level deﬁned  g¢lm©                                                                                        principle csa work induction algorithm                                                                                                                                                                                                             ¥                  ¥                                        computational constraints focused                                                                                                    fast induction algorithms algorithms ef            ¦    case   end case handled return ﬁciently combined csa experimented naive  ing number instances largest class divided bayes nn each datasets measured  total number instances classiﬁer selects training set accuracy each classiﬁer using tenfold cross  frequent class backward elimination quite sim validation set features each dataset sub  ilar payoff calculated sampling permutations sequent work used induction algorithm   gave  set features left each phase elimination highest cross validation accuracy detailed table     maximal permutation size  important role different feature selection schemes com  ciding contribution values different features pared datasets described          selected way ensures different com                                                                                binations features interact inspected induction algorithm performing feature      impact demonstrated section                       selection serve baseline                                                                                                                                                                                                                                                                                                                                                                         number selected features  selection sub regularized linear svm using       package  routine controls redundancies selected features joachims  datasets                                                                                                                                                                      ¤   dataset           fwd   bwd                      reuters dataset feature selection using ran   reuters     nb                                dom  forests did best yielding accuracy level    reuters     nb                                 features far csa   arrhythmia                                  backward elimination version   features   internet ads nn                               koller sahami  example report   dexter                                    markov blanket algorithm yields approximately  se   arcene                                  lected features accuracy levels                                            dataset                                                            table  parameters classiﬁer used csa al reuters dataset csa backward elimination      gorithm each dataset   induction algorithm used did best yielding accuracy level   fea      csa nb naive bayes  number features selected tures comparison koller sahami       forward selection each phase  number features elim port markov blanket algorithm yields approx    inated backward elimination each phase  permutation imately  selected features accuracy levels    size ¡ number permutations sampled estimate   dataset  contribution values explanation hyperparameters  arrhythmia dataset dataset considered  chosen text                                         difﬁcult csa backward elimination did                                                                best yielding accuracy level   features                                                                                                        ¨§                                                            forward selection higher depth value    did      classes split binary classiﬁcation prob                                                            better wrapper implying consider      lems                                                            features concomitantly perform good feature se         filtering using mutual information classiﬁcation lection dataset comparison grafting al               ing   binned continuous domains estimate   gorithm perkins et al  yields accuracy level          mutual information                                   approximately  dataset                                                               filtering using pearson correlation coefﬁcient internet ads dataset algorithms did approx      classiﬁcation using                                 imately leading accuracy levels                                                               csa slightly outperforming oth     random  forests feature selection breiman                                                                ers interestingly wrapper algorithm did      classiﬁcation using                                  select feature ﬁrst phase nn algorithm       feature selection using forward selection wrapper neighbors classes distance      simple wrapper greedily selects feature   each feature checked leading arbitrary selection      improves classiﬁer’s validation performance classes classiﬁer’s performance                                                               constant through phase yielding zero contri                                                      equivalent forward selection csa                                                                                        bution values selecting higher depth      classiﬁcation using performing feature selection levels simple nn algorithm boosted          forward selection csa parameters described perform classiﬁers svm                                                                                               ¤      table  parameters  chosen dexter dataset dexter dataset used      expected number times each feature sam algorithm  decision trees process      pled higher  contribution value threshold                                   ¢                              feature selection linear svm perform                                        stopping selection    termination fea    actual prediction features selected      ture selection ﬁxed choosing contribution value                    ¢                                             did satisfying accuracy lev                       threshold      hyperparameter selection                                ¢                                 els feature selection algorithms                      ¤          formed                                impractical use svm csa large datasets                            classiﬁcation using performing feature selection overcome difference classiﬁers          backward elimination csa parameters   forming feature selection classiﬁer used                                         ¤      scribed table  parameters  chosen  actual classiﬁcation added optimization phase      expected number times each fea forward selection algorithm stopped          ture sampled higher  contribution value phase tenfold crossvalidation performed                                           ¢                                                      threshold stopping elimination    hy     dataset similar way used opti                                                 ¤      perparameter selection performed   mize ﬁlter methods simple mutual information fea      ¢                                                     ture selection performed best followed closely                                                            contributionselection algorithm backward elimi    avoid overﬁtting validation set used calcu nation version random forests implies      lating payoff csa used mfold cross validation dexter contribution single features signiﬁcantly                     £¢ ¡ ¤¢¤ ¦¥  instead single      set                            outweigh contribution feature combinations    feature selection classiﬁcation results          task classiﬁcation forward selection algo                                                            rithm did linear svm feature selec  table  summarizes classiﬁers’ performance test tion signiﬁcantly lower number features  set number features selected each ex  arcene dataset just case dexter  periments accuracy levels fraction correctly use process feature selection lin  classiﬁed test set instances                             ear svm perform actual prediction features                                                         dataset      wrapper      fwd        bwd                                                                                                         arrhythmia slp−   reuters                                                     dexter slp−   reuters                           −   arrhythmia                 internet ads                              −   dexter                   arcene                                 −                                                                             −     fs   svm       corr        mi         rf               log  frequency                               −                                                         −                                                                                 −                             −   −   −    −   −    −   −                                                    log cv                                                                        figure  powerlaw distribution contribution values log  table  comparison accuracy levels number features se log plot distribution contribution values absolute value  lected different datasets upper table wrapper fwdbwd ﬁrst phase arrhythmia dexter prior making  csa forward selectionbackward elimination parameters feature selection demonstrates power law behavior corre  table  table fs feature selection svm sponding plots datasets show identical powerlaw char  linear svm feature selection corr feature selection using acteristics different slopes eliminated  pearson correlation mi feature selection using mutual informa sake clarity  tion rf feature selection using random forests accuracy levels  calculated counting number misclassiﬁed instances  given percentages number features selected given backward elimination gradual  brackets                                             stable increase contribution values eliminated                                                        features peaks graph contribution values      selected csa backward elimination obtained figure demonstrate contribution values change      better performance rest algorithms csa iterates  case selection single fea     dataset csa backward elimination ture considerably increased contribution value      feature selection using mutual information yielded feature pointing intricate dependencies features      best results poor performance csa forward figures   assist explaining      selection explained poverty data com ward elimination usually outperforms feature selec      paring number features algorithm selected tion methods including forward selection high      ﬁrst phases features explain train dimensionality datasets feature assists pre      ing data coincidence avoided selecting fea diction merely coincidence selected ac      tures truly contribution task classiﬁcation count truly informative features forward selection      phenomenon explained portrait section  penalized severely case signiﬁ                                                        features chosen backward    summary   datasets csa backward                                                        elimination maintains signiﬁcant features  elimination achieved best results cases csa                                                        non eliminated set feature truly enhances classi  achieved second best result                                                        ﬁer’s generalization validation set    closer inspection results               eliminated leads stable gen                                                        eralization behavior backward elimination test set  msa intent capturing correctly contribution                                                        through algorithm’s progress figure   elements task enables examine distribution  contribution values features figure  depicts  loglog plot distribution contribution values  final notes  ﬁrst phase arrhythmia dexter prior making contributionselection algorithm presented paper  feature selection distribution follows scalefree views task feature selection context coali  power law implying large contribution values abso tional games uses wrapperlike technique combined  lute value rare small ones quite common novel ranking method based shapley  justifying quantitatively need feature selection tribution values features classiﬁcation accuracy  datasets observed possess similar power csa works iterative manner each time selecting  law characteristic                                   new features eliminating taking account    behavior algorithm through process fea features selected eliminated far  ture selectionelimination displayed figure  csa similarly wrapper algorithms restricted  forward selection algorithm identiﬁes signiﬁcant features selection induction algorithm used evaluating fea  ﬁrst phases sharp decrease contri tures sets time limitations problem  bution values features selected following phases reduced parallelizing advantage shared                                                        
