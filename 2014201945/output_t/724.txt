   hybrid discriminativegenerative approach modeling human activities      jonathan lester tanzeem choudhury nicky kern gaetano borriello blake hannaford         department electrical engineering university washington seattle wa  usa                               intel research seattle seattle wa  usa        department science darmstadt university technology darmstadt germany           department science university washington seattle wa  usa                         abstract                          typically accelerometers locations body                                                         bao intille  kern et al  placement       accurate recognition tracking human        sensors multiple predefined locations quite       activities important goal ubiquitous     obtrusive limitations approach       computing recent advances development  ultimate goal embed devices       multimodal wearable sensors enable gather  clothing technology far commercially       rich datasets human activities   available widely accepted result single sensing       problem automatically identifying     device integrated existing mobile platforms       useful features modeling activities remains cell phone appealing users       largely unsolved paper present hybrid likely garner greater user acceptance work bao       approach recognizing activities combines intille  shown appropriate sensor subset       boosting discriminatively select useful features locations does effect recognition scores       learn ensemble static classifiers    significantly  compared       recognize different activities hidden markov sensors use single sensor reduced       models hmms capture temporal             average accuracy  hypothesis       regularities smoothness activities tested incorporating multiple sensor modalities offset       activity recognition using  hours information lost using single sensing device       wearablesensor data collected volunteers furthermore multiple modalities better suited       natural unconstrained environments models    record rich perceptual cues present       succeeded identifying small set maximally environment cues single modality fails       informative features able identify  capture multiple modalities shown promise       different human activities accuracy    earlier activity recognition experiments lukowicz et al                                                                                                                 capture diverse cues movement sound light      introduction                                      ongoing activities built small   task modeling human activities bodyworn  sensing unit  sq includes different   sensors received increasing attention recent years sensors accelerometer audio irvisible light high  especially ubiquitous computing ubicomp field frequency light barometric pressure humidity temperature   bao intille  lukowicz et al  patterson et compass using sensors collected large   al  originally research annotated dataset various human activities   activity recognition using vision sensors gavrila volunteers period weeks compute    pentland  increasingly     different features sensor   dominated various types wearable sensors like  modalities attempt capture various attributes   accelerometers audio fertile application domain raw signal   activity recognition health care arena especially activity recognition choice sensors   elder care support longterm healthmonitoring  features derived driven human   assisting cognitive disorders addition activity intuition easily available   recognition important component modeling higher performance practicality using right features   level human behavior tracking routines rituals social crucial recognition working developing   interactions                                         framework allows systematically identify     majority research using wearable devices modalities features useful machine   concentrated using multiple sensors single modality recognition discrimination natural human activities end want models accurately recognize different portions data furthermore calculating   track variety activities lightweight features integral features incorporate   run devices like cell phones longer time window varies seconds   people carry minimizing computation long minute restrict time windows use   cost recognition important goal  data past functions     main approaches used classification latency    machine learning generative techniques                                                         feature selection discriminative activity models   model underlying distributions classes ii                                                         earlier work shown discriminative methods   discriminative techniques focus learning                                                         outperform generative models classification tasks ng   class boundaries rubinstein hastie                                                          jordan  additionally techniques bagging   approaches used extensively vision                                                         boosting combine set weak classifiers   wearable sensing communities recognizing                                                         improve accuracy overfitting   various human behavior activities work presented                                                         training data schapire  viola jones    paper hybrid approach combines                                                         shown boosting used method   techniques modified version adaboost proposed                                                         combining classifiers method selecting   viola jones  used automatically select                                                         discriminative features use proposed approach   best features learn ensemble discriminative                                                         select fraction total features train   static classifiers activities wish recognize                                                         simple ensemble classifiers recognize broad set   second classification margins static classifiers                                                         activities   used compute posterior probabilities   used inputs hmm models discriminative capturing temporal regularities   classifiers tuned make different activities activities people perform certain natural   distinguishable each hmm layer regularities temporal smoothness people   static classification stage ensures temporal abruptly switch forth walking driving   smoothness allows continuously track    car recent history help predicting   activities                                           present using sequence posterior probabilities     rest paper organized follows section  computed instantaneous classifiers train   provides overview activity recognition hidden markov models hmms significantly improve   section  presents feature selection discriminative performance smoothness recognition   classifier training method section  describes incorporating static classification results   results classifiers combined hmms  overcome weakness hmms effective classifiers   section  describes experimental results  jaakkola haussler     performance section  discusses   conclusions possible future directions               selecting right features                                                          given rich set sensor data features classifiers      activity recognition overview              work best select right features enable   problem address systematic identification classifiers discriminate classes   modalities features suited accurate remove features useful   recognition natural human activities second problem confuse classifiers possible   tackle features effectively used hand pick optimal features certain activities   develop models accurately recognize track various viable solution number activities   activities brief overview different large sensor signals intuitive   components activity recognition        scenarios automatic techniques finding right set                                                         features increasingly important practical activity  sensing feature extraction                        recognition use minimal number features   using shoulder mounted multisensor board figure simplest possible models needed high accuracy    collect approximately  samples data   second reduce dimensionality bring   details data compute total  features  feature selection activity classification   include linear melscale fft frequency coefficients    using boosted decision stumps   cepstral coefficients spectral entropy band pass filter paper assume people engage different   coefficients integrals mean variances combine type activities given set activities   features various sensors produce  dimensional    assume set   feature vector hz sensors training data each activities each sample                                                                                                    different sampling rates multiple instances training set consists feature vector fk      feature  second window operate extracted sensors each activity ai  figure  flow diagram describing classification presented paper sensor board records sequence raw sensor   recordings compute feature vector pick fifty features class feature vector supply   inputs ensemble decision stumps classifier each decision stumps classifier outputs margin time   sequence margins converted probabilities fitting sigmoid sequence probabilities supplied   hmm classifiers each outputs likelihood class highest likelihood classified class   interested finding ranking feature set    use results decisionstump based classifier     iii    rk  based usefulness recognizing later sections decision stump finds optimal                                               τi                θi  activity  want cutoff point  threshold  each feature fm minimizes                                                    τ                                           θi  ranked feature set adding features  weighted error hfmm  fmm                                                              −  does significantly improve accuracy classifier hfmm    ci  ∆errorcii   errorcii  ≤ε                      rrn               rr  τ            boosted static classifiers classification margin                             τi         τi  reason estimating    data point reflect confidence prediction   reduce computational complexity classifiers schapire et al   margin example   extracting useful features final weighted fraction weak classifiers votes assigned   goal classifiers run devices users carry correct class    wear computational costs classifiers                         τi                                                                                    αiihf  critical                                                                ∑ mm                                                                        mf         τi    each activity  iteratively train ensemble                          αi                               iii                                                      ∑  weak binary classifiers hn  figure                    iii                                                  iiii  obtain ranking  rk  features using          hff  signm   variation adaboost algorithm proposed viola constructing classifiers output posterior   jones  weak classifiers constrained use probability useful especially want combine   feature each iteration boosting results multiple classifiers later method   select feature associated weak learner    computing posterior probability directly fit sigmoid                                       εi  hfmmthat minimizes training error mmf  function output ensemble classifier platt                          εi  weighted data error mmf   used reweight  figure case posterior probabilities                                                αi  data iteration compute weight derived follows      hf end process ranking                   ϕ   mm                                                                      mf   features based useful each feature               ϕ                                                                pc ϕ       constant    discriminating ai  activities j≠            mf                                       set weak classifiers hfmmand weights static classifier predicts label each data point                     αi  classifiers  final output weighted independently time independence   combination weak classifiers estimating assumption clearly invalid prediction previous   error ci  function number features used data points help current classification   τi ci  given data point temporal model uses confidence predictions   prediction ci                                   classifiers csi  instead raw features likely                                                         greater impact performance ability                              τi                hiiif α sign                 recognize activities continuous time chunks                            ∑ mm                                                          allow learn people transition activities     each classifier ci  uses τi  features allow learn people’s behavior   fraction total number features available activity patterns section      ii    fτi                                    combine confidence values static classifiers     tried different weak classifiers build timeseries models activities   discriminative decisionstump ii generative naïve   bayes model conditional probability distributions   incorporating prediction history using   modeled using histograms  binsdimension hidden markov models   weak classifier experiments decision stump                                                         hmms successfully used modeling different   consistently outperformed naïve bayes classifier                                                         types timeseries data speech recognition gesture                                                                      accelerometer                                                                   ambient light irvis                                                                             audio                                                                     barometric pressure                                                                       digital compass                                                                      hifreq vis light                                                                            ir light                                                                      relative humidity                                                                   temp barometer                                                              temp relative humidity                                                                         visible light                                                    table  percentage features  originated                                                    different sensors averaged activity classes                                                        figure  testing error rates class function number alternatively trained various states    features selected  features selected    testing errors classes leveled data graphed single hmm recognize different activity classes    averaged smaller feature selection runs learn transition characteristic activities                                                    choose activities primitive    tracking use hmms capture temporal single transition statistic meaningful    dynamics instead directly using raw features believe output hmms used    selected previous section trained hmms train single dynamic model complex behavior    using posterior probabilities static classifiers transition statistics    advantage using posterior probabilities informative     advantage results discriminatively    trained classifier reduce complexity  experiments    hmms earlier work jaakkola haussler     shown benefits combining generative models validate approach recorded  hours data    discriminative classifiers deriving kernel functions consisting large number activities sitting    probability models clarkson pentland  walking jogging riding bike driving car using    oliver et al  used output hmm model wearable multisensor board  dataset collected    input hmm yin et al  used multiple days volunteers    output static classifiers directly hmms speech researchers various indoor outdoor locations     reading application compute recordings long stretches hour    margin class posterior probability classifiers average duration activities    effective raw outputs platt ranged seconds entering building hours                                              driving car volunteers asked                                   λ                performing series activities naturally     each activity new hmm model figure   learned using sequence examples individual specific constraints order example “go                                                    building walk inside” capture daytoday   instances construct new input feature space based                                                    variations activities collected multiple instances   posterior class probabilities                                                     various activities course weeks                      pc                   average hour data activity                                                     instances activity                                                                                            feature selection                      pcn                                                  feature selection stage selected  total                                                    data available each class training based                                                                                     given set observations ff    each activity training examples derived ranking features                                 λ   learn parameters hmm  using standard each activity individually using boosted decision   expectationmaximization em method during testing stump procedure described section  figure  shows   continuous sequence use compute testing error function number features used                       λ   likelihood value lι   time using sliding classification results classification    window duration ∆t  figure –          error tapers τi   features classes                                                    pick features                     λff                     littttit          performance improves slightly testing error     final segmentation classification based improving   features practical    hmm highest likelihood value      advantage features selection significantly                                                    reduce computational burden resource    c maxl  figure        iii                                          constrained devices drastically affecting figure  output static decision stumps classifiers hz hmm classifiers trained output probabilities static   classifiers continuous  minute segment data results overlaid ground truth obtained   annotating video recorded webcam worn volunteers video used determining ground truth   additional sensor input      performance performing boosting                                                        continuous classification results   weighting data selecting discriminatory features                                                         decision stumps results table  quite   successively based error perform better                                                         good figure  illustrates classification   taking nonboosted reweighting approach                                                         errors encountered continuous trace majority   selecting best  features accuracy true positive                                                          trace tends correctly classified decision   true negative  total  examples boosted                                                         stumps scattered misclassifications   features selection average  higher                                                         addition hmm layer static classifier   nonboosted method table  lists contribution                                                         helps smooth classification errors shown   different sensors final classifier majority                                                         line figure       features came accelerometer audio                                                           parameters hmms trained using    barometric pressure sensors barometric pressure data                                                         example scenes average  minutes scenes each   useful distinguishing activities involved floor                                                         class each hmm hidden states gaussian   transitions walking updown stairs elevator updown                                                         observation probabilities classification performed   sensor sensitive pick pressure                                                         using  second sliding window  second overlap   differences single floor                                                         table  shows sliding window performance results   static classification results                         hmm using posterior probabilities inputs tested   using  features tested performance concatenated test scenes overall accuracy   ensemble classifier different weak classifiers – case  interesting note points   decision stump discriminative ii naïve bayes  figure  hmm ground truth differ appear   generative total duration test dataset somewhat natural realistic example classifying   half hours decision stumps outperformed region ground truth walking sitting   naïve bayes classifiers large percentage table  shows standing fact hmm output reveals   precision true positivetrue positive  false positive deficiencies ground truth example   recall true positivetrue positive  false negative segments ground truth marked walking   numbers  activities dataset using fact standing determined postanalysis video   ensemble decision stumps table  lists average experimenters correctly recognized standing   precision recall numbers naïve bayes hmm   decision stump classifiers                             compare performance standard hmm                                                         approach trained new set hmms used                                                          raw features inputs output static                                                         classifiers performance hmms 
