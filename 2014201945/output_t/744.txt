                  kernel conjugate gradient fast kernel machines                                  nathan ratliff andrew bagnell                               robotics institute carnegie mellon university                                         pittsburgh pa  usa                                        ndrdbagnellricmuedu                          abstract                            propose novel variant conjugate gradient algo                                                        rithm kernel conjugate gradient kcg designed speed      propose novel variant conjugate gradi learning kernel machines differentiable loss func      ent algorithm kernel conjugate gradient kcg   tions gaussian process mean inference kernel logistic      designed speed learning kernel machines regression algorithm motivated understand      differentiable loss functions approach ing gradientbased methods rely implicitly      leads better conditioned optimization problem particular metric inner product scholkopf¨  smola      during learning establish upper bound    natural kernel learning problems algo      number iterations kcg indicates rithm inherit metric functions kernel pro      require square root num vides section  show approach      ber iterations standard conjugate gradient interpreted riemannian metric method similar amari’s      quires practice various differentiable ker natural gradient amari  nagaoka       nel learning problems ﬁnd kcg consistently   kernel metric expensive compute      signiﬁcantly outperforms existing techniques  section  establish upper bound number      algorithm simple implement requires iterations kcg indicates require fewer      computation iteration standard ap  square root number iterations stan      proaches motivated reproducing    dard conjugate gradient requires algorithm simple      kernel hilbert space rkhs theory    implement requires computation iteration      show datastructure techniques recently used standard approaches motivated reproducing      speed kernel machine approaches    kernel hilbert space rkhs theory practice various      matched algorithm reducing dominant differentiable kernel learning problems ﬁnd kcg consis      costs training function evaluation rkhs tently signiﬁcantly outperforms existing techniques      ner product computation                            recent research demonstrated beneﬁts space                                                        partitioning datastructures speed certain kernel ma    introduction                                       chines shen et al  gray  moore  show  kernel methods various incarnations gaussian techniques work kcg reduce  processes gps support vector machines svms kernel dominant computational burdens training rkhs function  logistic regression klr recently preferred evaluations inner product computations  approach nonparametric machine learning statistics  enjoy status conceptual clarity  preliminaries  strong empirical performance theoretical foundations brieﬂy review theory kernel machines    primary drawback kernel methods computa terms reproducing kernel hilbert spaces  tional complexity gps require inversion × scribe regularized risk functionals interested  variancekernel matrix implying running time optimizing during learning ﬁnish reviewing func  size training set svms require sim tional gradient generalization standard gradient  ilar computation solve convex program inner product spaces functions  tense research gone fast specialized approximations  scholkopf¨  smola                              reproducing kernel hilbert spaces    stateoftheart approaches kernel learning revolve rkhs functions hk complete inner product space  largely techniques iterative optimization algo known hilbert space arises completion                                                                             bx              ∈x  rithms learning representing solution set basis functions  kx    subset original data points algorithm applies  x×x →r symmetric positivedeﬁnite kernel func  directly line research address tion continuous domain known  conclusions                        index set common kernel used exclusively                                                    ijcai                                                    experiments exponential radial basis function rbf numerous important examples occur litera                             −x−x                                      ture scholkopf¨  smola  focus  kx   σ   rkhs inner product                                                      best known kernel algorithms non  functions    α kx      β kx                                             differentiable support vector machine particu  ﬁned                                                                     larly important tools machine learning classiﬁcation               fg hk      αiβjkxixj            regression                         ij  central idea rkhs reproducing prop  functional gradient  erty follows directly deﬁnition  states basis functions kx  ∈bx represen largescale problems problems direct                                                      lution inapplicable popular approach optimiz  ters evaluation formally ∈hk ∈x                                                      ing functionals through use iterative gradient   fkx  hk  fx basis functions normal  ized means evaluation scalar projec based techniques gradientbased algorithms steep  tion kx  note exists simple mapping est descent traditionally deﬁned respect gradi     x→bx                                      bx     ent arising euclidean inner product param  φ       domain rkhs basis  deﬁned kernel φxkx  follows eter vectors ways given regular  x ∈x                                              ized risk functional parameterized each                                                        gives rise different parameter gradient intuitively                                               φxφx  hk   kx kx  hk  kx    natural follow gradient deﬁned uniquely rkhs  complete overview concepts inner product review basic concepts  aronszajn                                     functional gradient    fundamental result ﬁrst proven kimeldorf  wahba functional gradient deﬁned implicitly   generalized scholkopf¨ et al  linear term change function small pertur  representer theorem makes possible direct bation  input mason et al   minimization particular class functionals states                    ∇                                            ⊂x×r                      gf f    fg  o   given subset     xiyi            dataset minimizer functional form                                                       following deﬁnition using rkhs inner product                                              fcx fx xnynfxn  hk    h  allows deﬁne kernel gradient regular                                                            x×r    →  arbitrary  ∞ →  ized risk functional scholkopf¨  smola  use three  is strictly monotonically increasing form f˜  basic formulas easily derived deﬁnition      ∈d αikxi    xi                                                     gradient evaluation functional deﬁne fx                                                               →     regularized risk functionals                                 fxffx       αikxix                                                            ∇  fkx   important class functionals common machine learn  ing representer theorem holds regular  gradient square rkhs norm deﬁne f   ized risk functional scholkopf¨  smola             →                                                                                              ff  hk    ff hk                  n                                         ∇                                   λ                          kfff          rf     lx fx   ffh                                                         chain rule let  → differentiable func                                                                            tion let  hk → arbitrary differentiable  functionals combine datadependent risk term                                                                                        functional ∇kgf  f∇kf  “prior” regularization term controls complexity  solution penalizing norm function straight forward application formulas brings  rkhs work focuses case differentiable following result kernel gradient regularized  argument important kernel machines risk functional equation   property instance write kernel logistic      n                                           ∈−                          ∂  gression zhu  hastie  form let    ∇  rf        lx  ∇ fλf                                                                      ∂z      xi  xi                                                                               n                                                     λ                              n                            yifxi                                    ∂       rklr    log            ff hk                                                                                                             lxiyiz fxikxi                                                                          ∂z                                                                    similarly write popular gaussian process            n  gression mean function regularized        λ    αikxi  squares classiﬁcation algorithm form                                                                                                                                    n                                                  n                                 λ                                      ∂                 rrls    − fx     ffh                              lxiyiz fxi  λαi kxi                                                                  ∂z                                                                           differently motivated algorithms optimize use αi denote parameters expan  functional                                           sion terms kernels kxi                                                    ijcai                                                      equation  allows easily ﬁnd kernel gradient algorithm  kernel conjugate gradient  functionals described use remainder                                                                               →     ∈h  paper                                          procedure kcgf                                                                           ←    kernel logistic regression klr                                                                                                   ←∇                                n                                               kf  γj kxj                                yi                               ←−     ∇krklr     λαi                kxi                                        e−yifxi                                                                                           gigi hk do                                                                      ←    regularized squares rls gaussian process          fi   fi  λihi λi argminλ fi                     n                                               λhi                     ∇                   −                                        ←∇                           krrls   fxi  yi  λαikxi              gi     kf fi  γj   kxj                                                            hi   ←−gi        ηihi  ηi                                                                           −                                                                                 gi gigi hk γ   −γ   kγ    kernel gradient riemannian metrics                                                                                                                gigi hk         γ   kγ  described kernel gradient function    ←   linear combination basis functions ker   end  nel gradient equation  demonstrates property sim  return fi  ilar representer theorem ∇kf  end procedure                             ∈    γikxi appropriate γi  words  kernel gradient represented ﬁnitedimensional                       subspace sd spanb    hk gradient descent type   kernel conjugate gradient algorithm                     method through  amounts modifying coefﬁ theory practice understood conjugate  cients αi current function γi gradient cg methods outperform standard steepest descent  understand kernel gradient modifying parameters procedures ashby et al  techniques           f˜ ← − λ∇kf ⇔ α˜i ← αi − λγi          used profusely machine learning particular  just standard gradient descent algorithm dif regularized risk minimization kernel matrix inversion                                                        gibbs  scholkopf¨  smola   ference coefﬁcients γi kernel gradient  parameter gradient ver section present algorithm term kernel                                                        conjugate gradient kcg takes advantage conju  ify differ ∇αf fkγ kernel  matrix γ vector coefﬁcients            gate direction search utilizing rkhs inner prod                                                                        derive relation way starting uct fg hk  α kβ algorithm  gives general non  parameter gradient deﬁne riemannian metric linear kcg algorithm polakribiere form ashby et al  sani  space parameters deﬁnes  essence algorithm  comes directly conju  tion size space parameters consider gate gradient replacing gradients functional  alternate deﬁnition gradient direction steepest equivalents replacing euclidean inner products  ascent “small” change coefﬁcients α         rkhs inner product                                                          note computational complexity iteration           ∇f αmaxf   α  γ st γ                      γ                                  kcg essentially identical conventional  shown taking γ γt γ gives vanilla pa parameter conjugate gradient pcg algorithm intuitively                            α                           kernel inner product takes time compared  rameter gradient ∇αf  deﬁning norm respect                                                     vanilla inner product used pcg kcg corre  rkhs inner product γ hk   ij γiγjkxixj                                           ∇          spondingly efﬁcient gradient computation  γ kγ  gives functional gradient coefﬁcients kf   ∇  fkγ  ∇      γ kx  pos    −∇                                                 α                                   αf  hassani                               sible case rls step through iteration each    interpretation kernel gradient makes connec algorithm show number operations equiva  tions metric methods clear instance lent                              amari amari  nagaoka  considers use met emphasize despite somewhat involved deriva  ric derived information geometry leads “nat tion implementation algorithm just simple ex  ural gradient” algorithms applicable                                                        tension pcg differences change  compute metric probabilistic models inner product αt β → α β kx αt kβ  given gaussian processes klr unfortunately comput                  ij                                                          different ways simpler gradient computa  ing natural gradient cases expensive                                                        tion point line optimization step   instance gaussian processes expensive inverting                                                        solved closedform case quadratic risk func  kernel matrix computational difﬁculty                                                                                        tionals rls starting point  α kx   striving avoid contrast computing kernel gradi                                                                                         search direction  γ kx   ent cheap cheaper fact standard parameter                                                                                                       gradient                                                                            −α  kγ                                                                    arg min  λh      furthermore practice ﬁnd deriving kernel gra      λ               γ  aγ  dient using functional gradient rules easier deriving pa hessian quadratic functional pa  rameter gradient                                     rameterized α note formula differs                                                    ijcai                                                    derived parameter gradient −αt γγt aγ  kcg analysis  numerator’s inner product common theme derived kernel conjugate gradient algorithm  algorithm theoretical experimen                                                                                      normative point view arguing fg hk deﬁned nat  tal results given suggest little reason ural notion inner product rkhs  prefer pcg kcg differentiable kernel optimization procedure strong empirical perfor  algorithms                                           mance kcg noted previous section                                                        sense surprising given using “correct” inner    experimental results  kernel conjugate            product deserves analysis examine linear case     gradient                                           rls analysis transparent                                                        presumably similar results hold near optima nonlinear  benchmarked kcg pcg classiﬁcation risk functionals  regression tasks cases kcg signiﬁcantly note classic bound error reduction cg  formed pcg                                           luenberger     ﬁrst test performed using klr usps                         √                                                                                         −     dataset trainingtest size  com                √κ                                                                        eia ≤             ea  mon onevsall task recognizing digit  used                      κ   length scale hyperparameter σ used rifkin iteration number hessian                                                                                                   et al  rls classiﬁcation regularization quadratic form condition number κ xa  ax                                                                            −   ∗  stant λ  figure  summarizes results log scale norm ei  xi  loosely√ speaking gives    second used rls regression classiﬁcation running time complexity κ pcg  using abalone soil datasets addition usps start analyzing effective condition number  dataset abalone dataset  consisted  training kcg essentially variants cg algorithm’s  examples  test examples  attributes exper dynamics described terms preconditioning  imental setup equivalent smola  scholkopf¨ spectrum hessian ashby et al    soil dataset contained threedimensional exam veriﬁed inspection algorithm kcg equiva  ples soil ph levels areas honduras partitioned lent implicitly preconditioned conjugate gradient algo  training set size  test set size  rithm preconditioner kashby et al  follow  dataset corresponding hyperparameters λ  ing theorem relates running time pcg kcg light  σ  provided gonzalez  bound given  results summarized figure                  theorem   let κpcg condition number rrls    rls exists quadratic                   equation  let κk condition number ker                                                        nel matrix kxixjij  condition number                                                pα   α   λiα − α                κkcg  resulting preconditioning rls risk functional                                                                             relation κpcg  κk κkcg                                                                       ≥     ≥     ≥  provide lower bound regularized risk gibbs proof let σ σ  σn eigenvalues   scholkopf¨  smola  theory suggests condition number κk  σσn                                                                                                                                       hessian rrls    λk  eigenvalues  upper bound kcg converges comparably    lower bound upper bound pcg lags σi  λσi  σiσi  λ given terms eigenvalues  considerably implies faster convergence implies       gap termination criterion scholkopf¨  smola                   σ   σ  λ        σ  λ    rightmost plots figure  contrast iterations    κpcg                 κk                                                                                 σn  σn  λ        σn  λ  equivalent multiples wallclock time speeds pcg                                                        symmetric positivedeﬁnite preconditioned  kcg rls klr plot number                 −       −    iterations each reach level performance hessian   λkk    λi                                                        corresponding eigenvalues σi  λ κpcg   terms loss dataset plots terminated                                            vergence achieved measured gap termination κk κkcg  criterionscholkopf¨  smola  hyperparameters condition number κk typically large  chosen holdout set ﬁgures conﬁrm particular regularization constant decreases  analysis section suggests takes asymptotic bound convergence pcg approaches  square time achieve level square bound kcg alternatively reg  performance using pcg does kcg finally ta ularization constant increases κkcg approaches  implying  ble  directly compare number iterations needed convergence bound kcg convergence  achieve convergence number data sets taken bound pcg remains bounded oκk   uci data repository rls klr expect number iterations kcg dramati  averaged datasets kcg  times faster cally pcg  standard conjugate gradient approach                   informative note decrease computational                                                        complexity pcg kcg oκ oκisat     uci  repository httpwwwicsuciedu      steepest descentoκ pcg                                                               ∼mlearnmlrepositoryhtml                             oκ    luenberger                                                     ijcai                                                    figure  upper left shows relative performances log scale kcg pcg usps data set optimizing klr  remaining three leftmost plots show relative convergence rls green red lines depict pcg performance upper  lower bound gapconvergence quadratic forms light blue line gives signiﬁcantly tighter performance kcg  upper bound column shows beneﬁt using kdtrees single run training set size  using kcg rightmost plots show equivalent number pcg iterations required achieve performance  iteration kcg covtype data set uci data repository right right show performances  rls klr respectively approximately quadratic relationship seen cases theory suggests      treeaugmented algorithms                          dominated computation parameter gradi                                                        ent rewrite parameter gradient ∇  stationary kernels case ma                                   α                                                        ∇ fx ∇ fx∇ fx   jority basis functions bd nearly orthogonal                                                                          ducing complexity ﬁnding ∇kf ∈hk  stems relationship degree orthog evaluating times case kcg al  onality basis functions kx kx ∈bd                                                     gorithm trees tradeoff balances  euclidean distance  case iteration complexity essentially equivalent  using exponential rbf kernel given kcg pcg using treeaugmented function evaluation  orthogonality basis functions increases ex                                                          noting fxfx   kα   suggests  ponentially square euclidean distance                               −                                      closed form quadratic line minimization upper  points                                       lower bounds rls kcg pcg eas    previous work has shown evaluation rkhs                                                        ily augmented expanding hessians au   functions fx   αikxix fast                                                        λk   case upper bound al  λi  holds using nbody type algorithms gray  moore  case lower bound used tree  intuitively idea store training data space augmented rls experiments described  partitioning tree kdtree moore   used experiments recursively descend  tree during evaluation pruning negligible contributions  experimental results  treeaugmented    maximum minimum impact each set pruned    algorithms  points easily calculated resulting upper lower experiments addition using soil dataset  bounds evaluation error demonstrate described section  performed large scale rls regres  datastructures algorithms used reduce                                                       sions using treeaugmented kcg variable sized sub  iteration  computational cost kcg pcg sets pugetsound elevation map using hyperparameters  during learning evaluation                λ  σ   knnπ size    inner loop computational bottleneck kcg   training set size entire height map  evaluating functions calculating kernel inner case chose   ×       product  we rewrite rkhs inner product  largest resulting datasets order   tween       αikxig       βikxi  points noted naive¨ implementation        fg hk      ij αiβjkxixj   αigxi case did cache kernel evaluations kernel matrix  ducing computational complexity rkhs function  matrices datasets  points proved  evaluations simultaneously encompass  bottlenecks similarly iteration complexity pcg  httpwwwccgatecheduprojectslarge     models                                                    ijcai                                                    
