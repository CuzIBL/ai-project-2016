                                   contextdriven predictions                                    marc bellemare     doina precup                                             mcgill university                                        school science                                      marcgbdprecupcsmcgillca        keywords  prediction learning associative memories observable agent long periods time  contextbased model                                   imagine example environment divided                      abstract                          gions certain observations appear certain                                                        gions objects evolve world involve      markov models keystone artiﬁcial   ing track additional state information recent      intelligence decades  work predictive representations sutton tanner       main unsatisfactory environment mod     littman et al  aims bridge gap explicitly      elled partially observable pathological structing state sufﬁcient statistic predicting      examples history ﬁxed length suf  future experience agent      ﬁcient accurate prediction decision making      hand working hidden state like goal work build predictive represen      hidden markov models partially observable   tation based ﬁxed length history      markov decision processes high computa    learned incrementally data speciﬁcally work      tional cost order circumvent problem geared environments observations subjec      suggest use contextbased model  tive systems hope state agent      approach replaces strict transition probabilities needs concerned based directly ac      inﬂuences transitions method proposed     cessible formally observations      provides tradeoff fully partially lead better predictions immediate future work      observable model discuss capacity    based heteroassociative memories sparse dis      framework model hierarchical knowledge  tributed memories kanerva  puts emphasis      abstraction simple examples given predicting future events way palliating      der show advantages algorithm      noisy inputs model handle new observations                                                        generalize certain extent neces                                                        sity model speciﬁcation advance deﬁning    introduction                                       observation features paper explicitly dis  ability predict future events necessary compo cuss planning rewards assume predictions  nent intelligent agents facilitates accurate planning used determine actions  standard approach predict future solely through similar problems explored connectionist  observations example using ﬁxedorder model markov literature example elman  considers notion  chain unfortunately history ﬁxed length context neural network closely related  sufﬁcient accurately predict observation previous state through simple tasks shows  methods using variable history window mccallum recurrent neural network learn base predic   work practice largely heuristic tions past inputs summarized hidden units’ activa  different approach making predictions through time tion research symbol prediction task close  introduce notion latent hidden state en described bose et al   vironment hidden markov models rabiner  neural network approach problem focus  partially observable markov decision processes kael particular kind network architecture implementation  bling et al  models clearly separate observa recently boltzmann machines used prediction  tions hidden states track current state through time taylor et al  proposed algo  through belief vector assuming hidden state rithm signiﬁcantly different mentioned  requires knowledge state transition focus weighting past observations direct fashion  probabilities assumed known learning agents through internal state agent simi  approach appears restrictive knowledge necessarily lar prediction task addressed gopalratnam cook  bounded state representation furthermore true  context text parsing ap  state involve elements proach data structures geared dealing                                                    ijcai                                                     symbols approach works discrete contin prediction purposes  uous observations experiments contain discrete associative memory frameworks proposed  observations                                   sdms used today ﬁnd hop    paper organized follows section  brieﬂy ﬁeld networks hopﬁeld  boltzmann machines  review sparse distributed memories based fahlman et al  rely idea neuron  ideas discuss hopﬁeld networks boltzmann ma like elements share undirected connections  chines beneﬁts sparse distributed memories neighbors simplest version each element  section  formally deﬁne framework fol states each undirected connection associated  lowing discuss simple algorithm learning weight inﬂuences corresponding neighbor  observations section  examples showing ing certain state based weight sign usually  predictive power algorithm section  finally positive weight indicates units  section  discusses sort environments state negative weight indicates  method suitable current failings compared existing different states attempts minimize en  models used obtain abstraction  ergy energy increases units                                                        states violate inﬂuence connection weight    background                                           inputs given algorithms forcing certain                                                        elements stay given state example explicitly  sparse distributed memories sdms developed deﬁning input units ﬁxed values algorithms  der model longterm memory kanerva  undirected nature obtaining minimum energy state  capacity memories retrieve noisefree solution requires iterating unit states energy  version input vector autoassociation retrieve stagnates unfortunately process slow learning  associated output vector heteroassociation opposed model computationally expensive  types associative memories sdms require restricted boltzmann machines recently used  iterative process order converge desired answer achieve good results hinton et al   respect resemble feedforward neural networks  modelled sdms used pre  diction example robot navigation rao fuentes  framework                                                  unfortunately sdms suffer big disadvantage    sdm  divided parts hard locations nature deterministically noisecorrecting  output words hard location simply binary vector allow predict events occur discuss  length input vector each hard location proposed framework make assumptions  corresponds output word composed integer regarding environment interested modelling  valued vector possibly different length          deﬁne terms let deﬁne percept    vector given input memory distance realvalued vector representing observation secondly  di each hard location computed original imple assume association onetoone basis  mentation di simply hamming distance output given percept associated  obtained following way each cepts different strengths association each  hard location distance di ≤ δ δ threshold formalize proposing input percept maps  value corresponding word added output sum distribution associated percepts  actual output thresholding sum deﬁne memory set cells each  oi si   oi              cells acts hard location single    learning sdms fairly straightforward wish cept associated denote ci through nota  map input target word determine hard tional abuse each cell saliency value si associated  locations activated di ≤ δbyv each vector output weights wi wi represents  add corresponding output word             directed associations cells wi    hard locations orthogonal obtain size general denote weight matrix  close linear approximator systems saliency vector corresponding  studied extensively example associative search net presented input percept pit  works barto et al  hard locations computes activation vector α similar activation  thogonal memory potentially gains general sdms α realvalued vector elements  ization capacity producing complex combinations taking values   αi indicates  mappings various works sdms experimented perfect match ciand αi indicates  extending handle real values computing ca match usually like αi  work  pacity deciding craft hard locations bet use simplest activation function αi  ter suit given task presented sdms ci  αi effect assuming  ing propose intuitive coverage input space percepts actually symbolic subject noise  through actual potential inputs work idea rein need case general  forcement learning ratitch precup  sdms allow association based  heteroassociative capacity makes promising past observations circumvent use saliency                                                    ijcai                                                     value cells activation trace formally   initialize hard locations  time step set saliency vector                          ←                       ← γs  α                                   each episode                                                                    ←     similar cumulative eligibility trace rein    repeat  forcement learning sutton  γ decay factor       compute π  restricted form type encoding used       observe percept  prediction bose et al  furber et al       update based π − αp  αi exactly percept  rep          ← γs  αp  resents past sequence observations provided  cept observed twice note purposes table  contextbased prediction algorithm  does need exponentially decaying  trace observations goal serve context obtain                                                        cell wij represents strongly inﬂuences  better predictions section  discuss possible     way improving equation                  prediction of                                                                                   σ     eτβi              β            algorithm diverges sdms attempts predict let      recall    ki                                                        note  percepts match hard locations separating                                            output words deﬁne secondary activation ∂          ∂   eτβk   τeτβk     ∂                                                              π                         σ      β − eτβi            β                                                                                         denoted  vector representing prediction weight ∂wij     ∂wij  σ      σ      ∂wij  each cell interested predicting                                                                                 τs π   − π      percepts represented cells observed assume                     hard locations represent actual percepts       −τsjπkπi        compute β                       β                           let   vector errors   πi − αi                                                        equation obtain  equation notice weight matrix                                                                       deed acts set associations experiencing percept leads ∂               ∂                                                                     πk − αk      πk  related percepts predicted                  ∂wij                    ∂wij    values β correct                              prediction order signiﬁcantly different function      τsjπi − πiπi − αi −  πiπkπk − αk  β preserves ordering results valid proba                              ki  bility distribution used predict time step                                                                                          τs π  − π   −   π      chose use simple boltzmann distribution using β                       given                                                                           ki                               τβ                                                                τsjπi − π ·                     cis                                eτβi                                                      modify output weights through standard  distribution’s entropy controlled τ standard tem update rule learning rate ∈    perature parameter  ∞ experiments  used τ values yield better results                                  ∂                                                                       wij ← wij −                                                                                         ∂wij    learning                                                          usually probabilityproducing systems trained using  having discussed algorithm predicts events likelihoodbased gradient rea  learning accomplished log sons prefer use sum squared errors                                    ically use saliency vector contextual hints compute gradient explicitly allows train  allow make better prediction modify output combination hard locations through                  weights based                                     α vector interesting percepts acti    assume known set hard vate three hard locations noise              pt                            locations let percept observed time  sim interested good generalization capabilities experiments       ct                                  st  ilarly  set cells weight matrix  used maximum likelihood gave worse results                                          pt   αpt  saliency vector denote activation  believe fact ’ground                                pt                   assuming want predict future truth’ distribution approximating instead                          present modify produce probability dis constructing appropriate distribution through associa                  αpt            π  tribution similar  formally let probability tion                                         distribution  deﬁne prediction error     second learning problem ignore                                                π − α                     hard locations pre                                                   deﬁned ﬁxed way learning recognize percept                                                      just hard prediction discuss issue                th                 αi component αp  compute section  algorithm presented table                                            th  gradient error respect wij  weight                                                     ijcai                                                           episodes                                   episodes                                                   context                                                                                                context                                                                                                                      table  predicted observation based order past obser                                                        vations  table  predicted observation frequencies based num  ber training episodes                                    episodes                                                                          sequence pairs                                                                                     examples                                                                      section examples order show                      proposed algorithm perform prediction simi                    lar fashion strict markovian model                 restricted ﬁxed history length states ex              amples use sequences numbers observations each  number encoded nsized vector corresponding                  bit set  set  maximum number              observations note natural task percept vec                tor represent sensory input structure              welldeﬁned simplify matters assume agent  receives percept  ﬁrst bit set  begin table  predicted observation based longterm context  ning each episode receives during episode values italic show actual observation pre  similar deﬁning special start symbol learn dicted highest probability  ing kth order markov model expect  little impact learns continuously                                                        predicted probabilities decrease training  using learning rate  gave sufﬁciently                                                        increases interested predicting end  stable results decay factor γ arbitrarily set                                                        symbol increase τ obtain higher probabilities                                                          choice parameters initial symbol      ﬁrst experiment simplest shows predicted occur roughly  chance ﬁrst  approximate symbol frequencies simply experiment  produce separate oneobservation episodes certain                                                          set sequences looked shows  frequencies episodes produces                                                         learn use context point past  produces  order avoid variance results                                                        differentiate predictions speciﬁcally  chose use ﬁxed sequence episodes                                                        target symbols   predicted based  start  end  episodes containing ’s experi                                                        ﬁrst symbol occurs    enced regular intervals inbetween estimated frequencies                                                        training strings predicted probability correct  given table  show probability each                                                        symbol reported table   event certain number training episodes actual fre                                                          seen table follows slow  quencies given lefthand side note given                                                        degradation predicting correct event differenti  probabilities sum boltzmann distribu                                                        ating observation remote fact   tion assigns nonzero probability events                                                        systematically predicted higher probability    predicted unlikely impossible                                                        artefact experiment sequence containing  sufﬁcient training estimates converge                                                         presented sequence containing   true values learning rate prevents obtaining                                                        teresting note longest example given  exact probabilities output weights oscillate                                       −                                                        saliency context percept time prediction    episodes                                                        seen number iterations increases    second experiment aimed showing sys algorithm learns correctly distinguish  tem despite having explicit indication ordering events  fact discriminate based recency example strings  use respectively   goal show  predict high probability end  discussion  symbols appear based ordering results given framework presented provide way  table                                               predicting events perfectly accurate sub    clearly number training iterations increases ject history length constraints does require explicit  able precisely predict true sym state knowledge chief fact algo  bol based context symbols    rithm shown handle temporal ordering                                                    ijcai                                                     key predictions han happens seen failure frame  dle predicting observations unordered sequences work suited environments  brief example imagine environment ob observations consider  tain clues ’state’ world represented algorithm builds causality link percept  separate percept possibly occurs problem solved following way  ’state’ markov models relying strict order repeated occurrence percept does provide additional  observations need samples produce clear pre information instead question duration  dictions algorithm hand infer observation framework implicitly handles duration  weak clues general percept               allow nonzero output weights cell    technical nature saliency vector self output weights context  weight updating scheme ﬂexible modiﬁed larger order accurately predict event occurs  ﬁt different situations example experimented longer period time events  modifying saliency vector relevant percepts predicted relevant contextual information  context remove quickly irrelevant ones past  easily considering gradient error                                                          purposefully left recognition problem  current prediction respect saliency each cell                                                        presentation algorithm constructing suitable hard lo  update rule similar used output                                                        cations simple framework im  weights weight updating scheme improved                                                        plicitly proposes different approach recognition set  preventing negative gradients currently weight                                                        temporally spatially related percepts associ  date rule reduces probability events did                                                        ated form single abstract object  occur αi  time increases probability                                                        case vision example multiple  actual observation hinder later learning                                                        poses mapped object  purely additive approach closer frequencybased model  perform better                                   similar fashion algorithm restricted sin    obviously able predict events accu gle modality extended include multiple modal  rately sufﬁcient agent need correct ities associated separate pre  trol reinforcement learning point view nat diction each modality context constituted  urally incorporate reward framework three speciﬁc modalities capacity opens opportunities  ways explicitly attempt assign value each building knowledge through associations modali  observation compute predicted expected value based ties striking possibility link  context similar idea developed ratitch ceptual knowledge actions represented  precup  algorithm did explicitly model associative framework proposed  observation prediction fully observable  framework modify weight update rule use discussion suggests use actual percepts  magnitude rewards focus learning important parts abstraction strong contextual clues abstract percept  interestingly modify activates instances example idea  saliency vector based reward information percept tree simply cartoon tree maps  known associated high reward penalty kinds trees interesting  kept context longer                      sidering abstract object regular percept allows                                                        manipulate atomically regard abstract na    case truly stochastic event similar                                                        ture having atomic abstractions turn allows abstract  presented ﬁrst experiment section  predic                                                        gives capacity build hierarchical knowl  tion deﬁnition accurate                                                        edge additional apparatus  sense drawback algorithm transitions expected  fully determined past event ways past research association prediction mainly fo  address problem assume event cused obtaining averages noisy observations nov  truly stochastic nature stochasticity ap elty approach comes fact em  pears through lack contextual information case phasis importance associations incomplete  try inferring missing past events increasing observations framework takes radically differ  saliency percepts usually cause current observa ent view perceptions noise overcome through  tion approach explicitly consider association noisy signals associative memories dis  variance prediction qualitatively large output weights cussed share fault geared pro  suggest strong evidence prediction ducing ideal output hope framework  event strongly activated context reason build possibly noisy associations  believe stochasticity appear observations combined yield correct answer    question largely ignored far pa produce probability distribution  handling percepts occur associations need able use events  short interval time algorithm way time past algorithm achieving  specifying observation occurred twice lose promising predictive knowledgebuilding pur  capacity kth order model make separate prediction poses                                                    ijcai                                                     
