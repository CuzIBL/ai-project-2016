        building portable options skill transfer reinforcement learning                                    george konidaris     andrew barto                    autonomous learning laboratory department science                                  university massachusetts amherst                                        gdk bartocsumassedu                        abstract                             background      options framework provides methods rein    options framework      forcement learning agents build new highlevel      skills options usually learned option consists three components      state space problem agent      solving used tasks              πo   →        similar different state spaces              io        →       troduce notion learning options agent                 βo        →        space space generated feature set                                                        πo option policy probability distribution      present retains semantics suc                                                        actions each state option deﬁned io      cessive problem instances problem initiation set indicator function  states      space agentspace options reused later                                                        option executed  βo      tasks share agentspace dif termination condition giving probability option      ferent problemspaces present experimental    terminating each state sutton et al  options      results demonstrating use agentspace op  framework provides methods learning planning using      tions building transferrable skills show options temporally extended actions standard rein      perform best used conjunction   forcement learning framework sutton  barto       problemspace options                                                          algorithms learning new options include                                                        method determining create option expand    introduction                                       initiation set deﬁne termination condition  recent research reinforcement learning focused learn policy policy learning usually performed  hierarchical methods barto  mahadevan  offpolicy reinforcement learning algorithm  particular options framework sutton et al agent update option simultaneously taking  integrates macroaction learning reinforcement learn action sutton et al   ing framework option learning methods work creation termination usually performed  state space problem agent solving identiﬁcation goal states option created reach  time lead faster learning later tasks goal state terminate does initiation set  state space learned options useful set states goal reachable previ  reused later tasks related ous research selected goal states varierty methods  distinct state spaces                                variable change frequency hengst  local graph    propose learning portable options using sepa partitioning s¸ims¸ek et al  salience singh et al  rate representations representation problemspace  research focused extracting options  markov particular task hand agentspace exploiting commonalities collections policies sin  markov retained successive task gle state space thrun  schwartz  bernstein   instances each require new problemspace perkins  precup  pickett  barto   possibly different size dimensionality konidaris  methods learn options state space  barto  options learned agentspace reused agent performing reinforcement learning  future problemspaces semantics agentspace reused problem new  remain consistent tasks                       problem space available state abstraction    present results experiment showing methods jonsson  barto  hengst  allow  learned agentspace options signiﬁcantly improve perfor automatic selection subset space option  mance sequence related distinct tasks learning require explicit transformation space  best used conjunction problemspeciﬁc options ravindran  barto                                                     ijcai                                                                                                                     sequences tasks agentspace               di problemspace state descriptor sufﬁcient                                                                                            cj  concerned agent required solve se distinguish state agent                                                                             quence related distinct tasks deﬁned follows space descriptor ri reward obtained state  agent experiences sequence environments generated goal reinforcement learning each task sj ﬁnd  underlying world model policy πj maximizes reward  physics types objects present en agent given learns set higher  vironments sensations receives each en level options reduce time required solve task  vironment agent creates representations ﬁrst options deﬁned using portable tasks  state descriptor sufﬁcient distinguish markov states cause form meaning problemspace  current environment induces markov decision scriptor change task  process mdp ﬁxed set actions agent form meaning agentspace descriptor does  does change set states transition probabilities deﬁne agentspace option components  reward function depend environment          π cja    →    agent currently agent works different state                                                                                  cj      →   space transition probabilities reward func                                                                                   β    cj     →    tion each task sequence each state                        spaces problemspace                               agent learning task option policies    agent uses second representation based different spaces types policies updated  features consistently present retain se simultaneously agent receives agentspace  mantics tasks space shared tasks problemspace descriptors each state  sequence agentspacethetwo  spaces stem different representational requirements  experiments  problemspace models markov description particular  task agentspace models potentially nonmarkov  lightworld domain  commonalities sequence tasks consider agent placed environment consisting sequence  sequence tasks related share agentspace rooms each room containing locked door lock    approach distinct taken prior reinforce possibly key order leave room agent  ment learning research ﬁnding useful macroactions unlock door step through order unlock  sequences tasks bernstein  perkins  precup  door lock press key  pickett  barto  thrun  schwartz wherethe present room agent holding success  tasks state space different fully unlock door agent obtain key moving  reward functions appropriate sequence tasks picking agent receives reward  deﬁnition requires agentspace semantics remain  leaving door ﬁnal room step penalty  consistent each task completely distinct − each action actions available movement  state space konidaris barto  used each grid directions pickup action press  distinction learn shaping functions agentspace speed action environments deterministic unsuccessful  learning sequence tasks               actions moving wall result change state    simple example appropriate sequence tasks equip agent light sensors grouped  sequence buildings each robot equipped threes each sides ﬁrst sensor each triplet  laser range ﬁnder required reach target room detects red light second green blue each  laserrange ﬁnder readings noisy non sensor responds light sources side agent rang  markov robot likely build metric map ing reading  light source  building explores forming buildingspeciﬁc prob   squares away open doors emit red light  lem space range ﬁnder readings form keys ﬂoor held agent emit green  agent space meaning consistent light locks emit blue light figure  shows example  buildings robot eventually learn options pieces data form problemspace descriptor  agentspace corresponding macroactions like moving lightworld instance current room number  nearest door options based solely ordinates agent room agent  sensations agent space referencing problemspace key door open use  individual metric map used speed light sensor readings agentspace seman  learning building robot encounters later tics consistent lightworld instances case                                                        agentspace  continuous variables higher dimen    options agentspace                             sion individual problemspace  consider agent solving problems state spaces types agent   sn action space aweviewtheith state task used ﬁve types reinforcement learning agents agents  sj consisting following attributes         options agents problemspace options agents                                                        perfect problemspace options agents agentspace                                             si di ci ri                   options agents option types                                                    ijcai                                                                                                           functions solve underlying task instance                                                        agentspace descriptors markov room light                                                        worlds present experiments                                                        experimental structure                                                        generated  random lightworlds each consisting                                                          rooms width height  cells door                                                        lock randomly placed each room boundary                                                                                                                     rooms included randomly placed key                                                        sulted state space  approximately                                                          stateaction pairs  average evaluated                                                        each problemspace option agent type  lightworlds                                                         samples each generated lightworld                                                          evaluate performance agentspace options                                                        agents gained experience similarly obtained            figure  small example lightworld        samples each sample ran agents                                                        training  training experiences                                                        each training experience sample lightworld consisted                                      λ          agents options used sarsa  greedy  episodes training lightworld randomly selected                α    γ    λ       action selection                                 remaining  agents updated  learn solution policy problemspace each state options during evaluation sample lightworld                                    action pair assigned initial value             dates discarded training experience    agents problemspace options initially agentspace options received prior training  learned option each prespeciﬁed salient event picking evaluation lightworld  each key unlocking each lock walking through each  door options learned problemspace used results  parameters agent options used figure  shows average learning curves agents employ  policy tracebased treebackup updates precup et al  ing problemspace options figure  shows  intraoption learning options got reward  agents employing agentspace options ﬁrst time  completed successfully used discount factor  agentspace option agent encounters lightworld performs  action taken room similarly agent options evidenced  deﬁned states value function ex topmost learning curves each ﬁgure performance  ceeds minimum threshold  options rapidly improves experience lightworlds af  learned problemspace useful ter experiencing single training lightworld agent  learned each individual lightworld               shallower learning curve agent using problem    agents perfect problemspace options given pre space options  experiences learning curve  learned options each salient event performed similar agent perfect problemspace op  option updates identical standard tions compare bottommost learning curve figure  agent options represented agents perfectly  options trained light  transferrable fully learned options                  world tested comparison figures    agents agentspace options learned solution   shows agentspace options successfully  policies problemspace learned option policies transferred lightworld instances  agentspace each agent employed three options figure  shows average learning curves agents em  picking key going through open door ploying types options ﬁrst time agents  unlocking door each one’s policy function encounter lightworld perform agents  light sensors sensor outputs ing problemspace options compare second high  tinuous used linear function approximation each op est curve figure  rapidly improve  tion’s value function performing updates using gradient performance performing better agents using agent  scent α  offpolicy tracebased treebackup space options  experiences performing nearly  dates agent gave each option reward  com agents perfect options conjecture  pletion used step penalty  discount factor improvement results factors agent   option taken particular state space larger individual problemspace  value function exceeded minimum threshold  problemspace options easier learn scratch  options learned agentspace                                                             transferred lightworld instances                   episodes used testing                                                        agents types options agentspace value function ap    finally agents types options included proximator diverged restarted episode  represent agents learn general portable speciﬁc known problem backup method used precup et al  nonportable skills simultaneously                    did occur during number samples    note agents used discrete problemspace value obtained agents agentspace options                                                    ijcai                                                                                                                                                                                                                          perfect options                                  learned options                                                                                                  options                                                                                                                                                                                                                                                         actions                                                                                                                                                                                                                                                                                                                lo po                                                                               episodes                                                                                    figure  learning curves agents problemspace op figure  total steps episodes agents  tions                                                options learned problemspace options lo perfect                                                        options po agentspace options  training experi                                                                          experiences        ences dark bars option types  training                                    experience                                experiences        experiences light bars                                    experiences                                    experiences                                                                   episodes agents using options problemspace               actions                                  options perfect options agentspace options option                                                        types experience training environments rapidly drops                                                    number total steps required nearly low num                                                    ber required agent perfect options clearly                                                         shows agents using types options consistently                                                       episodes                      better using agentspace options note                                                        error bars figure  small decrease                                                        experience indicating consistent transfer  figure  learning curves agents agentspace op  tions varying numbers training experiences    conveyor belt domain                                                experiences                                    experience         conveyor belt set objects row                                experiences                                    experiences        feeders row bins types objects                                    experiences                                                    triangles squares each bin starts capacity                                                        each type objects issued time feeder                                                    directed bin dropping object bin           actions                                  positive capacity type decrements capacity                                                        each feeder directly connected opposing bin                                                        through conveyor belt connected belts                                                    pair ﬁxed points length                                                       run conveyor belt moves                          episodes                      current object step belt try                                                        moves object connection  figure  learning curves agents agentspace point each action results penalty −  problemspace options varying numbers training ex causes object dropped bin spare capacity  periences                                            case results reward asmallexample                                                        conveyor belt shown figure                                                                              agentspace options explains agents  ing agentspace options training experiences                 perform like agents options like agents              problemspace options second options learned  problemspace represent exact solutions speciﬁc sub  goals options learned agentspace gen figure  small example conveyor belt problem  eral approximated likely  slightly efﬁcient speciﬁc subgoal explains each camera tracks current object  agents using types options perform better returns values indicating distance  units  long run agents using agentspace options  bin each connector current belt    figure  shows mean total number steps required space generated camera present conveyor                                                    ijcai                                                     belt problem retains semantics agent                                  space discrete relatively small    states learn policies function approxima       tion nonmarkov limited     −  range inability distinguish belts  used problemspace                                        −    problemspace descriptor conveyor belt instance      reward                                                                  −  consists three numbers current object number                            learned options                                                                                       perfect options                                                                  −  belt far belt lies technically                       options  include current capacity each bin                                                                  −    omit obtain good policies generated                                                                                                     episodes   random instances  objects  belts each  length  randomly selected interconnections  sulting problemspaces    states  figure  learning curves agents problemspace op    ran experiments agents learned three options tions  current object bin end belt                               currently moving belt     moving belt used  agent types experimental structure   −  agentspace options did use function approximation                                                                  −                                                                  reward    results                                                                                 experiences                                                                  −                   experience  figures    show learning curves agents employing                            experiences                                                                                          experiences                                                                  −                   experiences  options problemspace options perfect options agents                           experiences  employing agentspace options agents employing types                                                                  −                                                                                      options respectively                                                      episodes    figure  shows agents agentspace options  prior experience initially improve quickly eventu  ally obtain lower quality solutions agents problem figure  learning curves agents agentspace op  space options figure  training experiences tions varying numbers training experiences  sult roughly curve agents using problemspace                                 options  training experiences agentspace options  signiﬁcant improvement limited           range good perfect options initial   −  dip probably limited range agentspace op                                                                  −      tions limited range camera fact reward                                                                                           experiences  locally markov subgoals      −                    experience                                                                                           experiences    figure  shows agents option types                               experiences                                                                                           experiences  experience initial dip outperform problemspace op   −                    experiences  tions immediately likely agentspace op                                                                  −                                                                                      tions able generalise belts figure  shows                     episodes  mean total reward each type agent agents  ing agentspace options eventually outperform agents using                                                        figure  learning curves agents types op  problemspace options agentspace op                                                        tions varying numbers training experiences  tions limited range agents using types  options consistently outperform agents option  type eventually approach performance agents using  prelearned problemspace options                    closely related difference allocentric                                                        egocentric representations space guazzelli et al                                                           expect learning option agentspace    discussion                                         actually harder solving individual problem  concept agentcentric representation closely space instance case experiments  lated notion deictic egocentric representations situations learning types options simultaneously  agre  chapman  objects represented likely improve performance intraoption learning  point view agent global frame methods allow update options  reference expect problems especially experiences better general simultane  robotics agentspace representations egocentric ex ously learn general portable skills speciﬁc exact  cept manipulation tasks likely object nonportable skills allow bootstrap each  centric problems involving spatial maps expect addition agentspace descriptors reinforce  difference problemspace agentspace ment learning framework introduces design problem similar                                                    ijcai                                                     
