        topological value iteration algorithm markov decision processes                                        peng dai    judy goldsmith                                          science dept                                           university kentucky                                             anderson tower                                        lexington ky                         abstract                          recently types algorithms proposed                                                        efﬁciently solve mdps ﬁrst type uses reachabil      value iteration inefﬁcient algorithm    ity information heuristic functions omit unnec      markov decision processes mdps puts  essary backups rtdp barto bradke  singh      majority effort backing en laohansen  zilberstein lrtdpbonet      tire state space turns unnecessary  geffner hdp bonet  geffner athe      cases order overcome prob    second uses approximation methods simplify      lem approaches proposed    problems guestrin et al  poupart et al       lao lrtdp hdp stateofthe       patrascu et al  aggregates groups states      art ones use reachability analysis  mdp features represents factored mdps      heuristics avoid unnecessary backups solves factored mdps factored mdps ex      approaches fully exploit ponentially simpler strategies solve tricky      graphical features mdps use fea  spudd  hoey et al slaofeng  hansen       tures yield best backup sequence state srtdp feng hansen  zilberstein  examples      space introduce algorithm named topolog   use prioritization decrease number inefﬁ      ical value iteration tvi circumvent cient backups focused dynamic programming ferguson       problem unnecessary backups detecting   stentz  prioritized policy iteration mcmahan       structure mdps backing states based  gordon  recent examples      topological sequences prove backup     propose improvement value iteration algo      sequence tvi applies optimal experimen   rithm named topological value iteration combines      tal results show tvi outperforms vi lao   ﬁrst technique algorithm makes use graph      lrtdp hdp benchmark mdps              ical features mdps does backups best order                                                        necessary addition soundness op    introduction                                       timality algorithm ﬂexible independent  statespace search common problem ai planning assumptions start state ﬁnd opti  similar graph search given set states set mal value functions entire state space easily  actions start state set goal states problem tuned perform reachability analysis avoid backups  ﬁnd policy mapping states actions starts irrelevant states topological value iteration  start state ﬁnally arrives goal state heuristic algorithm efﬁciently make use extant  cision theoretic planning boutilier dean  hanks  heuristic functions initialize value functions  attractive extension classical ai planning paradigm  allows model problems actions  background  uncertain cyclic effects uncertainty embodied section basics markov decision pro  event leads different outcomes oc cesses extant solvers  currence outcomes unpredictable  guided form predeﬁned statistics sys  mdps dynamic programming solvers  tems cyclic event leave state mdp fourtuple set states  unchanged return visited state        given time consider    markov decision process mdp model represent developing sequence discrete time slots  ing decision theoretic planning problems value iteration stages each time slot event allowed  policy iteration howard  fundamental dy effect stage each state associated set                                                                           namic programming algorithms solving mdps   applicable actions effect applying action  algorithms inefﬁcient make change current state  spend time backing states redundantly state stage transition function each action ta                                                     ijcai                                                    × →   speciﬁes probability changing state  solvers  s                   →    applying state        instant reward                                                              cc                             barto et al barto bradke  singh  proposed  formulation use  instant cost instead online mdp solver named real time dynamic programming  value function   → gives maximum                                                      algorithm assumes initially algorithm knows  value total expected reward state  information start  horizon mdp total number stages sys state goal states simulates evolution  tem evolves problems horizon is ﬁnite number number trials each trial starts start                                  csi     aim minimize value           state ends goal state each step trial  steps inﬁnitehorizon problems reward accu greedy action selected based current knowledge  mulated inﬁnitely long path deﬁne values state changed stochastically during trial  inﬁnitehorizon problem introduce discount factor visited states backed algorithm succeeds  γ ∈   each accumulated reward case goal certain number trials ﬁnished                ∞  γicsi  minimize                                     lao  hansen  zilberstein  solver    given mdp deﬁne policy π  →   uses heuristic functions basic idea expand ex  mapping states actions optimal policy tells plicit graph g iteratively based type bestﬁrst  choose actions different states order maximize strategy heuristic functions used guide state  expected reward bellman bellman  showed expanded time new state expanded  expected value policy π computed using set cestor states backed iteratively using value iteration                  π                        πs  value functions  ﬁnitehorizon mdps      lao heuristic algorithm uses mean ﬁrst pas  deﬁned cs deﬁne π according π                                                 sage heuristic lao converges faster rtdp          π                            π              expands states instead actions        vtscs       tπss svt                          s∈s                              advantage rtdp ﬁnd good sub                                                        optimal policy pretty fast convergence rtdp  inﬁnitehorizon mdps optimal value function slow bonet geffner extended rtdp labeled rtdp  ﬁned                                                                                                                                lrtdp  bonet  geffner  convergence  smina∈ascsγ       tas sv γ ∈   lrtdp faster approach mark state                           s∈s                         solved bellman residuals states                                                     reachable through optimal policy small  equations named bellman equations  state solved regard value function  based bellman equations use dynamic program converged treated “tip state” graph lrtdp  ming techniques compute exact value value func converges start state solved  tions optimal policy easily extracted choosing hdp stateoftheart algorithm bonet  action each state contributes value function geffner bonet  geffner hdp uses    value iteration dynamic programming algorithm similar labeling technique lrtdp discovers  solves mdps basic idea iteratively update value connected components solution graph mdp hdp  functions state converge each iteration labels component solved states com  value function updated according equation  ponent labeled hdp expands updates states  update bellman backupthebellman resid depthﬁrst fashion rooted start states  ual state deﬁned difference states belonging solved components regarded tip  value functions consecutive iterations bell states experiments show hdp dominated lao  man error deﬁned maximum bellman residual lrtdp racetrack mdp benchmarks  state space bellman error heuristic function hmin bonet  geffner used  threshold value conclude value functions algorithms make use start state informa  converged sufﬁciently policy iteration howard  tion constraining number backups states  approach solve inﬁnitehorizon mdps consisting unreachable start state backed  interleaved steps policy evaluation policy improve make use heuristic functions guide search  ment algorithm stops policy improvement promising branches  phase changes algorithms suffer ef  ﬁciency problems each iteration each algorithm  bound polynomially number states number  limitation current solvers  iterations puterman                    algorithms listed make use inherent fea    main drawback algorithm each tures mdps study sequence state  eration value functions state updated ups according mdp’s graphical structure  highly unnecessary firstly states backed intrinsic property mdp potentially decides com  fore successor states type backup plexity solving littman dean  kaelbling  fruitless show example section  secondly example figure  shows simpliﬁed version mdp  different states converge different rates simplicity omit explicit action nodes transition probabil  states converged need ities reward functions goal state marked  subset state space iteration      ﬁgure directed edge states means second                                                     ijcai                                                    state potential successor state applying action removing action nodes changing paths like  ﬁrst state                                    → →  s directed edges s causal                                                        relation graph gcr original mdp path state                                                        gcr means causally dependent ssothe                                                        problem ﬁnding mutually causally related groups states                                goal        reduced problem ﬁnding strongly connected                                                        components gcr                                                          use kosaraju’s cormen et al  algorithm                                                        tecting topological order strongly connected compo               figure  simpliﬁed mdp                nents directed graph note bonet geffner bonet                                                         geffner used tarjan’s algorithm detection    observing mdp figure  know best se strongly connected components directed graph  quence states ssss apply solver use topological order  sequence states require components systematically each component  backup efforts algorithms mdp kosaraju’s algorithm simple implement  mentioned detect optimal backup time complexity linear number states  sequence moment start mdp state space large overhead ordering  look solving common graph search problem state backup sequence acceptable experimental   vertices apply essentially strategies sults demonstrate overhead compensated  solving mdp graphical structure equivalent computational gain  clique mdp simpler solve pseudocode tvi shown figure  ﬁrst  clique mdp basic strategies solvers use kosaraju’s algorithm ﬁnd set strongly connected  “intelligent” subroutine distinguish various mdps components graph gcr topological order note  use different strategies solve intu each ∈ maps set states mwethenuse  ition want design algorithm able discover value iteration solve each cycles  intrinsic complexity various mdps studying components need solve notice  graphical structure use different backup strategies entire state space causally related tvi  mdps different graphical properties             equivalent vi    topological value iteration                        theorem   topological value iteration guaranteed                                                        converge optimal value function  ﬁrst observation states value functions  causally related mdp state s succes proof ﬁrst prove tvi guaranteed terminate ﬁnite  sor state applying action athenv dependent time each mdp contains ﬁnite number states  s reason want s ahead contains ﬁnite number connected components solving  causal relation transitive mdps cyclic each components tvi uses value iteration  causal relations common states value iteration guaranteed converge ﬁnite time tvi  ﬁnd optimal backup sequence states idea actually ﬁnite number value iterations termi  following group states mutually causally nates ﬁnite time prove tvi guaranteed  related make metastate let verge optimal value function according update  metastates form new mdp thenm    longer  sequence tvi point algorithm value func  cyclic case states  tions states component backed  verse topological order words depend value functions components  big states virtual iteration backed components  big states originally sets states apply backed reason tvi lets value  strategy value iteration policy iteration linear functions state space converge sequentially  programming ﬁnd mutually  component converged value functions states  causally related states                              safely used tip states inﬂuenced    answer question let look graphical components backed later                                  structure mdp ﬁrst mdp  regarded  straightforward corollary theorem  directed graph gvethesetv state nodes  each node represents state action nodes corollary  topological value iteration updates  each action mdp mapped vertex value functions component necessary  edges eing represent transitions indicate update sequence update optimal  causal relations edge state node  node means candidate action state  optimization  conversely edge pointing s means applying implementation added optimizations al  action positive probability changing gorithm reachability analysis tvi does assume  state s ﬁnd path → → s know initial state information given informa  state causally dependent s simplify tion tvi able detect unreachable components                                                     ijcai                                                                  topological value iteration                experiment   tvimdp   δ   sccm                                             tested topological value iteration compared   fori ←  cpntnum                               running time value iteration vi lao lrtdp     ← set states sid  cpntnum     hdp algorithms coded properly opti     vis δ                                         mized run intel pentium  ghz proces   vis set states δ                            sor main memory cache size kb     true                                     operating linux version  compiler     each state ∈                            gcc version                                                       smina∈ascsγ     s∈s tas sv      bellman error δ                  domains      return                                          use mdp domains experiments ﬁrst   sccmdp  kosaraju’s algorithm                   domain model simulating phd qualifying exams    construct gcr removing action nodes    consider following scenario ﬁctional department                             g       construct reverse graph cr cr             qualiﬁed phd science    size ← number states gcr                   pass exams each cs area months depart    ←  size                                ment offers exams each area each student takes each    sid ←−                                         exam wants passes each time     postr posti arrays length size exams consider types    cnt ← cpntnum←                                grading criteria ﬁrst criterion pass    ←  size                                fail course untaken each exam students    sid  −                                                       taken failed certain exam     df                                     fore chance passing exam sec    ←  size                                ond criterion little trickier assign pass conditional    postrs ← postis                              pass fail each exam probabilities passing    cnt ← cpntnum←                                certain exams vary depending student’s past grade    ←  size                                exam state domain value assignment    sid ←−                                         grades exams example ﬁve exams    ←  size                                failpasspasscondpassuntaken state refer         sid  −                                                  ﬁrst criterion mdps qese second qetewhere     df gcrpostrs                             refers number exams        cpntnum  ←  cpntnum                                                        second domain use artiﬁciallygenerated “lay            cpntnum      return          cr                               ered” mdps each mdp deﬁne number states               dfsgraph                                        partition evenly number nl layerswe      sid ← cpntnum                                                     number layers numerical values allow states                      s     each successor                            higher numbered layers successor states states         sid  −                                                  lower numbered layers vice versa each state        df s                                                     limited set allowable successor states succsthe      posticnt ←                                                     parameters mdps maximum number      cnt ← cnt                                                      actions each state ma maximum number                                                                                                   figure  pseudocode topological value iteration successor states each action given state weletthe                                                        pseudorandom number generator pick number ac  ignore dynamic programming step reachability tions ma each action let action  computed depth ﬁrst search overhead anal number successor states ms states chosen  ysis linear helps avoid considering unreach uniformly succs normalized transition  able components gains compensate probabilities advantage generating mdps way  trouble introduced extremely useful small layered mdps contain nl connected com  portion state space reachable reachabil ponents  ity analysis straightforward provide pseu actual applications lead multilayered  docode optimization use heuristic mdps simple example game bejeweled each level  functions heuristic values serve good starting point layer consider chess variant  value functions tvi program use hmin pawns played stochastic opponent each set  heuristic bonet  geffner reachability anal pieces appear board leads  ysis use heuristics help strengthen competitive stronglyconnected component  ness tvi hmin replaces expected future reward examples know multilayered  bellman equation minimum value standard mdp benchmarks examples racetrack  admissible heuristic                                 mdps tend single scc rendering tvi better                                                       vi checking topological structure mdp takes      smin   csγ  · min             min                      tas        negligible time compared running solvers                                                     ijcai                                                      domain         qes     qes      qes     qes    qet    qet     qet     qet                                                                                                                                               ofsccs                                                             ∗                                hmin                                                                       vih                                                          laoh                                                       lrtdph                                                        hdph                                                 tvih                                                           vihmin                                                         laohmin                                                      lrtdphmin                                                       hdphmin                                                 tvihmin                                                           table  problem statistics convergence time cpu seconds different algorithms different heuristics qual  exams examples  −    easy decide use tvi use artiﬁ layer number increases mdps complex  cially generated mdps                        states large numbered layers relatively small                                                        succs ms cycles layers    results                                          common takes greater effort solve large numbered  consider variants ﬁrst domain layers small numbered ones surprisingly ta  sults shown table  statistics shown ble  number layers increases    • tvi outperforms rest algorithms running time each algorithm increases      stances generally fast convergence increase rate tvi smallest rate greatest      appropriate update sequence state space smallest running time tvi  versus  vi  lao      avoidance unnecessary updates                  lrtdp fact tvi applies                                                        best update sequence layer number large al    • hmin helps tvi helps vi lao                           qe                           update large numbered layers requires      lrtdp especially domains               effort time tvi spends small numbered ones    • tvi outperforms hdp way dealing mains stable algorithms property      components different hdp updates states second experiment ﬁx number layers      unsolved components depthﬁrst fashion vary state space size tvi better al      converge pick optimal sequence gorithms seen table  state space       backing components tvi solve problems  seconds shows      time algorithm does spend time checking tvi solve large problems reasonable      components solved time note statistics include represent      date component necessary            common cases chosen favor tvi best  notice hdp shows pretty slow convergence qe result shows tvi runs  seconds mdps  domain implementation hdp nl ma ms vi needs  suitable solving problems large numbers actions  seconds lao takes  seconds lrtdp  readers interested performance hdp mdps quires  seconds  smaller action sets refer bonet  geffner    statistics performance artiﬁcially generated conclusion  layered mdps shown table   include  hdp statistics hdp slow cases introduced analyzed mdp solver topolog  ignore results applying hmin heuristic ical value iteration studies dependence relation  display scale using heuristic value functions state space use dependence  each element table average running relation decide sequence states algo   instances mdps conﬁguration note rithm based idea different mdps different  varying nl maandms yields mdp conﬁgura   graphical structures graphical structure mdp  tions present results representative intrinsically determines complexity solving mdp    ﬁrst group data ﬁx state space notice current solvers detect information  size  change number layers statistics ta use guide state backups solve mdps  ble  show tvi dominates note problem sizes different graphical structure                                                     ijcai                                                    
