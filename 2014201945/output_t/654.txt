 stationary deterministic policies constrained mdps multiple rewards                                    costs discount factors                                   dmitri dolgov edmund durfee                        department electrical engineering science                                           university michigan                                           ann arbor mi                                        ddolgov durfeeumichedu                        abstract                            numerous domains classical                                                        mdp model proves inadequate dif      consider problem policy optimization    ﬁcult fold relevant feedback environment      resourcelimited agent multiple time  rewards agent receives costs incurs sin      dependent objectives represented mdp  gle scalar reward function particular agent’s actions      multiple discount factors objective function addition producing rewards incur costs      constraints show limiting search sta measured differently rewards making      tionary deterministic policies coupled novel hard impossible express scale      problem reduction mixed integer programming   example natural problem delivery agent maxi      yields algorithm ﬁnding policies mize aggregate reward making deliveries subject      computationally feasible algorithm straints total time spent en route problems naturally      heretofore identiﬁed simpler case modeled constrained mdps arise      constrained mdp single discount   mains example telecommunication applications      factor technique provides new way ﬁnd lazar  desirable maximize throughput      ing optimal deterministic policy previous subject delay constraints      methods ﬁnd randomized policies      analyze properties approach situation classical mdp model      implementation results                           expressive agent receives multiple                                                        ward streams incurs multiple costs each different                                                        discount factor example delivery agent face    introduction                                       rushhour situation rewards making deliver  markov decision processes bellman  provide sim ies decrease function time delivery action pro  ple elegant framework constructing optimal policies duces lower reward executed later time  agents stochastic environments classical mdp trafﬁc conditions improve time delivery ac  mulations usually maximize measure aggregate tion executed faster later time rewards  ward received agent instance widelyused dis crease trafﬁc conditions improve different time scales  counted mdps objective maximize expected problem naturally modeled discount fac  value sum exponentially discounted scalar rewards tors allowing agent evaluate tradeoffs  received agent mdps number early late delivery problems multiple discount fac  nice properties subject principle local op tors frequently arise domains example  timality according optimal action state agent involved ﬁnancial ventures dif  independent choice actions states ferent risk levels time scales model multiple  optimal policies mdps stationary deterministic discount factors allow decision maker quantita  depend initial state tively weigh tradeoffs shorter longerterm  properties translate efﬁcient dynamicprogramming investments feinberg shwartz  ex  algorithms constructing optimal policies mdps amples provide justiﬁcation constrained mod  puterman  policies easy imple els discount factors  ment standard agent architectures                   price pay extending classical model                                                        introducing constraints discount factors  material based work supported honeywell interna stationary deterministic policies longer guaranteed  tional darpaipto coordinators program                                     air force research laboratory contract fa––c– optimal feinberg shwartz    searching   views conclusions contained document optimal policy larger class nonstationary  authors interpreted representing randomized policies dramatically increase problem com  ofﬁcial policies expressed implied defense ad plexity fact complexity ﬁnding optimal policies  vanced research projects agency government broad class constrained mdps multiple costs rewards discount factors known solution results extended optimization criteria  algorithms exist aside special cases fein discussed section  policy said markovian  berg shwartz  furthermore historyindependent choice action does depend  nonstationary randomized policies history states actions encountered past  reliably executable basic agent architectures example current state time addition  paruchuri et al  described executing randomized policy does depend time called stationary  policies multiagent systems problematic    deﬁnition stationary policy markovian deter    paper focus ﬁnding optimal sta ministic policy prescribes execution  tionary deterministic policies mdps multiple action state randomized policy chooses actions  wards costs discount factors problem according probability distribution  studied proven npcomplete stationary randomized policy π described  feinberg  formulated nonlinear non mapping states probability distributions actions  convex mathematical program unfortunately aside π  × →   πia deﬁnes probability  intractable techniques general nonconvex optimization agent execute action encounters state  problems heretofore practically solvable deterministic policy viewed degenerate case    contribution paper present approach randomized policy action each  solving problem reduces mixedinteger lin state nonzero probability executed  ear program – formulation npcomplete policy π initial conditions α  →    available wide variety efﬁcient solution algo specify probability distribution state space time  rithms tools make practical ﬁnd optimal                                                         agent starts state probability αi  stationary deterministic policies show termine evolution total expected  approach fruitfully employed subclass discounted reward agent receive  mdps multiple costs single reward                  ∞  function discount factor problems linear pro                                                                                    uγ π α       γ ϕitπiaria        gramming polynomial time ﬁnd optimal stationary                   ia  randomized policies kallenberg  heyman sobel                                                        ϕ refers probability state time   problem ﬁnding optimal stationary deter                                                          γ ∈   discount factor  ministic policies npcomplete feinberg   implementable solution algorithms existing previously aside wellknown puterman   general nonlinear optimization techniques constrained mdp total expected discounted reward  show integerprogrammingbased approach ﬁnds op optimization criterion exists optimal policy                                                         ∗  timal stationary deterministic policies π stationary deterministic uniformlyoptimal  compared empirically optimal randomized policies  term means policy optimal                                                        initial probability distributions starting state    remainder paper ﬁrst section  es ∗  tablish baseline brieﬂy reviewing techniques solv uγ π  α ≥ uγ π α ∀π α  ing unconstrained mdps section  standard ways solving mdps  strained mdps present approach solving puterman  use dynamic programming  strained mdps single reward multiple costs value policy iteration better  discount factor rewards costs expand suited constrained problems reduce mdps linear pro  case multiple rewards costs dis grams lps discounted mdp formulated fol  count factors section  section  provides empirical lowing lp d’epenoux  kallenberg  max  evaluations observations section  discusses imization lp dual morecommonly used mini  sults thoughts applying techniques mization lp value function coordinates  ﬂavors constrained mdps                                                                                                                            xja − γ    xiapiaj  αj                                                                                                                                      max     riaxia          ia                                                                                                                                                           ia    background unconstrained mdps                                         xia ≥   unconstrained stationary discretetime fullyobservable set optimization variables xia called oc  mdp deﬁned tuple hs ri  cupation measure policy xia interpreted  ﬁnite set states agent  ﬁnite total expected discounted number times action                                                                              set actions agent execute  × × →   executed state xia gives total expected dis                                                                              stochastic  piaj   transition function piaj counted ﬂow through state constraints  probability agent goes state executes action lp interpreted conservation ﬂow through  state  × → bounded reward function each states optimal policy computed  agent gets reward executing action state solution lp                     ia                                                               solution mdp policy procedure selecting          πia  xia   xia                action state maximizes measure ag                       gregate reward paper focus mdps appears lead randomized policies  total expected discounted reward optimization criterion absence external constraints use strictly positiveinitial conditions αi   basic feasible solution limited nonconsumable resources reduced milp  lp maps deterministic policy uniformly following proposition provides basis reduction  optimal puterman  kallenberg  lp                                                         proposition  consider mdp hs αi policy π  unconstrained mdp serves basis solving                                                        corresponding occupation measure given α constant  strained mdps discuss                                                         ≥ xia ∀i ∈ ∈ set binary variables ∆ia                                                           ∀i ∈ ∈    constrained mdps                                     ∆ satisfy following conditions                                                                         suppose agent getting rewards executing            ∆ia ≤   ∀i ∈              actions incurs costs ck  × → ∈                        cia cost type incurred executing action   xiax ≤ ∆ia   ∀i ∈ ∈          state actions time consume energy states π α nonzero prob  case say types costs                                                                   ability visited  xia   π deterministic  natural problem pose maximize expected following holds  discounted reward subject upper bounds                                                                         ∆     ⇔                   tal expected discounted costs let label total expected           ia        ia  discounted cost type ∈                 proof consider state i∗ policy π initial dis                        ∞                               tribution α nonzero probability visited                                                         cγ π α        γ ϕitπiacia           xi∗a   occupation measure non                       ia                          negative action state  abstractly write optimization problem nonzero occupation measure                                                                        ∗  cost constraints                                                 ∃a  ∈  st xi∗a∗                                                   max uγ π α   cγ π α ≤ bc         forces ∆i∗a∗    forces zero               π                                                                    ∗                                                      values ∆’s state   bc upper bound cost type prob                       ∗  lem feasible exists optimal stationary        ∆i∗a    ∀a    policy computed solution following given  turn means occupation measure  lp kallenberg  heyman sobel         actions zero                                                                                        ∗                                                                     xi∗a   ∀a                           xja − γ   xiapiaj  αj                                                                                                    π                              ia                      translates fact policy                                                      terministic ∆ia   ⇔ xia          max                                             ia ia     ciaxia ≤                      proposition  immediately leads following milp                                            ia       ia                               solution yields optimal stationary deterministic poli                                           xia ≥                           cies                                                                                        constrained mdps type solved                  xja − γ    xiapiaj  αj                                                                             polynomial time adding constraints dis                       ia                                                                             count factor does increase complexity mdp                                                                                                        xia ≤   addition constraints problem                  ia                                                                            ia  general uniformlyoptimal policies max xiaria                                                                                                         lp  yield randomized policies               ∆ia ≤   argued section  difﬁcult implement                                                                                              deterministic ones                                                                                                                             xiax ≤ ∆ia    desirable compute optimal solutions                                                                                           ≥   ∆   ∈     class stationary deterministic policies             ia        ia  harder problem feinberg  studied computed polynomial time exam                                                        ple solving lp  objective function replaced  problem showed npcomplete using reduc   tion similar filar krass  reduced max ia xia setting maximum value  mathematical program augmenting  following reduction milp valuable  constraint ensuring xia state nonzero mains difﬁcult implement randomized sta                                                        tionary policy agent’s architectural limitations                 −                                    ia    ia    ia    ia                  domains limitations  resulting program  linear                                                        present used evaluating quality vs  vex presents signiﬁcant computational challenges                                                        implementationdifﬁculty tradeoffs randomized    show  reduced mixed integer linear deterministic policies during agentdesign phase  program milp equivalent  beneﬁ  cial milps wellstudied wolsey   exist efﬁcient implemented algorithms solving  constrained mdps multiple discounts  reduction uses techniques similar ones em turn attention general case mdps  ployed dolgov durfee mdp multiple streams rewards costs each owndiscount factor γn ∈  total expected reward synchronization different occupation  weighted sum discounted reward streams    measures constraint forces deterministic policies                                   ∞                    program  nonlinear nonconvex                                          uπ α     βnuγn π α    βn       γnϕitπiaria  difﬁcult solve                              ia                 ﬁnding optimal stationary deterministic policies                           th                           present reduction program  linear integer  βn weight reward stream deﬁned  reward function rn  s×a → similarly each program equivalent  just like previous  total expected costs weighted sum cost streams section reduction milp allows exploit wide                                      ∞                 array efﬁcient solution techniques tools reduction                                   ckπ α     β  ck π α     β      γt ϕ tπ ckn  based following proposition                 kn γn             kn        ia ia                            nia                 proposition  consider mdp hs αi sev                             th  βkn weight discounted stream cost eral discount factors γn ∈  set policies  type deﬁned cost function ckn  × → πn ∈  corresponding occupation mea                                                                                                  notice mdp multiple discount factors sures policy π discount factor γn deﬁne                                                                    reward functions kn cost functions unlike stant ≥ xia ∀n ∈  ∈ ∈ set binary  constrained mdp previous section  variables ∆ia     reward cost functions respectively               xn ∆ satisfy following conditions                                                                           goal maximize total weighted discounted              ∆ia ≤   ∀i ∈             ward subject constraints weighted discounted costs                                                                           max uπ α   π α ≤  ∀k ∈            ≤  ∆ia  ∀n ∈  ∈ ∈        π                                                    ia                                                                                                   feinberg shwartz   developed general sets reachable states   xia                                                          deﬁned occupation measures   theory constrained mdps multiple discount factors   demonstrated general optimal policies nei  ∀n ∈  furthermore πn deterministic                                                                          ther deterministic stationary  πia  πia ∀n ∈   special cases feinberg shwartz  im                        ∗  plementable algorithms ﬁnding optimal policies proof consider initial state αi∗   follow  problems given complexity im ing argument proposition  policy state  plementing nonstationary randomized policies deterministic                                                           ∗                                         ∗  ﬁnd worthwhile consider problem ∃a  xi∗a∗   ∆i∗a∗   xi∗a   ∆i∗a   ∀a   constructing optimal stationary deterministic policies implies occupation measures xn pre  mdps feinberg  showed ﬁnding optimal scribe execution deterministic action a∗                                                             ∗             policies belong class stationary randomized state  xia tied ∆ia   deterministic policies npcomplete task occupation measures xn correspond  mulated problem ﬁnding optimal stationary policies deterministic policy initial states   αi   following mathematical program based eq   expand statement induction                                                reachable states clearly set states                           xja − γn    xiapiaj  αj                                                                                                            reachable step                                   ia                                                        argument xn map                               kn                                                                            βkn    cia xia ≤          deterministic policy forth                                     max     βn            ia                        immediately follows proposition  prob                  ia ia              ia                       lem ﬁnding optimal stationary deterministic policies                        xia   xia                                                ia       ia       mdp weighted discounted rewards constraints                                                     formulated following milp                                                 ≥                                                                                  ia                                                        − γ         α                                                                                ja       ia iaj                                                                                        ia  program occupation measure each discount                    factor γ  ∈  expresses total reward total                      kn                                                                                   βkn    cia xia ≤ bc   costs weighted linear functions occupation mea                                                                                             ia  sures ﬁrst set constraints contains conservation max βn riaxia                                                                                 xn ≤ ∆    ﬂow constraints each occupation measures       ia       ia       ia                                                                                set constraints ensures occupation mea                   ∆   ≤                                                                                    ia  sures map policy recall                                                                                                                     previous section limit search deter              xia ≥  ∆ia ∈    ministic policies imposing following additional                                            straint occupation measures  feinberg  ≥ max xn constant proposition                                                                         ia                                            milp produces policies opti                xia − xia       xia  xia                                                         mal class stationary deterministic ones present       absolute value sample run                                          milp time                                                                                                                                                                                                                                                                                                                                            sec                                                  policy  value                                                      relative  value                                                                                                 randomized                                                     milp  time normalized                                                                                                                                                        deterministic                                     performance profile                                                                                                                                                                                                                                                                                                                                                   γ                 γ                                                                                                                   constraint level          constraint level           bound sec                                                                                                       figure  value deterministic randomized policies solution time proﬁle mdp discounts     time best knowledge practical al tic policies functions constraint level     gorithms ﬁnding optimal solutions larger policy  means policies incur zero cost feasible   class constrained mdps multiple discount factors strictest possible constraint  means upper                                                         bound cost equals cost optimal unconstrained     experimental observations                          policy agent constrained ﬁrst observation                                                         illustrated figure value stationary deter   implemented milp algorithm ﬁnding opti                                                         ministic policies constrained problems reasonably close   mal stationary deterministic policies constrained mdps                                                         optimal observe value determinis   empirically evaluated class test problems                                                         tic policies changes discrete manner jumps   following discussion focus constrained mdps                                                         certain constraint levels value random   section  problems better studied                                                         ized policies changes continuously course   existing algorithms ﬁnding optimal randomized                                                         natural given space randomized policies   policies serve benchmarks al                                                         tinuous randomized policies gradually increase   ternative algorithms ﬁnding policies optimal                                                         probability taking “better” actions cost constraints   general constrained mdps multiple discount factors                                                         relaxed hand space deterministic poli     empirical analysis tried answer follow                                                         cies discrete quality jumps constraints   ing questions  deterministic policies perform                                                         relaxed permit agent switch better action   compared optimal randomized ones                                                          number size jumps value   averagecase complexity resulting milps answers                                                         function depends dynamics mdp highlevel   questions obviously domaindependent                                                         picture experiments   following discussion viewed comprehen   sive characterization behavior algorithms figure shows running time milp solver   constrained mdps believe experiments function constraint level figure   provide interesting observations problems plots contain values averaged  runs error     experimented large set randomlygenerated bars showing standard deviation data indicates   problems meaningful manuallyconstructed milps  easyhardeasy complexity proﬁle   domain randomly perturbed ways sharp phase transition hard easy   big picture resulting experiments randomly problems quickly hard gradu   generated domains similar ally easier cost constraints relaxed   manuallyconstructed example providing certain measure complexity proﬁle gives rise question regarding   comfort stability validity observations source difﬁculty solving milps “hard”   report results manuallyconstructed domain region difﬁcult ﬁnd good feasible solutions     test domain used simplistic model timeconsuming prove optimality figure suggests   autonomous delivery agent mentioned introduc case considered   tion based multiagent example dolgov fortunate outcome algorithms performance   durfee domain agent operating proﬁles successfully used anytime manner   grid world delivery sites placed randomly ﬁgure contains plot quality best solution                                                                                                              grid agent moves grid incurring small function time bound imposed milp solver   negative rewards receives positive problems hardest constraint region constraint level   wards making deliveries agent’s movement non value  graph shows good policies   deterministic agent probability getting usually produced quickly   stuck randomlyplaced dangerous locations agent let conclude somewhat intriguing observation   incurs scalar cost time objective milp solution time constrained mdps   maximize total expected discounted reward subject multiple discount factors section  generated   upper bound total expected discounted cost solved large number random mdps discount     results experiments summarized figure    figure shows values randomized determinis cplex  performed role milp solver
