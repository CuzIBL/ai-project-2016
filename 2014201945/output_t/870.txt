      using hierarchical bayesian model handle high cardinality attributes                     relevant interactions classiﬁcation problem                ∗                     jorge jambeiro filho                           jacques wainer                 secretaria da receita federal                   instituto computac¸˜ao             alfndega aeroporto viracopos            universidade estadual campinas               rodovia santos dummont km                         caixa postal              campinassp brazil cep           campinas  sp brazil cep                  jorgeﬁlhojambeirocombr                      wainericunicampbr                        abstract                             positive examples dataset imbal                                                        anced usually handled different resampling      employed multilevel hierarchical bayesian    strategies chawla et al  resampling      model task exploiting relevant interactions quires retraining classiﬁers each different assignment      high cardinality attributes classiﬁcation costs false positives false negatives      problem overﬁtting model   text costs known advance priorities changes      calculate posterior class probabilities pattern acording antifraud demands vary       combining observations train  example example false negatives cost      ing set prior class probabilities ob train classiﬁers possible      tained recursively observations patterns cost assignments advance      strictly generic  model      achieved performance improvements standard     hand produce reliable probability      bayesian network methods like naive bayes     estimates directly original dataset work      tree augmented naive bayes bayesian net    human resource allocation easier      works traditional conditional probability ta example time deﬁne selection rate sr      bles substituted noisyor gates default ta matches available human resources speciﬁc task      bles decision trees decision graphs detecting wrong customs codes considering anti      bayesian networks constructed cardinality fraud demands moment examples veriﬁed      reduction preprocessing phase using agglom   naturally sr examples likely      erative information bottleneck method            involve misclassiﬁcation allocation                                                        combine probability estimates costs vary                                                        example example retraining    introduction                                       comes unnecessary customs administration discuss                                                        cost criteria decided concentrate  countries imported goods declared bayesian techniques use resampling  importer belong large set classes customs technique requires retraining costs change  codes important each good correctly classiﬁed domain specialists claim combinations  each customs codes mean different tributes values involving make  customs duties different administrative sanitary probability instance positive signiﬁcantly higher  safety requirements goal project develop expected looking each value separately  tool considering attributes declared custom code combinations critical patterns beneﬁt  dcc importer imp country production cp entry critical patterns like use bayesian net  point receiving country epr estimate each work bnpearl  attribute nodes parents  new example probability involves misclassiﬁca class node structure direct bn struc  tion estimates used larger ture  allocates human resources different types antifraud op  erations                                               bn considering xji possible value node    data set  examples correct classiﬁcation xj πjk complete combination values Πjthe  negative examples  examples set parents node xj vector θjk θjki   misclassiﬁcation positive examples dataset ﬁrst xjiπjk contained cpt node xj assessed  attribute  distinct values second  values frequencies values xj training   values fourth  values      instances Πj  πjk distributions xj given                                                        different combinations values parents xj    ∗this work harpia project supported assumed independent dirichlet prior probability  brazil’s federal revenue                              distribution θjk usually adopted applying bayes rule                                                    ijcai                                                    integrating possible values θjk proportional product cardinality                                                        parents attributes proportional sum                                 njki  αjki              jki   ji jk                        cardinality causal independence assumptions           θ        π                                                       njk   αjk              incompatible goal capturing critical patterns  njki number simultaneous observations possible use ﬂexible representations  xji πjk training set njk  ∀i njki αjki conditional probability distributions node given par  value parameters dirichlet prior probabil ents like default tables dfs friedman goldszmidt  ity distribution αjk  ∀i αjki equivalent sample  decision trees dts friedman goldszmidt  size prior probability distribution            decision graphs dgs chickering et al     dirichlet prior probability distribution usually according friedman goldszmidt  using  sumed noninformative yields           representations adequate learning procedures                                                        induces models better emulate real complexity                             njki  λ                xjiπjk                         interactions present data resulting network                            njk  λmj                   structures tend complex terms arcs  parameters dirichlet distribution equal require fewer parameters fewer parameter result  small smoothing constant λandmj number possible smaller overﬁtting problems hand using tra  values node xj direct estimation ditional cpts assume probability distributions    direct bn structure node cpt node given combinations values par  estimated class node attribute nodes ents independent distribution actu  parents conditional probability table cpt ally identical dts dfs dgs reﬂect represent  class node structure contains  ×  cpd using variable number parameters  parameters clear rarely seen combinations proportional number actually different distributions  attributes choice structure direct bn struc using dts dfs dgs represent conditional distri  ture equation  tends produce unreliable probabilities bution node given parents assume prob  calculation dominated noninformative prior ability distribution node given different combina  probability distribution suggests using direct tions values parents identical com  bn structure traditional cpts overﬁtting pletely independent possible  problems                                             sumptions hold    instead direct bn structure choose net gelman et al  asserted modeling hierar  work structure does lead large tables chical data nonhierarchically leads poor results  achieved limiting number parents network parameters nonhierarchical models ﬁt data accu  node naive bayes duda hart  extreme ex rately parameters ﬁt existing data  ample maximum number parents limited lead inferior predictions new data words  class node parent node tree overﬁt contrast hierarchical models ﬁt data  augmented naive bayes tan friedman et al  adds overﬁtting reﬂect similarities  tree structure naives bayes connecting non distributions assuming equality  class attributes limits maximum number par observing slight modiﬁcation equation  used  ent nodes limiting maximum number friedman et al  deﬁnition smoothing  parents limits ability capture interactions schema tan data used es  attributes beneﬁt critical patterns timate cpt node parent  prefer                                  hierarchical    high cardinality attributes creating trou              njki  · xji  ble reasonable idea preprocess data reducing xjiπjk                                                                                                            njk   cardinality attributes use example ag  glomerative information bottleneck aibn method slonim constant deﬁnes equivalent sample size  tishby  task process prior probability distribution di  ducing cardinality attribute blind respect rect estimation ade ade uses probability distribution  class attribute assessed wider population build informative prior  likely cardinality reduction result signiﬁcant probability distribution narrower population  improvement ability capture critical patterns hierarchical nature approach used  depend attribute             example cestnik  ade consequence    number probabilities estimated stead noninformative dirichlet prior probability distribu  large compared size training set tion adopting dirichlet prior probability distribution  ﬁll traditional conditional probability tables αjki ∝ xji  cpts satisfactorily pearl  recommends  ade gets closer true probability distribution  adoption model resorts causal independence discrimination power signiﬁcantly better  sumptions like noisyor gate using noisyor num linear combination factors njkinjk xji  ber parameters required represent conditional prob second factor closer true probability distribution  ability distribution cpd node given parents instead constant counterpart direct estimation                                                    ijcai                                                    equal combination values Πj example    gw   discrimination power                                                                       ade jumps speciﬁc population set                   training examples Πj  πjk general pop consider gw  inﬂuences crgw  di  ulation training set contrast present rectly crgw   crgw  inﬂuence  model hierarchical pattern bayes hpb patterns gw  captured recursive  moves slowly smaller populations larger ones bene process ﬁrst step decomposition crgw   ﬁting discrimination power available each level expression evaluated recursively apply                                                        bayes theorem    hierarchical pattern bayes classiﬁer                                                                             gw crp cr                                                            crgw    hpb generalization ade employs hierarchy                     gw   patterns combines inﬂuence different level patterns         ∝                          way speciﬁc patterns dominate                 wwwl   cr  cr  represented combines patterns wwwl elements gw   level making strong independence assumptions calibra  approximate     joint  probability  tion mechanism                                       wwwlcr   product marginal    hpb works classiﬁcation problems attributes probabilities  nominal given pattern training set pairs                                                                                       cwherec  class label pattern hpb                                                                                                     ∝                calculates crw  class cr pattern     cr      cr     wj  cr         deﬁned                                                                        deﬁnition  pattern set pairs form   apply calibration mechanism  attribute  value attribute appear                                                                                      crgw  ∝ crgw   bpcr      attribute set said  undeﬁned missing                                  calibration coefﬁcient                                                          given equations   need calculate wj cr  deﬁnition  pattern generic pattern                                                        applying bayes theorem  ⊆ ify generic wesay  satisﬁes                                                            crwjp wj                                                                     wjcr                            deﬁnition  pattern strictly generic                         cr  ⊂                                                           estimate cr using maximum likelihood ap  deﬁnition  level pattern  levelw  num proach crnrn wherenr number ex  ber attributes deﬁned                        amples training set belonging class crandn  deﬁnition  gw  set patterns strictly total number examples training set happens  generic pattern                              nr zero use equation  case just                                                        deﬁne crw  zero pattern     hierarchical model                             know substitute wjcr right                                                     side equation  equation  able clear  hpb  calculates posterior probability cr             ing strategy similar direct estimation factor wj identical classes  prior probabilities considered given need worry                                                          wj pattern estimation crwj   crgw     parameters dirichlet prior probability distrib recursively using equation  recursion ends                                                        gw   contains pattern case  utionusedbyhpbaregivenby    αr   · crgw                                                                     smoothing coefﬁcient consequently                                                  nwr   · crgw              calibration mechanism           crw                                                           nw                      spite strong independence assumptions naive bayes                                                        know perform domains misclas  nw number patterns training set satisfy siﬁcation rate considered domingos pazzani   ing pattern nwr number instances naive bayes know produce unbalanced  training set satisfying pattern class label cr probability estimates typically “extreme”    given equation  problem calculate                                              sense close zero close  cr    basic idea write cr aim obtaining better posterior probability distribu  function various crwj wj patterns                                                   tions calibration mechanisms try compensate  belonging calculate each cr wj recursively overly conﬁdent predictions naive bayes pro  using equation                                      posed bennett  zadrozny   deﬁnition  gw  subset gw  elements using equation  making stronger independence  level equal levelw  −                     assumptions naive bayes naive bayes assumes                                                    ijcai                                                    attributes independent given class • prior trivial classiﬁer assigns prior probability instance  possible equation  assumes aggregations models involving dgs constructed follow  tributes independent given class ing chickering et al  models involving dfs  aggregations attributes common know dts constructed following friedman goldszmidt  assumption false main consequences stronger  using mdl scoring metric  unrealistic assumption extreme probabil                                                          tried different parameterizations each method  ity estimates naive bayes’ ones compensated                                                        sticked parameter set provided best results  calibration mechanism equation  calibration                                                        best results mean best area hit curve   mechanism analogous used zadrozny                                                           selection rate auc inthey axis chose  calibration decision tree probability estimates                                                        represent recall  ntruepositivesnpositives instead    selecting hpb coefﬁcients                        absolute number hits does change                                                        form curve makes interpretation easier rep  equations   require respectively speciﬁcations resented selection rate log scale emphasize begin  coefﬁcients classiﬁcation single ning curves using hit curve compared  stance equations applied hpb calculation                                                     probability distributions estimated models  cr  different patterns  optimal distribution actually test set using measures  values different each pattern  root mean squared error rmse mean cross entropy    case coefﬁcients use heuristic mo mce figure  table  table  show results  tivated fact level pattern gw                                                           selection method parameters explained  levelw  −  higher level attributes                                                          smoothing coefﬁcients employed hpb au  common aggregations extreme proba                                                        tomatically optimized optimization involves leave  bility estimates stronger effect                                                        cross validation takes place absolutely  calibration mechanism coefﬁcient                                                        current training set  fold cross validation varies  equation  equal blevelw  −  experi                                                        current training set eliminating possibility ﬁtting  mental constant                                                        test set coefﬁcients deﬁned heuristic    case coefﬁcients employ greed opti                                                        scribed section  constant bwevariedb  mization approach starts general pattern                                                        enumeration       sticked  family speciﬁc ones pat                                                         constant produced best  tern family set containing patterns deﬁne exactly                                                        sults  fold cross validation process avoid effects  attributes possibly different values                                                        ﬁne tuning reshufﬂed data set starting    assuming coefﬁcients ﬁxed                                                         fold cross validation process hpb results  pattern families generic family                                                        present came second process   single coefﬁcient needs speciﬁed  allow use equation  calculate crw                                                                        hpb   pattern belonging  select coefﬁcient        dg bsm cb  using leave cross validation order maximize       aibntan                                                                       nb  area hit curve induced calculate           tan  crw  training patterns inf                         prior      experimental results                                    recall  classiﬁcation methods tested weka experi  menter tool witten frank  using  fold cross val  idation compared classiﬁers built using following  methods                                                              •                                                                   hpb hpb described paper                                     selection rate    • nbnaivebayes    • noisyor bn direct bn structure using noisyor instead cpt    • tan tan traditional cpts                    figure  selection rate recall avoid pollution    • ade direct estimation bn direct bn structure smoothing schema present curves related subset tested meth      described friedman et al               ods    • direct estimation bn direct bn structure traditional cpts    • aibntan tan traditional cpts trained dataset cardinality reduction optimization aibntan involves  parameters      using aibn previously applied                tan smoothing constant stan aibn smooth    • dg cbm bn dgs complete splits binary splits merges enabled                                                               • dg cb bn dgs complete splits binary splits enabled employed hit curves instead popular roc    • dg bn dgs complete splits enabled  curves match interests customs adminis    • dg cm bn dgs complete splits merges enabled trations directly human resource allocation deﬁnes    • hc dt bn decision trees learned using hill climbing hc mdl scoring selection rate needs estimate number positives      metric                                           instances detected                                                               • hc df bn default tables learned using hc mdl selection rates                                                     ijcai                                                    ing constant saibn  minimum mutual informa  learned default tables included rows learned  tion constant mmi  varying stan  enumera   decision trees decision graphs involved splits  tion        saibn enu consequence cases resulting models little  meration     mmi enumeration discrimination power     best results ob using decision graphs binary splits enabled  tained triple besttriple stan saibn  critical patterns separated  mmi    did cover grid    sulted signiﬁcant improvement beginning  did observe abrupt variations hit hit curves patterns left  curves covered immediate neighbors resulted loss discrimination power selection  besttriplewe believe exhaustive covering rates   necessary                                              hypothesis tests show hpb signiﬁcantly better                                                        classiﬁers regards auc auc rmse      method                                mce performed hypothesis tests se      hpb     ±±±±±                                     tan      ±±±±±  lection rate      steps  hpb signif      aibntan ±±±±± icantly better classiﬁers selection rate      nb       ±±±±±      noisyor ±±±±±   exceptions signiﬁcantly bet      bndg cb ±±±±±  ter tan    aibntanin      bndg cbj ±±±±±      bndg   ±±±±±      bndg cbj  bndg cb      ade     ±±±±±                    ±±±±±             bndg cj  ±±±±±     hpb beneﬁts critical patterns involving      hc df    ±±±±±      hc dt    ±±±±±    attributes considers inﬂuence speciﬁc      prior    ±±±±±    patterns consequence performs selection                                                        rate   table  recall different selection rates std dev                                                           conclusions    method     auc   auc   rmse    mce parameterization    hpb     ±±±±       domain preselection imported goods veriﬁ    tan     ±±±±     cation combinations attribute values constitute    aibntan ±±±± besttriple    nb      ±±±±     critical patterns inﬂuence probability ﬁnd    noisyor ±±± infinity          ing positive instance signiﬁcant high    bndg cb ±±±±      bndg cbj ±±±±      cardinality attributes domain exploiting    bndg  ±±±±     ade     ±±±±     patterns overﬁtting challenging addressed         ±±±±      problem using hpb novel classiﬁcation method based    bndg cj ±±±±     hc df   ±±±±      multilevel hierarchical bayesian model    hc dt   ±±±±        hpb shown capable capturing inﬂuence crit            ±±±±    prior                                               ical patterns overﬁtting loosing discrim                                                        ination power exhaustion critical patterns  table  area curve accuracy probability esti test set hpb resulted hit curve unambigu  mates optimal parametrization                     ously better methods hpb pro    noisyor prior parameters optimiza duced better probability estimates according accuracy  tion methods involves smoothing measures table   stant cases exhaustively varied hpb validated specialized domain  enumeration        equations simple reﬂect particularities  enumeration covers different magnitudes smoothing domain characterized high cardi  constant time avoids ﬁne tuning      nality attributes relevant interactions    naive bayes noisyor unable capture inter use hpb good option domains  actions attributes explore critical pat characteristics provide light  terns explains performance beginning develop good models domains  hit curve tree augmented naive bayes explores hpb training time exponential number  teractions attributes performed better tributes linear number training instances  naive bayes noisyor beneﬁt dependent attributes cardinality hpb  critical patterns involving attributes deci applicable domains attributes  sive selection rate                domains faster methods training    applying cardinality reduction attributes time depends cardinality attributes  structing tan model did lead signiﬁcant im hpb bayesian model sense gel  provements hit curve                          man  et al  parameters associated    substituting traditional cpts bayesian network subpopulation assumed drawn general dis  decision trees default tables decision graphs bi tribution calculation involved probability dis  nary splits disabled hit curves worse tributions considering available evidence                                                    ijcai                                                    
