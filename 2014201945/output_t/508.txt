                         bayesian inverse reinforcement learning                     deepak ramachandran                                 eyal amir                    science dept                       science dept          university illinois urbanachampaign   university illinois urbanachampaign                       urbana il                              urbana il                           abstract                          expert atkeson schaal  reward                                                        function generally succint robust transferable      inverse reinforcement learning irl prob representation task completely determines op      lem learning reward function underlying  timal policy set policies addition knowledge      markov decision process given dynamics     reward function allows agent generalize better      behaviour expert irl    new policy computed environment changes      motivated situations knowledge irl likely effective method      rewards goal preference elici paper model irl problem bayesian      tation task apprenticeship learning perspective consider actions expert evidence      learning policies expert paper use update prior reward functions solve      show combine prior knowledge evi   reward learning apprenticeship learning using poste      dence expert’s actions derive probabil rior perform inference tasks using modiﬁed      ity distribution space reward functions markov chain monte carlo mcmc algorithm show      present efﬁcient algorithms ﬁnd solutions markov chain distribution uniform      reward learning apprenticeship learn prior mixes rapidly algorithm converges      ing tasks generalize distribu correct answer polynomial time show      tions experimental results show strong improve  original irl special case bayesian irl birl      ment methods previous heuristicbased laplacian prior      approaches                                         number advantages technique                                                        previous work need completely speciﬁed op    introduction                                       timal policy input irl agent need  inverse reinforcement learning irl problem assume expert infallible incorpo  ﬁned russell  follows determine reward rate external information speciﬁc irl problems  function agent optimizing given  measurement prior model use evidence multiple experts  agent’s behaviour time variety circum irl ﬁrst studied machine learning setting                                                                            stances  measurements sensory inputs agent ng russell  described algorithms   model environment context markov optimal rewards mdps having ﬁnite inﬁnite  cision processes translates determining reward states experimental results show improved performance  function agent knowledge policy executes techniques ﬁnite case  dynamics statespace                    rest paper organised follows section    tasks irl accomplishes ﬁrst  deﬁne terms notation section  presents  ward learning estimating unknown reward function bayesian model irl process section  discusses  accurately possible useful situations use model reward learning apprenticeship  ward function example learning section  discusses sampling procedure  structing models animal human learning modelling sections    present experimental results related  opponent competitive games pokerbots improve work conclusions respectively  formance suboptimal human opponents learning  reward functions account utility money prefer  preliminaries  ences certain hands situations idiosyncrasies recall basic deﬁnitions theorems relating  billings et al  connections various markov decision processes reinforcement learning  preference elicitation problems economics sargent  ﬁnite markov decision problem tuple    second task apprenticeship learning  using obser γ  vations expert’s actions decide one’s behaviour • ﬁnite set states  possible situation directly learn policy •  aak set actions                                                    ijcai                                                      •  × × →   transition probability func        tion                                                                     • γ ∈   discount factor                                                                                          •  →  reward function absolute value                         bounded rmax                                                          rewards functions state irl prob                                                                                                            lems typically limited information value                           action want avoid overﬁtting                                                                                        markov decision process mdp tuple γ                             terms deﬁned reward func                     tion avoid confusion use abbreviation mdp  markov decision processes problems  figure  example irl problem bold lines represent                                                 adopt following compact notation ng optimal action each state broken lines represent                                            sn  russell  ﬁnite mdps  fix enumeration     action action probabilities   ﬁnite state space reward function                                                                    going states    respectively  function statespace represented  actions deterministic  dimensional vector ith element rsi                              π   →    stationary policy map       dis   evidence expert  counted inﬁnitehorizon value policy π reward func  tion state ∈ sdenoted πs given  present details bayesian irl model fig   π                                                    derive posterior distribution rewards   st     st   es  γr st γ  st   π                                prior distribution probabilistic model expert’s    prsti sti πt sti πsti sti  goal actions given reward function  standard reinforcement learning ﬁnd optimal policy consider mdp γ agent  π∗ πs maximized ∈ π  π∗ expert operating mdp assume reward  shown example sutton barto function chosen known prior distribu   policy exists ergodic tion pr irl agent receives series observations  mdps solution markov decision problems expert’s behaviour ox  sa sa skak  useful deﬁne following auxilliary qfunction  means state si took action ai time        π                               π                         rrsγes∼t  sa·v      step  generality specify algorithm                                                                                        q∗ · ·            uses determine possibly stochastic policy  deﬁne optimal function         make following assumptions behaviour  function optimal policy π∗ reward function                                                           attempting maximize total accumulated    finally state following result concerning markov                               decision problems sutton barto          ward according  example  using                                                            epsilon greedy policy explore environment  theorem   bellman equations let markov decision                                                             problem  γ policy π  →     executes stationary policy invariant wrt  given                                              time does change depending actions            ∈ ∈ π  qπ                           observations previous time steps                 and   satisfy          π                                π          example agent learned policy         srsγ          πssv                                                          mr  using reinforcement learning algorithm                            s                                                       expert’s policy stationary make following        π                               π         arsγ                independence assumption                            s                                                          prx  ox rprx     sarprx sar    π optimal policy iff ∈                                                                           prx skakr                   π ∈        qπ                   expert’s goal maximizing accumulated reward                       argmax                                                           ∗                           a∈a                          equivalent ﬁnding action value each                                                        state maximum larger q∗s  bayesianirl                                          likely choose action state like  irl currently viewed problem infering single lihood increases conﬁdent ’s ability  ward function explains agent’s behaviour select good action model exponential dis                                                                                              ∗  little information typical irl problem tribution likelihood siai potential  answer example consider mdp shown function                                                                                          ∗  figure  three reasonable kinds reward                      αx siair                                                                  prx siair    functions r· high positive value low values                   zi  explains policy tries return α                                ·      ·                           parameter representing degree  state     high values  ﬁdence ’s ability choose actions high  respectively probability distribution needed  represent uncertainty                               note probabilities evidence conditioned                                                    ijcai                                                                                                                                           −                                                                  plaplacersr       σ  ∀s ∈                                                                                     σ                                                           underlying mdp represented planningtype                                                            problem expect states low negative                                                            rewards states high rewards corre                  αx                                      sponding goal modeled beta dis                                                            tribution reward each state modes                                                            high low ends reward space                                                                                           ∀s ∈                                                               beta                                                                                      −                                                                               rmax    rmax                                                           section  example informa                figure  birl model                tive priors constructed particular irl problems                                                           inference  value distribution satisﬁes assumptions easy                                                        use model section  carry tasks  reason likelihood entire evidence                                                         described introduction reward learning appren                              αx eox              prx ox                            ticeship learning general procedure derive minimal                                                      solutions appropriate loss functions posterior eq                     ∗  eox   siai appropriate  proofs omitted lack space  normalizing constant think likelihood func  reward learning  tion boltzmanntype distribution energy eox                                                        reward learning estimation task common loss  temperature αx     compute posterior probability reward func functions estimation problems linear squared  tion applying bayes theorem                     error loss functions                                                                             ˆ              ˆ                                                                   llinearr r     −                           pr                                   rˆ       − rˆ          pr   ro                                            se                                          pr                                                     rˆ actual estimated rewards respec                             α eo                                                                      tively drawn posterior distribution                                                                                      ˆ                                                      shown expected value lser minimized    computing normalizing constant z hard setting rˆ mean posterior berger   sampling algorithms use inference need similarily expected linear loss minimized setting rˆ  ratios densities points prob median distribution discuss compute  lem                                                  statistics posterior section                                                           common bayesian estimation problems use    priors                                           maximum posteriori map value estimator  information given assume fact following result  rewards independently identically distributed iid theorem  expert’s policy optimal fully  principle maximum entropy prior func speciﬁed irl algorithm ng russell   tions considered paper form exact equivalent returning map estimator model  prior use depends characteristics  laplacian prior  problem                                                          irl problems posterior distribution    completely agnostic prior typically multimodal map estimator rep      use uniform distribution space −rmax ≤ resentative measures central tendency like mean      rs ≤ rmax each ∈ want spec      ify rmax try improper prior  apprenticeship learning                      ∈                                    apprenticeship learning task situation    real world markov decision problems parsi teresting attempting learn policy π      monious reward structures states having neg formally deﬁne following class policy loss functions      ligible rewards situations better                                                                 lp     rπ     ∗ − π        assume gaussian laplacian prior                       policy                                                                                 ∗                                                                                                  −                     vector optimal values each state         pgaussianrsr√         σ  ∀s ∈                                                                         πσ                    acheived optimal policy norm                                                       wish ﬁnd π minimizes expected policy loss  αx fig  simpler treat αx just posterior distribution following theorem  parameter distribution                        accomplishes                                                    ijcai                                                    theorem    given  distribution reward   algorithm policywalkdistribution mdpm step size δ   functions mdp  γ loss function   pick random reward vector ∈ rsδ                                 ∗                         π  policyiterationmr  lpolicyrπ minimized πm  optimal policy                                                             repeat  markov decision problem γ ep                                                              pick reward vector r˜ uniformly random  proof bellman equations  derive fol   neighbours rsδ  lowing                                                               qπs r˜   ∈                  π             π −                          compute                                 ri  −  γt                        ∃s ∈ qπs πs r˜ qπs r˜  π s×s transition matrix policy π π˜  policyiterationmr˜π  state ∈ ﬁxed π value function linear   ii set  r˜ π  π ˜ probability                                                                   min r˜π˜   function rewards                                               rπ                  π                                                              rws π  ·                                                            ˜                                                                       r˜            min rπ                                                                  set      probability   rπ  ws π s’th row coefﬁcient matrix −     π                                                      return  γt  −  suppose wish maximize ev πs                                                   figure  policywalk sampling algorithm          π  max ev   max ews π·rmaxws π·er   π                 π                 π                                                        steps policy iteration sutton barto start                             ∗    deﬁnition equal   optimum value ing old policy π policywalk correct                                     π  π∗  function  maximizing policy  op efﬁcient sampling procedure note asymptotic                                ∈ π  timal policy  states        memory complexity gridwalk             π   π∗  maximum                                          second concern mcmc algorithm speed       ∗ ≥ π      ∈                      reward functions convergence markov chain equilibrium dis             π     policies                           tribution ideal markov chain rapidly mixing                           ∗        π        elpolicyπ  e −  p          number steps taken reach equilibrium polynomially                            ∗                           bounded theoretical proofs rapid mixing rare  minimized π  πm                     show special case uniform prior    instead trying difﬁcult direct minimization markov chain posterior  rapidly mixing using                                                        following result applegate kannan   expected policy loss ﬁnd optimal policy                                                        bounds mixing time markov chains pseudolog  mean reward function gives answer                                                        concave functions                                                        lemma   let · positive real valued function deﬁned    sampling rapid convergence                                                                                             cube ∈ −d  ≤ xi ≤ positive  seen reward learning apprenticeship satisfying λ ∈   α β  learning require computing mean posterior distribu  tion posterior complex analytical deriva     fx − fy≤α   − ∞  tion mean hard simplest case uni  form prior instead generate samples distribu  tions return sample mean estimate fλx − λy ≥ λfx−   λfy − β  true mean distribution sampling technique use  mcmc algorithm gridwalk  vempala   fxlogf  markov chain induced  generates markov chain intersection points gridwalk policywalkonf rapidly mixes                                                              ndαeβ      grid length δ region denoted δ                 log   steps    computing posterior distribution partic                                                        proof applegate kannan   ular point requires calculation optimal qfunction  expensive operation use modi theorem  given mdp γ   ﬁed version gridwalk called policywalk figure   distribution rewards rprx rox                                                                                                         efﬁcient moving markov chain deﬁned  uniform prior pr  ∈ −  sampler keeps track optimal policy π rmax ≤ ri ≤ rmaxifrmax    current reward vector observe π known efﬁciently sampled error  inon  log   steps  function reduced linear function algorithm policywalk  reward variables similar equation  step                                                                                                         performed efﬁciently change optimal policy proof uniform prior points   easily detected moving reward vec ignore sampling purposes normal                                                                                     α      tor chain r˜ ∈ izing constant let      π        ˜       π     ˜                             choose arbitrary policy π let   πs qs theorem                                                                                                                 π  happens new optimal policy usually slightly dif      fπrαx       ai  ferent old computed just                                                                         ijcai                                                                           reward loss                                            policy loss                                                                                     qlbirl                                               qlbirl              kgreedybirl                                          kgreedybirl                    qlirl                                               qlirl               kgreedyirl                                           kgreedyirl                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      figure  reward loss                                 figure  policy loss                                                                                    posterior samples  note fπ linear function fr ≥ fπr               ∈     alsowehave                                                                                                                                                                                                                                            q∗       qπ        π  ≤   max                 maxsa    maxsaπ   maxsπ max                                                − γ             −                    ∗          rmax                                −                  ≥−                    ≤             − − − − −        similarly minsa         −γ                                   rs  α nr                   α nr     max       ≥−     max                                               true rewards     −γ   π         −γ                                                                                                                            αx nrmax                                                                                                      fπr ≥ fr −                                                                     − γ                                                                                             −  ∈ λ ∈                                                                   −                                                                 − − − − −                                                                                        rs   fλr  −  λr   ≥   fπλr −  λr                        ≥   λfπr−   λfπr                        ≥   λfr−   λfr        figure  scatter diagrams sampled rewards arbi                                                        trary states given mdp expert trajectory com                              αx nrmax                            −                           puted posterior shown close true distribution                                  − γ    satisﬁes conditions lemma  β                                                        policy qlearning mdp  reward function  αx nrmax             −γ     · −γ                        learning rate controlled agent allowed                                                        converge optimal policy came reasonably close            −       α  nr      α             ≤      max             second agent executed policy maximized ex            −                                                                                    ∞      − γo                pected total reward steps  chosen                                                        slightly horizon time  markov chain induced gridwalk al      birl used policywalk  sample posterior  gorithm policywalk algorithm  mixes    distribution  uniform prior compared results                       rapidly   number steps equal  methods average   distance true    eo                           log         log                 reward function figure  policy loss   norm                                                        figure  learned policy true reward    note having rmax   really restric                                                        measures show substantial improvement note  tion rescale rewards constant factor                                                        used logarithmic scale xaxis  computing mean changing optimal pol                                                          measured accuracy posterior distribu  icy value functions functions scaled                                                        tion small comparing true distribution                                                         rewards set generated rewards gave rise                                                        trajectory expert figure  show scat    experiments                                        ter plots rewards sampled posterior  compared performance birl approach true distribution state mdp ﬁgures show  irl algorithm ng russell  experimentally posterior close true distribution  generated random mdps states  varying   rewards drawn iid gaus  domain knowledge prior  sian priors simulated kinds agents show domain knowledge problem  mdps used trajectories input ﬁrst learned corporated irl formulation informative prior                                                    ijcai                                                    
