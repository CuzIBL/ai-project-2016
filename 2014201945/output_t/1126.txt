             evaluating classifiers means test data noisy labels                            chuck lam                                 david stork               department electrical engineering                ricoh innovations                        stanford university                     sand hill road suite                        stanford ca                        menlo park ca                      chucklamstanfordedu                           storkriiricohcom                          abstract                          labeled samples orders magnitude nigam et al                                                                 expensive timeconsuming task       building pattern recognition col­     order build extend success reducing       lecting accurately labeling training testing labeling cost turn problem reducing need       data paper explore use inexpen­ accurately labeled data classifier evaluation stage       sive noisy testing data evaluating classifiers fact experiments learning labeled       performance assume  human labeler      unlabeled data use labels testing train­      provides category labels known mislabeling  ing nigam et al  need address       rate  trained classifier labeler labeling cost classifier evaluation       statistically independent derive num­      ber noisy test samples arc average    areas commerce general economics       equivalent single perfectly labeled test sam­ labeling higher quality accuracy la­      ple task evaluating classifiers perfor­ beling greater associated cost greater cost       mance practical realistic error misla­ greater expertise labeler need mul­      beling rates number equivalent test patterns tiple passes crosschecking addi­      surprisingly low derive upper   tional cost clean truth data labels       lower bound true error rate   situations marking text corpus labeling task       labeler classifier independent    complicated experts need passes                                                          reduce labeling errors eskin  furthermore                                                          application domains obtaining accurate labels simply    introduction                                         cost prohibitive example medical diagnostics                                                          true disease known expensive inva­  overall construction modern classification sive techniques similarly remote sensing send   divided broad tasks  specifying clas­ measuring instruments ground location obtain   sifier type  collecting data  training classifier ground truth transportation cost astronom­  learning  evaluating classifier testing duda ical quite literal remote sensing planets   et al  second stage data collection smyth  situations practice rely   divided tasks gathering samples labeling imperfect judgements experts smyth    recently machine learning community real­  ized practical cases expensive propose lower labeling cost classifier evalu­  design process labeling samples ation using cheaper noisy labels paper examines   example enormous number text docu­ methodologies estimating error rate classifier con­  ments internet obtained low cost fusion matrix using test data noisy labels shall   relatively labeled — slight labeling inaccuracy say    according content topic language style — con­ significant effect error rate estimate classi­  sistent way facilitate training classifier like­ fier performs addition data sets used   wise large databases recorded speech handwrit­ small expensive collect sense spend each   digits printed characters databases additional labeling effort increase label accuracy   labeled accurately labeled stork small data set data sets large cheap    reduce labeling expense researchers collect longer obvious spend each   sought ways modify training algorithms utilize additional labeling effort spend labeling   labeled unlabeled data blum mitchell  unlabeled data spend increasing accuracy   nigam et al  approach shown surprisingly labeled data present preliminary analysis   encouraging results cases reducing number question       learning                                                                                                  preliminaries notation    formulation assumes object possessing true label    € Ω set possible states    nature category membership object ob­  ject presented labeler marks label    guess situation labeling    error mislabeling classifier    hand presented feature vector represents    certain aspects object classifier outputs la­   bel yx guess notational convenience    classifier output dependence    feature vector implicit situation    classification error misclassification      probability labeler making mistakes    called mislabeling rate probability classi­ derivation assumed iv vt other­   fiers label different labelers label        wise noisy labels meaningless practice    called apparent error rate goal estimate     important inde­  pry  called true error rate note  pendence assumption labeler classifier make   possible high apparent error rate per­ errors independently stated succintly   fect classifier true error rate zero simply                      knowing labeler   high mislabeling rate classifier classify error pattern does change probability   test data perfectly disagree test la­ classifier make error vice versa   bels labels incorrect hand section  deal situations indepen­  possible zero apparent error rate dence assumption does hold meantime argue   high true error rate classifier labeler make idealization simplification based fact   kind mistakes                                         human generally classify samples using differ­     confusion matrix human labeler defined    ent methodologies make similar kinds                                                                  mistakes                                                                     example apparent error rate various true                                                                        error rates mislabeling rates                                                                  equation  gives way account noisy labels                                                                  calculating true error rate natural question                                                                  important correct influence noisy la­                                                                 bels lets consider classification systems error                                                                    rates   testing data sets                                                                    incorrect labels table  shows apparent error rate                                                                  classifiers different accuracy testing data different                                                                  mislabeling rates percentage increase true error      twoclass cases class         rate shown example    higher prior probability actual error rate good  testing labels wrong classifier true error rate    measure classifer usefulness example detecting     apparent error rate  higher    email spams network intrusions undesirable events      percentage increase dramatic noisier labels   rare easily error rate    accurate classifiers quick rule thumb    classifying events desirable situa­   labels relatively errors denominator   tions want compute entire confusion      eq  approximately  ignored mis­  matrix metrics precision recall frakes     labeling rate testing labels just   baezayates  denote rare class       additive component true error rate continuing pre­  spams network intrusions classifier precision    vious example  mislabeling rate classifier true   defined recall defined                         error rate  makes apparent error rate approximately                        analogously labeler note     actual    precision recall derived confusion   matrix class prior probabilities                       noisy labels estimating true error rate                                                                  assumed knowledge apparent error rate    obtaining true error rate                                            practice estimate rate us­                                                                 ing test data section analyze effects   examining relationship true apparent error     rates make constraint twoclass prob­       wc note classifi ers uci datasets accuracy   lem                                                  range kaynak alpaydin                                                                                                                   learning  table  left subcolumns table show apparent error rates different true error rates    different mislabeling rates based eq  assumed labeler classifier make errors    independently right subcolumns table uparrow signs show percentage increase apparent error    rate true error rate                                                                      variance error estimate given perfectly labeled data                                                                  vari•                                                                 ance ratio noisy labels perfect labels                                                                                                                                                                                               ratio help understand economic tradeoffs be•                                                                 tween using perfect noisy labels collecting perfect la•                                                                 bels collecting noisy labels cleaning of•                                                                 expensive just collecting noisy labels                                                                 self economically justified used noisy                                                                  labels long does need                                                                    unfortunately applying eq  requires know true    figure  figure shows number noisy labels needed  error rate classifier exactly trying    achieve variance true error rate estimate estimate good idea    single perfect label eq  plots represent dif• reasonable range true error rate case exam•   ferent true error rates mislabeling rate increases ine ratio wide range true error rate mislabeling    noisy labels arj needed achieve confidence note  rate ratio fall relatively narrow    ranges shown figure mislabeling range shown fig  relatively noisy testing    rate smaller true error rate single perfect label data  incorrect labels unless classifier    equivalent noisy labels                    accurate true error rate  cleaning                                                                  testing data perfectly labeled increases value    estimates assume  objects test set     factor words needs                                                                  noisy labels achieve effect    each feature vector true unknown label yi    noisy label classification assume    perfect label imagine perfect labels need collected    tuplesare independent domain expert noisy labels collected    identically distributed apparent er•               nonexpert high cost domain expert                                                                  justify use noisy labels   ror rate estimate        indicator function ievcnt   event  example evaluating noisy labels   true  estimate true error rate                                                                        reliable labels                                                                  labeling tasks experts make multiple passes                                                                  through samples ensure accurate labeling eskin                                                                   question wisdom policy samples      straightforward verify                        free labeling cost constraint       estimates                                  consider hypothetical labeling situation label  unbiased intuitively know confidence     ers each paid look  samples labelers   error estimates based test data noisy     error rate choices use   labels formalize intuition examine variance   labelers look completely different   true error rate estimate                               samples end testing set size                                                                   mislabeling rate choice look                                                                  exact samples assuming make independent       learning                                                                                                                                                                             table  apparent precision recall breakeven points                                                                                         different actual classifier preci•                                                                sionrecall labeler precisionrecall prior probabili•                                                                ties wi   respectively                                                                   classifications false positives false negatives                                                                 equal want know confusion matrix                                                                 addition domains classifying text spam   figure  figure shows labeling                                                                 distribution classes highly skewed error rate   policies optimal range mislabeling rate true                                                                 misleadingly low situations   classifier error rate based eq  problem posed                                                                 interested precision recall statistics esti•  labelers mislabeling rate                                                                 mated confusion matrix   paid label  samples policy label different                                                                    define joint distribution matrix labeler   samples creating test set  labels  samples                                                                 classifier   portion mislabeled policy label   samples creating testing set  labels  sam•  ples portion mislabeled various assumptions     labeling errors optimistically assume sample   wrong label labelers err test• estimated data note unlike anal•  ing set size  mislabeling rate better    ysis error rates necessary assume twoclass   policy                                                       problems     based previous discussion unbi•         goal recover classifers confusion matrix given   ased estimate true error rate testing set labelers confusion matrix joint distribution matrix   prefer gives lower variance estimate labeler classifier make independence    labels  samples policy assumption thatthen   variance lower  labels  samples policy   decomposition                                                                 rewrite decomposition matrix form solve                                                                 classifiers confusion matrix     little algebra                                                         •                                                                interesting observation realistic range                                                                 defined column vector prior prob•                   left hand side eq  negative                                                                 abilities              means choose                                                                     given derived define    labels  samples policy inaccurate labelers                                                                          probability vector   high mislabeling rate does occur practice smyth                                                                 case thatis column vec•   cases plotted policy boundary                                                                 tor  case combining                                                    fig    equations     fairly accurate labelers fig  shows                                                                             prefer  labels  samples policy   unless classifier error rate low hint practi•  tioners time spent cleaning labels effec•  example precisionrecall breakeven points   tive time spent labeling extra samples                    mentioned earlier benefit able recover                                                                 confusion matrix work preci•   obtaining true confusion matrix                             sion recall measures analyze following   evaluating classification systems need know   effects noisy labels measures reduce   just error rate cost different mis number variables examined look precision                                                                                                                 learning  recall breakeven points defined points precision   separately derive upper bound true error rate    recall equal simply denoted preci•     sionrecall given py precisionrecall confusion    matrix uniquely determined table  shows apparent   precisionrecall        different actual classifier precisionrecall labeler note assumption used deriving upper bound   precisionrecall table  assumesalthough                  limiting twoclass problems easily    values exactly verify bound exact mislabeling rate zero                                                                   bound exact apparent error rate zero                                                                  true error rate equal mislabeling rate     bounds true error rate                          looser assumption nonnegative dependency        classifier labeler independent                true error rate range prynoty ± piy   deriving true error rate eq  as•   example simulation nonnegative   sumption labeler classifier make errors in•         dependency classifier labeler   dependently argue assumption human        derivation lower bound achieved ex•  use different methodologies classify sam•      actly mislabeling rate small indepen•  ples algorithms inspired human reasoning   dence assumption true examine tight upper   neural networks learn human intuition  bound simulation taken pairs classes   avoid psychological biases harder imag• ucls optdigit dataset handwritten digit recog•  ine algorithms based abstract models support    nition dataset trained naive bayes classifier   vector machine err similar ways humans further•     nearestneighbor classifier duda et ai  training   application domains speech recognition  set each pair nearestneighbor classifier used   humans label samples based presentation   simulate labeler labeled testing set naive   object feature vector used classification   bayes classifier classifier evaluation   mathematical notions linear vector coefficients actual labels testing set mislabel•  little neurological basis application do•  ing rate nearestneighbor labeler true error   mains statistical text classification assumptions rate naive bayes classifier determined   blatantly violate human reasons as•  output naive bayes classifier nearestneighbor   sume words text independently generated  labelerare compared determine apparent error rate   grammatical way lastly make stronger argument      chosen optdigit dataset nearest  require training data labeled independently   neighbor algorithm know combination   testing data better perfectly labeled low error rate kaynak alpaydin    avoiding possibility learn biases  closely matching accuracy human labelers   bad habits training data cor•  fact pairs classes nearestneighbor algo•  relate labeling errors testing data               rithm zero error optdigit dataset interesting      reasoning indepen•     handwritten digit recognition task classical   dence assumption conceivable   example human labeling effort ap•  conservative assume nonnegative dependency          plied naive bayes classifier chosen                                                                  popular classifier sufficiently different nearest  probability classifier misclassifying sample neighbor interesting results   higher labeler mislabeled sample    table  shows results pairs classes   vice versa happen example training data nearestneighbor labeler nonzero error note   mislabeled way testing data  insignificant positive dependence naive bayes   classifier learned imitate mislabelings   classifier nearestneighbor labeler ex•  deliberately ignored case negative dependency     pected trained dataset use                                              hard    features assume independence features   pressed justification practice            explicitly naive bayes implicitly nearestneighbor      nonnegative dependency assumption easily incor•     through distance metric different   porated eq  changing equal sign lessthan  aspects naive bayes generative nearest  orequalto sign propagating change through deriva•  neighbor classifier discriminative naive bayes classi•  tion lower bound true error rate            fiers true error rate exactly upper bound                                                                  pairs   closer apparent error                                                                  rate pairs   simulation shows                                                                  upper bound tight nontrivial situations     second inequality tight mislabeling rate   discussion future work   small denominator inequality ap•  designing classification systems frequently   proximately                                               parameters learned automatically train      learning                                                                                                               
