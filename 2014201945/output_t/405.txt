           discriminative learning beamsearch heuristics planning                 yuehua xu                         alan fern                     sungwook yoon            school eecs                    school eecs           science  engineering        oregon state university           oregon state university           arizona state university          corvallis                corvallis                 tempe az        xuyueecsoregonstateedu        aferneecsoregonstateedu         sungwookyoonasuedu                        abstract                          ported successes substantial body                                                        work learning heuristics value functions control      consider problem learning heuristics search boyan moore  zhang dietterich      controlling forward statespace beam search ai  buro  virtually work focused      planning domains draw recent framework   search optimization problems problems involve ﬁnd      “structured output classiﬁcation” syntac ing “least cost” conﬁgurations combinatorial objects      tic parsing known learning search optimiza different ﬂavor types domains en      tion laso laso approach uses discrimi     countered benchmarks ai planning knowl      native learning optimize heuristic functions edge previous demonstrated      searchbased computation structured outputs benchmark domains ai planning      shown promising results number                                                                                         mains search problems arise   recent work yoon et al  progress      ai planning tend qualitatively different learning heuristics planning domains work focused      considered structured classiﬁcation improving heuristic used stateoftheart planner                                                                                        raises number potential difﬁculties di ff hoffmann nebel   particular approach      rectly applying laso planning paper used linear regression learn approximation differ      discuss issues lasobased ap ence ff’s heuristic observed distancesto      proach discriminative learning beamsearch goal states training plans primary contribution      heuristics ai planning domains conver work deﬁne generic knowledge representa      gence results approach present experi tion features featuressearch procedure allowed      ments benchmark domains results   learning good regression functions range plan      show discriminatively trained heuristic ning domains approach showed promising results      outperform used planner ff learning mechanism number potential shortcom      recent nondiscriminative learning approach ings importantly mechanism does consider                                                        actual search performance heuristic during learn                                                        ing learning based purely approximating    introduction                                       observed distancestogoal training data  number stateoftheart planners based learned heuristic performs poorly used search  old idea forward statespace heuristic search bonet learner makes attempt correct heuristic response  geffner  hoffmann nebel  nguyen et al paper consider learning approach tightly   success recent progress deﬁning couples learning actual search procedure iteratively  domainindependent heuristic functions work updating heuristic response observed search errors  range domains remain domains approach discriminative sense  heuristics deﬁcient leading planning fail tempts learn heuristic discriminates “good”  ure way improve applicability robustness “bad” states ﬁnd goal  planning systems develop learning mechanisms tempting precisely model distancetogoal  automatically tune heuristic particular domain based areas machine learning discriminative methods  prior planning experience work consider observed outperform nondiscriminative coun  applicability recent developments machine learning terparts main goal work demonstrate  problem particular given set solved planning beneﬁts context planning  problems target domain consider using discrim learning approach based recent framework  inative learning techniques acquiring domainspeciﬁc learning search optimization laso daume iii  heuristic controlling beam search                marcu  developed solve “structured    despite potential beneﬁts learning improve classiﬁcation” problems problems involve map  ward statespace planning heuristics ping structured inputs sentences structured outputs                                                    ijcai                                                    syntactic parses classiﬁcation posed small width beam search solutions hope  forming search candidate outputs guided heuris learned heuristic quickly solve new prob  tic laso provides approach discriminative learning lems practically solved prior learning  heuristics demonstrated good performance heuristic representation consider learning heuristic  structured classiﬁcation problems functions represented linear combinations fea  search problems corresponding structured classiﬁca tures hnΣiwi ·fin search node fi  tion qualitatively different typical ai feature search nodes wi weight feature fi  planning domains example structured classiﬁcation challenges approach deﬁning generic  search problems typically single small number feature space features selected weights  solution paths ai planning learned space rich capture im  large number equally good solutions given portant properties wide range domains  differences utility laso context clear amenable searching properties purpose    main contributions paper draw prior work yoon et al  deﬁned  lasoinspired algorithm learning beamsearch heuristics feature space based properties relaxed plans  prove convergence algorithm provide described search approach ﬁnding useful features  empirical evaluation number ai planning domains investigation use features work  empirical results show approach able learn addition using relaxedplan length heuristic  heuristics improve beamsearch compared using approach yoon et al  used simple weight  heuristic planner ff addition results show learning method weights tuned linear regres  discriminative learning appears advantage sion predict distancetogoal search nodes  existing nondiscriminative approach             training set approach showed promise obliv    follows ﬁrst problem setup learn ious actual performance heuristic used  ing planning heuristics overview search particular heuristic provides poor guid  laso framework structured classiﬁcation followed ance search training problems learning  description laso variant convergence analysis fi occur main objective work improve  nally present experiments conclude           formance investigating sophisticated weight learn                                                        ing mechanism tightly integrated search pro    learning planning heuristics                       cess iteratively adapting heuristic response observed                                                        search errors ﬁrst prior work struc  planning domains planning domain deﬁnes set tured classiﬁcation approach based  possible actions set states terms set adaptation setting  predicate symbols  action types  constants ca  state fact application predicate appropriate  learning heuristics structured  number constants state set state facts  each action ∈aconsists  action  classiﬁcation  action type applied appropriate number constants structured classiﬁcation problem learning map   set precondition state facts prea  sets state ping structured inputs structured outputs example  facts adda dela representing add delete ef problem partofspeech tagging goal learn  fects respectively usual action applicable state mapping word sequences sentences sequences  iff prea ⊆ application applicable action partofspeech tags recent progress structured classi  results new state s  dela ∪ adda ﬁcation includes methods based condition random ﬁelds    given planning domain planning problem tuple lafferty et al  perceptron updates collins   gwherea ⊆ais set actions ∈sis ini margin optimization taskar et al   tial state set state facts representing goal recent alternative approach daume iii marcu  solution plan planning problem sequence ac  views structured classiﬁcation search problem  tions aal sequential application se learns heuristic problem based training data  quence starting state leads goal state s ⊆ s particular given structured input problem labeling  paper view planning problems directed structured output treated searching through  graphs vertices represent states edges repre exponentially large set candidate outputs example  sent possible state transitions planning reduces graph partofspeech tagging sequence words  search path initial state goal     sequence word tags each node search space    learning plan focus learning heuristics pair y y partial labeling words  simple highly successful framework forward state learning corresponds inducing heuristic quickly  space search planning goal learn heuristics directs search search node  quickly solve problems using breadthﬁrst beam search sired output framework known learning search  small beam width given representative training optimization laso demonstrated stateoftheart  set problems planning domain approach ﬁrst formance number structured classiﬁcation problems  solves problems using potentially expensive search serves basis work  uses solutions learn heuristic guide laso assumes featurevector function                                                    ijcai                                                    fnfmn maps search nodes descriptive gorithm certain assumptions structure  features example partofspeech tagging features multiple good solutions relative target solution  indicators detect particular words la variant laso used planning  beled particular tags counts number times experiments variant based use breadthﬁrst  articletag followed nountag partial labeling                                                       beam search captured original laso   heuristic linear combination features useful context planning  hnf    · wwherew weight vector laso refer modiﬁed procedure laso∗  tempts select guides search target solution beam search breadthﬁrst beam search beam  directly integrating learning search process beam width generated each search step resulting  each training example laso conducts search guided beam nodes each step nodes cur  heuristic given current weights “search rent beam expanded children scored  error” weights updated avoid heuristic taken beam process  type error future process repeats continues goal node appears beam  convergence stopping conditions convergence results point solution plan beam width  stated daume iii marcu  certain small nodes search space pruned away  types weight updates                              resulting inability ﬁnd solution ﬁnding                                                        suboptimal solutions beam width increases    learning heuristics planning                   quality solutions tend improve                                                        time space complexity increases linearly beam  given success laso structured classiﬁcation width leading practical limitations goal work  interesting consider applications wider range learn domainspeciﬁc heuristic allows beam  search problems focus search ai plan search small replicate result using large  ning recall “learning plan” training set viewed form speedup learning  tains planning problems target solutions problem                                                          discriminative learning input learner set  viewed structured classiﬁcation training set                                                        xiyi pairs xi sg training problem  xiyi each xi sg planning problem                                                        target planning domain yi ssst   each ss   sequence states                                                    state sequence corresponding solution plan   solution plan  consider applying laso                                                                                                    training procedure attempt ﬁnd weights  learn heuristic guides forward statespace search                                                        each problem j’th state solution contained  ﬁnd solution each                                                     j’th beam search search error said occur    concept straightforward map planning case figure  gives pseudocode  laso framework obvious approach overall learning approach toplevel procedure  work search problems arising repeatedly cycles through training set passing each exam  ai planning different characteristics compared ple laso∗ arrive updated weights procedure  tackled laso far notably typi terminates weights remain unchanged cycling  cally large number good optimal solutions through examples user deﬁned stopping condition  given planning problem solutions dif                                ∗                                                          given training example xiyilaso conducts beam  ferent paths goal result simply reordering                             steps particular plan example blocks world search starting initial beam xi asin                                                        gle search node plan generating beam  particular state generally possible good        ∗                   actions does matter order various goal search xi sssj                                                        beam search error case update  towers constructed despite possibility good                   ∗  solutions laso attempt learn heuristic strictly weights way makes preferred heuris  prefers trainingset solutions equally good tic ideally remain beam time through  lutions training set raises potential search use weight updating rule similar                                                                                                        learning problem impossible solve dif ceptron update proposed daume iii marcu   ﬁcult good solutions xi                 „p                 «                                                                                     inherently identical yi cases simply clear   α · n∈b      − n∗  weights converge good solution                       approach overcoming difﬁculty  clude possible solutions training set  α≤  learning rate parameter  general practical enormous number feature vector search node current beam  possible good plans studying methods computing tuitively update rule moves weights direction  compact representations plan sets using decreases heuristic value increase preference  laso work continue use desired search node n∗ increases heuristic value  single target solutions evaluate algorithm nodes beam weight update beam  like original laso noting potential practical prob replaced single search node n∗ search contin  lems arise multiple solutions interestingly ues note each laso∗ guaranteed terminate  able derive convergence result al search steps generating training examples necessary                                                    ijcai                                                                                                                            heuristiclearn xiyib                        width bb using weights solve train    ←                                                                                  repeat unchanged large number iterations ing problems case corresponds        xiyi                               typical deﬁnition margin used original                ∗            laso xiyiwb                          laso target required ranked higher    return                                            nodes considering case b           ∗     laso ywb                                  show convergence cases “dominating”      planning problem sg                                                      weight vector exists weight vectors allow      solution trajectory ssst         search correctly solve training problems following      weight vector                                                 ∗     ←x  initial beam                     theorem shows laso uses large beam width     −                                  relative beam margin guaranteed converge         ← beamexpandb wb          ∗         ← ssj  desired node         ﬁnite number mistakes           ∗         ∈                          ∗            ← updatew                         theorem   exists weight vector                  ∗                                                                                ←n                                          beam marginb δδ training                                                        set beam width   δ   number     beamexpand                                                             δ      candidates ←                                                                                       ∈                                                        ∗                  br                                                        mistakes laso bounded                    candidates ← candidates∪ successorsn                                            δb−b −δb     ∈ candidates         hn ← ·  compute heuristic score proof sketch let wk weights kth mis     return nodes candidates lowest heuristic value                                                              suppose kth mis                                                        beam depth does       figure  discriminative learning algorithm                     ∗     ∗                             ∗                          tain target node   nij  using fact    convergence laso                                 ∈  wk · n∗ wk  · derive                                                                           prove certain assumptions laso∗ guar  wk   ≤ wk     induction implies                                                                    anteed converge ﬁnite number iterations set  wk   ≤ kr  using deﬁnition beam margin                                                                                               −       weights solves training examples particu derive · wk ≥ · wk  δ − δ                                                                                                      lar extend convergence results original laso                        −  −                                                         implies · wk ≥ δ δ  combin  case “multiple good solutions” proof simple                                                                                    ing inequalities noting    generalization used prove convergence         −  −                                                               w·w         √δ δ                                                         ≥       ≥            implying theorem  ceptron updates structured classiﬁcation collins  ww      kr    consider set training problems wherex                                                                        sg yi ssst  each xiyi denote notice  there a dominating weight      ∗                                                                                  nij    xi ssj node desired search                                                                                        vector mistake bound reduces δ  does  path depth example ialsoletdij set                                           ∗            depend beam width matches result stated  nodes reached search steps thatis                                                      daume iii marcu  behavior  dij set possible nodes beam                                                         bb case δ  δ use minimum  beam updates result let constant                                                             ∗                     beam width allowed theorem  bound  ∀i ∀n ∈ dij   n−f  ≤r                                          ij                           feature vector node  · denotes norm         factor    larger                                                            δ                             results stated terms existence bb result points tradeoff  weight vector achieves certain margin training mistake bound computational complexity laso∗  set use notion margin suited beam computational complexity each iteration  search framework meaningful creases linearly beam width mistake bound  weight vector ranks target solutions strictly best decreases beam width large agrees  solutions look just good bet                                                       intuition computation time willing  ter deﬁned beam margin triple δδ                                                       search planning time need learn  nonnegative integer δδ ≥   deﬁnition  beam margin weight vector beam                                                          experimental results  margin δδ onatrainingsetxiyi each                ⊆                                     present experiments ﬁve strips domains blocks  set dij dij size                                                        world pipesworld pipesworldwithtankage psr                                    ∗    ∀n ∈ dij − dij w·   − · nij  ≥ δ philosopher set time cutoff  cpu minutes                                         ∗          ∀n ∈ dij δ  w·  − · nij  ≥−δ  considered problem unsolved solution                                                        cutoff given set training problems gener                                  weight vector beam margin δδ each search ated solution trajectories running ff beam search                           ∗  depth ranks target node nij better nodes different beam widths taking best solution                                            margin δ ranks nodes bet training trajectory blocks world used          ∗  ter nij margin greater δ set features learned previous work yoon et al   condition satisﬁed guaranteed beam search fern et al  yoon  domains                                                    ijcai                                                                                                                                blocks world  used learned yoon et al  yoon       problems solved     average plan length                                                                             ∗                   ∗  cases include ff’s heuristic feature           len    laso   lr  len    laso   lr    used laso∗ learn weights learning rate                                                     ∗                                                            philosopher laso run  iterations                           learning beam width  domains                                   ∗                                                                               laso  run   iterations learn                                                                                                                 ing beam width  beam width did work                            philosopher learning times varied domains                                                                               pipesworld  depending number predicates actions         problems solved     average plan length                                                                             ∗                   ∗  length solution trajectories average time process len  laso  lr  len     laso   lr                                                                                    ing single problem single iteration  sec                    onds psr  seconds pipesworldwithtankage                                                                                                          seconds domains                                           domain details blocks world problems generated                                                                                                                 bwstates generator slaney thi´ebaux                             thirty problems   blocks used training               pipesworldwithtankage                                                                     problems solved    average plan length  data  problems    blocks used             ∗                    ∗                                                               len   laso   lr  len     laso   lr  testing  features domain including ff’s                        relaxplanlength heuristic domains taken                                                                                                              fourth international planning computation ipc                           each domain included   problems roughly ordered                                                                                                                     difﬁculty used ﬁrst  problems training                          remaining problems testing including ff’s relaxed                            planlength heuristic  features pipesworld                  psr                                                                      problems solved    average plan length   features pipesworldwithtankage  features psr                 ∗                   ∗                                                               len    laso   lr  len    laso   lr   features philosopher                                                                                                                                                        performance beam sizes figure  gives                           formance beam search each domain various beam                            widths columns correspond algorithms len                                                                                                                 beam search using ff’s relaxedplanlength heuristic                              beam search using heuristic uniform weights                                         ∗  features laso  beam search using heuristic learned                philosopher          ∗                                                           problems solved    average plan length                                                                             ∗                   ∗  ing laso  learning beam width speciﬁed     len    laso   lr  len    laso   lr  lr  beam search using heuristic learned linear                                                                                                                  gression yoon et al  each row corre                        sponds beam width shows number solved test                                                                                                                problems average plan length solved problems                          general algorithms beam width                         increases number solved problems increases solu                           tion lengths improve point number  solved problems typically decreases behavior typ figure  experimental results ﬁve planning domains  ical beam search beam width increases ∗  greater chance pruning solution trajectory laso signiﬁcantly improves blocks world  computational time memory demands increase pipesworld pipesworldwithtankage especially                                                        blocks world does solve problem psr  ﬁxed time cutoff expect decrease performance   ∗    laso∗  versus learning compared len laso∗   laso  improves beam width                                                         worse philosopher discussion  tended signiﬁcantly improve performance beam                           ∗  search especially small beam widths—eg blocks results show laso able improve  world beam width  laso∗ solves twice prob stateoftheart heuristic len majority                                                        domains learning beneﬁcial compared uniform weights  lems len average plan length reduced                               ∗  signiﬁcantly small beam widths beam width general best performance laso achieved  increases gap laso∗ len decreases small beam widths close used training                                                                          ∗  laso∗ solves problems comparable solution comparing laso  linear regression com  quality pipesworld laso∗ best performance pare prior nondiscriminative heuristic learning work  beam width  solving  problems len learned weights using linear regression yoon et  beam width increases performance gap decreases al  utilizing weka linear regression tool  laso∗ consistently solves problems len sults resulting learned linearregression heuristics  trends similar domains psr shown columns labeled lr  len solves slightly laso∗ large beam widths blocks world lr solves fewer problems laso∗                                                    ijcai                                                    
