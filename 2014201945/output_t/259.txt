                             updates nonlinear discriminants         edin andelic´   martin schaffoner¨    marcel katz    sven kruger¨    andreas wendemuth                                      cognitive systems group iesk                                       ottovonguerickeuniversity                                           magdeburg germany                                 edinandelicetechnikunimagdeburgde                        abstract                          mika  proposed method closely                                                        lated kfd lssvm proposed      novel training algorithm nonlinear discrimi method imposes sparsity solution greedy fashion      nants classiﬁcation regression reproduc using subset selection like billings lee nair et      ing kernel hilbert spaces rkhss presented   al  svm case similar greedy approaches exist      shown overdetermined linear  cauwenberghs ma et al       squaresproblem corresponding rkhs       especially case large data sets subset selection      solved greedy forward selection scheme practical method aims eliminate irrelevant      updating pseudoinverse orderrecursive redundant samples ﬁnding best subset ﬁxed      way described construction pseudoin  size nphard combinatorial search problem      verse gives rise update orthogonal restricted suboptimal search strategies forward selec      composition reduced gram matrix linear  tion starts training set adds sequentially      time regularization spirit ridge regres sample relevant according certain cri      sion easily applied orthogonal terion mean square error nair et al       space various experiments classiﬁcation external algorithm based elementary givens      regression performed show competi rotations used update qrdecomposition      tiveness proposed method                  duced gram matrix order construct sparse models                                                        gram schmidt orthogonalization used billings lee                                                         chen et al  orthogonal decomposi    introduction                                       tion gram matrix apply forward selection  models regression classiﬁcation enforce square second step obtain sparse models method known  loss functions closely related fisher discriminants orthogonal squares ols ols al  duda hart  fisher discriminants bayes gorithm requires computation storage  optimal case classiﬁcation normally distributed gram matrix prohibitive large datasets  classes equally structured covariance matrices duda paper simple efﬁcient way construct  hart mika  contrast svms ing lsms rkhs forward selection rule  squares models lsms sparse general lower memory requirements presented pro  cause overﬁtting supervised learning scenario posed method exploits positive deﬁniteness gram  way circumvent problem incorporate regular matrix orderrecursive update pseudoin  ization controlled continuous parameter model verse reveals best knowledge novel kind  instance ridge regression rifkin et al  penalizes update rule orthogonal decomposition solution  norm solution yielding ﬂat directions rkhs regularized second stage using generalized cross  robust outliers caused noise validation reestimate regularization parameter  suykens vandewalle  leastsquares svms ls  remainder paper organized follows  svms introduced closely related gaussian section  computationally efﬁcient update rules pseu  processes fisher discriminants linear set equa doinverse orthogonal decomposition derived  tions dual space solved using conjugate section  shown solution regularized  gradient methods large data sets direct method section  experimental results regression classi  small number data solution pruned kruif ﬁcation datasets presented finally conclusion given  vries hoegaerts et al  second section   stage close relation lssvm ker  nel fisher discriminant kfd shown van gestel et  update orthogonal decomposition  al  follows equivalence kfd supervised learning problem faced training  squares regression labels duda hart data set  xiyii xi denotes                                                    ijcai                                                     input vector ﬁxed size yi corresponding target noting pseudoinverse vector given                                           −   value contained regression                           qt  binary classiﬁcation assumed xi  xj            q†                                                                                                             focus sparse approximations models form                      qm                       yˆ  kα                      equation  written                                                                               †                         ·                                    − km−k       use mercer kernels   mercer  gives rise  α                    m−                                                       ˆm                                           symmetric positive deﬁnite gram matrix ele                   qm  ments kij  kxi xj deﬁning subspace rkhs                                                                   kt  −    k†    −    k†      learning takes place weight vector α                    m−  m−       m−   m−                                                                                           ααm  contains bias term corresponding                      qm  column    gram matrix                                                        matrix    consider overdetermined leastsquaresproblem                                   †                                                                       pm   − km−km−                                                                   αˆm argminkmαm     − y                                 α                                 orthogonal projection matrix implies sym                                                      metric idempotent equation  simpliﬁes  mth forward selection iteration reduced gram                                 m×m                                    α     q†  matrix km    km  ∈          ki                         ˆm                                             · · xm i∈                                denotes pre  combining   current weight vector αˆm  viously unselected column gram matrix updated  note reduced weight vector αm  ααm∈                                        rm                                                α         k†    − k†     q†        target vector               α      ˆm−        m−     m−                                                             ˆm     α                 †                 generalized inverses pseudoinverse                   ˆm               qm                   †          −                  km  kmkm      km                 revealing update                                                                                               lowest frobenius norm benisrael            k†    − k†    q†                                                                    k†       m−     m−                                                                                  †                    greville   corresponding solution                                qm                      α     k†                      ˆm                          current pseudoinverse                                                                                     lowest euclidean norm                          projection    lies subspace                    α                                 orthogonal km− follows immediately    partitioning form                                                                            qi qj    orthogonal decomposi                 km   km−km                      tion                                                                                               αm   αm−αm                                                                                                                      reduced gram matrix given orthogonal matrix  setting αm  αm  const square loss                                                                         qm  qm−qm                    lαm−αmkm−αm−       − − kmαm                                                           upper triangular matrix  minimum  leastsquaressense given                                                           †                                                um−         −                αˆm−  km−y  − kmαm                      um           qmqm     qmkm                                                                                m−  inserting   yields                                                        mth iteration omm operations required                       †                       †                                                      lαmi−km−km−kmαm−i−km−km−y              updates note inversion matrix qmqm                                                     trivial matrix diagonal condition  denoting identity matrix appropriate size number matrix qm increases number selected    note vector                                columns grows ensure numerical stability                   −    k†                      important monitor condition number matrix                      m−   m−              terminate iteration condition number exceeds  residual corresponding leastsquares regression predeﬁned value unless stopping criterion reached  km qm nullvector km earlier  nullvector unless strictly positive deﬁnite ensure  strictly positive deﬁniteness mandatory add  regularization selection basis  small positive constant ε main diagonal gram centers  matrix form →  εi forward selection  performed using strictly positive deﬁnite gram matrix goal forward selection scheme select  following km   assumed                  columns gram matrix provide greatest    minimum  met                        duction residual methods like basis matching pur                                                        suit mallat zhang  orderrecursive matching pur                     †             †              αˆm  qmi − km−km−y              suit natarajan  probabilistic approaches smola                                                    ijcai                                                     sch¨olkopf  contributions issue minimizing gcv respect λ gives rise  nair et al  forward selection performed simply estimation formula λ alternative way obtain  choosing column corresponds entry reestimation λ maximize bayesian evidence  highest absolute value current residual reasoning mackay   residual provides direction maximum differentiating  respect λ setting result  decrease cost function αt kα − αty zero gives minimum  gram matrix strictly positive deﬁnite method                                                                 ∂p˜                    ∂     p˜  used following experiments note                       trace                                                           p˜      tracep˜ p˜              rived algorithm applied     ∂λ                          ∂λ  ward selection rules following refer pro noting  posed method orderrecursive orthogonal squares                                                                  ∂p˜   orols                                                  yt p˜        λαt  qt     λi   −α    consider residual                                         ∂λ     ˜        ˜                    e˜m  − yˆm  − qmα˜            equation  rearranged obtain reestimation                                                        formula  mth iteration vector α˜m contains orthogonal                                                                                          weights                                                             ∂tracep˜ m∂λy p˜                                                             λ                                           regularized square residual given                                          −                                                                  tracep˜ mα˜ mqmqm  λim  α˜                                              e˜m     e˜me˜m  λα˜ mα˜                                                                                                                                                  ∂     p˜           qt                       p˜                                      trace                                                                                                                                                                                       ∂λ           λ  qi  λ denotes regularization paramter minimum                             given                                      forward selection stopped λ stops changing sig         p˜       −   qt     λi  −qt             niﬁcantly                                      computational cost update om oro                             qt                  p˜    −                          los algortihm summarized pseudocode algorithm                    m−                                 λ  qmqm  current residual corresponding regularized  experiments  squares problem updated               show usefulness proposed method empirically                                                        experiments regression classiﬁcation                                 qt                    p˜    −                     formed experiments gaussian kernel             ˜m  m−                                              λ  qmqm                                                                                                                                                                                                                                     x −                                   qm                                exp   −                        −                                                                                         ˜m−                                                      σ                               λ  qmqm  orthogonal weights                                used kernel parameter σ optimized using fold                                                        crossvalidation experiments classiﬁcation ex                       yt             α                 ≤ ≤              periments onevsrest approach used obtain multi             ˜ mi                                               λ  qi qi                          class classiﬁcation hypothesis  computed forward selection stopped  classiﬁcation  original weights recovered                                                        classiﬁcation  wellknown benchmark datasets                            −                    αˆ  um α˜m                   chosen usps dataset contains  pixel values hand                                                        written digits training testing instances  whichisaneasyinversionsinceum  upper triangular                                                          letter dataset contains  labeled samples    each iteration chooses qi corresponds                                                        character images based different fonts each  highest absolute value current residual adds                                                                         letter fonts randomly distorted produce  qm− possible determine number basis func                                                        dataset unique stimuli dataset predeﬁned  tions using crossvalidation use instance                                                        split training testing exist used ﬁrst  bayesian information criterion minimum description                                                                                                   instances training remaining instances  length alternative stopping criteria following gu                                                                                       testing  wahba  orr  possible use gen                                                          optdigits database digits handwritten turkish  eralized cross validation gcv  stopping criterion                                                        writers contains digits written writers train  summarize results details orr                                                                                   ing set generated ﬁrst writers digits writ    gcv  given                                                                                                                     remaining independent writers serve testing                                                               p˜ my                    stances database generated scanning pro           gcv                                                                        cessing forms obtain  ×  matrices                          tracep˜             reduced  ×                                                     ijcai                                                     algorithm  orderrecursive orthogonal squares  orols                                                           data set   sv    orols  require training data labels ykernel                         usps                                                                                 letter                          λ ←     ←          k†         initializations                                    optdigits             iopt                         pendigits                                                                           satimage            λ changes signiﬁcantly qm illconditioned                                                     table  test errors   benchmark datasets       update e˜m                                       vsrest approach used average fraction selected basis                                                        centers  parantheses       ﬁnd index iopt entry e˜m highest    absolute value                                                          regression       iopt ←ioptiopt                                                        regression ﬁrst perform experiments synthetic       ← iopt                                    dataset based function sincxsinxx ∈                                                        −  corrupted gaussian noise training         compute kiopt                                    testing instances chosen randomly using uniform         compute qiopt                                    distribution interval results illustrated       km  ←  km−kopt                                ﬁgures  table                                                           additionally real world datasets boston       qm  ←  qm−qopt                                abalone available uci machine learning                 †                                        repository chosen hyperparameters optimized       update km um  using kopt qopt            fold crossvalidation procedure datasets ran                                                        dom  partitions mother data training testing       update λ                                         generated   partitions   instances                                                        training   testing boston       ←                                          abalone dataset respectively continuous features                                                        rescaled zero mean unit variance abalone    end                                           boston gender encoding male  female infant    return αˆ miopt                                    abalone dataset mapped                                                                 mean squared error mse orols compared                                                        forward selection algorithm based qrdecomposition    pendigits contains penbased handwritten digits digits gram matrix nair et al  results table  written touchsensitive tablet  show mse improved signiﬁcantly orols  resampled normalized temporal sequence contrast orols qr method uses external algo  pairs coordinates predeﬁned test set formed rithm reorthogonalization update orthog  entirely written digits produced independent writers onal decomposition observed update scheme    satimage dataset generated landsat multi best knowledge novel update needs  spectral scanner image data each pattern contains  pixel reorthogonalized long gram matrix  values number indicating classes rank improvement accuracy rea  central pixel                                        son good performance orols furthermore    caracteristics datasets summarized table  noted best performance orols  results seen table  especially optdig boston dataset quite favourable compared best                                                                                ±       pendigits datasets orols appears signiﬁcantly formance svms mse     sch¨olkopf smola                                                              superior compared svms performance    maining  datasets comparable svms                                                                            method   rmse                                                                           svm           data set    classes   training   testing                                                                           rvm           usps                                                                                                    orols          letter                             optdigits                                                                             table  average rmse sinc experiment       pendigits                                                                                  satimage                                 randomly generated points used training  testing                                                        standard deviation gaussian noise  runs   table  datasets used classiﬁcation experiments results avereged  runs                                                      ijcai                                                                                                                                                  training points                                     basis centers                                             true function                                                estimated function                                                                                                                                                                                                                 rmse                                                                                                                                                                                                                −        −                                                            − −  − −  −                                                                                                                                 noise standard deviation    figure  example ﬁt noisy sinc function using    figure  rmse ﬁts noisy sinc function dif  randomly generated points training  testing standard ferent noise levels    randomly generated points  deviation gaussian noise  root mean square used training  testing results avereged   error rmse  case  points selected runs each noise level  basis centers                                                        pseudoinverse updated orderrecursively reveals                                                     current orthogonal decomposition reduced gram                                                      matrix forward selection scheme generalized                                                        cross validation serves effective stopping criterion                                                              allows adapt regularization parameter each iteration                                                    extensive empirical studies using synthetic realworld                                                      benchmark datasets classiﬁcation regression suggest                                                        proposed method able construct models               rmse                                              competitive generalization ability advantage                                                    proposed method compared svms simplic                                                    ity sparsity achieved computationally efﬁcient way                                                        constuction better controlled                                                              svm case optimization problem solved fur                                                    thermore contrast svms orols allows easy incor                                                       poration multiple kernels kernel parameters                                            number training data          varied different training instances order obtain                                                        ﬂexible learning machines possibility examined                                                        interesting direction future work  figure  rmse ﬁts noisy sinc function dif step future work development  ferent training set sizes  randomly generated points proposed algorithm tasks like dimensionality reduction  used testing standard deviation gaussian noise onlinelearning   runs results avereged  runs  each size                                                        references           dataset       qr          orols                benisrael greville  benisrael          boston    ±   ±               greville generalized inverses theory applications         abalone    ±  ±             wiley   table  mean square error mse standard deviations billings lee  billings lee non  boston abalone dataset using different methods linear ﬁsher discriminant analysis using minimum  average fraction selected basis centers  paran squared error cost function orthogonal  theses                                                  squares algorithm neural networks –                                                         chen et al  schencfncowanandpm  conclusion                                              grant orthogonal squares learning radial ba  computationally efﬁcient training algorithm orthogo sis function networks ieee transactions neural net  nal squares models using mercer kernels presented works –                                                     ijcai                                                     
