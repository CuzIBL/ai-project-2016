                   logistic regression models fast cbir method                                     based feature selection                            ksantinidzioubcolin dubeau                                          university sherbrooke                                      science department                               riadhksantini djemelziouusherbrookeca                                         mathematic department                             bernardcolin francoisdubeauusherbrookeca                        abstract                          information relative relevances featurebase                                                        feature vectors noise vectors dis      distance measures like euclidean distance tance measures fail irrelevant features hurt      widely used measure similarities trieval performance probabilistic approaches promis      feature vectors contentbased image ing solution cbir problem compared      retrieval cbir systems sim  standard cbir methods based distance measures      ilarity measures assumption  lead signiﬁcant gain retrieval accuracy fact      probability distributions local relevances approaches capable generating probabilistic sim      feature vectors irrelevant fea ilarity measures highly customized metrics comput      tures hurt retrieval performance probabilis ing image similarity based consideration distinc      tic approaches proven effective solu tion relative feature vector relevances previ      tion cbir problem paper use  ous works based probabilistic approaches peng et      bayesian logistic regression model order al  used binary classiﬁcation classify database      compute weights pseudometric improve color image feature vectors relevant irrelevant cae      discriminatory capacity increase im nen pauwels  used classical quadratic logistic      age retrieval accuracy pseudometric weights regression model order classify database image feature      adjusted classical logistic regression vectors relevant irrelevant aksoy et al  used      model ksantini et al  bayesian                                                        weighted distances order measure simi      logistic regression model shown sig  larity degree images aksoy haralick      niﬁcantly better tool classical logistic  measure similarity degree query image      gression improve retrieval performance database image using likelihood ratio derived      retrieval method fast based feature bayesian classiﬁer      selection experimental results reported paper investigate effectiveness bayesian      zubud wang color image databases proposed     logistic regression model based variational method                                  deselaers et al                        order adjust weights pseudometric used ksan                                                        tini et al  improve discriminatory ca    introduction                                       pacity increase image retrieval accuracy pseudo                                                        metric makes use compressed quantized versions  rapid expansion internet wide use                 digital data real world applications ﬁeld daubechies wavelet decomposed feature vectors  medecine security communications commerce acad weights adjusted classical logistic regression  emia increased need efﬁcient image database show thanks variational method used  creation retrieval procedures reason content bayesian logistic regression model signiﬁcantly better  based image retrieval cbir approach proposed tool classical logistic regression model compute  approach each image database associated pseudometric weights improve querying  feature vector capturing certain visual features image sults retrieval method fast efﬁcient based  color texture shape similarity measure feature selection evaluation retrieval method  used compare feature vectors ﬁnd similar ing models separately performed using precision                                                                                                   ities images assumption images scope curves deﬁned kherﬁ ziou                                                            section brieﬂy deﬁne pseudometric  close each feature space visu    ally similar distance measures like euclidean distance section  brieﬂy pseudometric weight com  widely used feature vector compar putation using classical logistic regression model  ison cbir systems similarity mea showing limitations bayesian  sures based distances feature vec logistic regression model appropriate pseudo  tors feature space lack metric weight computation bayesian lo                                                    ijcai                                                                                                                                                     ˜  gistic regression model data feature vectors intended similar xi  training performed models feature selection absolute value difference scaling factors  based image retrieval method feature vectors used daubechies wavelets decomposed compressed                                                                                                      j−  represent database images presented section fi quantized versions feature vectors xkik  nally section  perform experiments val numbers mismatches resolution level  idate bayesian logistic regression model use coefﬁcients suppose pairs  precision scope order show advantage similar feature vectors pairs dissimilar ones  bayesian logistic regression model classical logistic class Ω contains explanatory vectors                                                                                             regression terms querying results         associated binary target variables xi si repre                                                        sent pairs similar feature vectors class Ω    pseudometric                                  contains explanatory vectors associated binary                                                                       ir  ir  given query feature vector featurebase db target variables xj sj represent pairs                                                      dissimilar feature vectors pseudometric weights w˜  feature vectors tk   db having  components                                                              j−  each aim retrieve featurebase similar wk intercept chosen optimize  feature vectors achieve db feature following conditional loglikelihood  vectors daubechies wavelets decomposed compressed                      n         n                                                                                                  ir  coefﬁcients each quantized measure l˜ww  wj−v logpi   logpj    similarity degree target feature vector tk                             featurebase use onedimensional version                                                                   ir  pseudometric used ksantini et al  given pi pj relevance irrelevance probabili  following expression                              ties respectively given                                     ˜  ˜− ˜ −             ˜c  ˜c                              j−   tk       tk            wbini qq  tkq                       ˜                                                                         pi     −w˜xi −   wkxki −                           iq˜ci                                                                                                                                                                                                                      j−                                                          ir           ˜ ir          ir                                                            pj      ˜wxj   wkxkj                                    ˜c     ˜c          ˜c     ˜c           qqitkqi                                             qqitkqi                                                                                       ex                                                         ex logistic function rea  ˜        ˜                                    ˜c  tk scaling factors tk qqi son standard optimization algorithms fisher scoring      ˜c                                                gradient ascent algorithms clogg et al   tkqi represent ith coefﬁcients daubechies                                                        invoked cases especially   wavelets decomposed compressed coefﬁcients                                                        exponential likelihood function  quantized versions w˜ wbini’s weights                                                      existence zero explanatory vectors maximum  compute bucketing function bin groups lat likelihood fail estimates parameters  ters according resolution levels    weights intercept optimal ex                                              binilogi         −     ist boundary parameter space                                                        complete quasicomplete separation Ω                                                            Ω    weight computation                               function arbitrarily large standard                                                        optimization algorithms diverge krishnapuram et al   order improve discriminatory power pseudo Ω Ω large highdimensional                                       j−  metric compute weights w˜ wkk using clas standard optimization algorithms high computa  sical logistic regression model bayesian logistic regres tional complexity long time converge ﬁrst  sion model separately deﬁne classes relevance problems solved smoothing parameter  class denoted Ω irrelevance class denoted Ω estimates assuming certain prior distribution  order classify feature vector pairs similar dis parameters reducing parameter space  similar basic principle using bayesian logistic problem solved using variational trans  gression model classical logistic regression formations simplify computation parameter  allow good linear separation Ω Ωandthen estimates jaakkola jordan  mo  compute weights represent local relevances tivates adoption bayesian logistic regression model  pseudometric components                      based variational methods    classical logistic regression model            bayesian logistic regression model  model each feature vector pair represented ex bayesian logistic regression framework three  planatory vector binary target variable speciﬁcally main components chosen prior distribution  ith feature vector pair associate explanatory vec parameters likelihood function pos           ˜                        tor xi xixi  xj−i  ∈ × binary terior distribution three components formally com  target si   depending bined bayes’ rule posterior distribution contains                                                    ijcai                                                                                                                                                                                                           ∝                                 available knowledge parameters          i qi π  model priors having different distributional                                                         forms gaussian prior advantage having low com                           putational intensity smoothing parameter estimates                                                                                    i qi  ﬁxed mean away unreasonable extremes                                                   likelihood function conjugate                                                                                                                   eq −                                                                           −     ϕ  h−  gaussian prior posterior distribution tractable                    qi   form mean computation involves highdimensional    i                                       integration high computational cost according    jaakkola jordan  it’s possible use accurate eq eq expectations respect                                                                                                     i  variational transformations order approximate like                                 tanh                                                                                                                                                 distributions  respectively ϕ  i    lihood function simpler tractable exponential form    case thanks conjugacy gaussian prior dis i variational parameters ap  tribution parameters combined proximation posterior distribution considered  likelihood approximation obtain closed gaussian form adjustable lower bound proper gaussian distribu  approximation posterior distribution tion posterior mean μpost covariance matrix Σpost  number observations large number variational estimated following bayesian update equa  parameters updated optimize posterior distribution ap tions  proximation large computational cost                                                                                                   high bayesian logistic regression model pro −         −                                                                            Σpost    Σ           ϕieqi xixi     pose use variational transformations jensen’s                  equality order approximate likelihood function                                                                                                                                              tractable exponential form explanatory vectors      Σ       Σ−         −        observed instead distributed according speciﬁc μpost   post      μ          eqi xi     distributions posterior distribution approximated                      gaussian depends variational pa weight intercept computation algorithm  rameters computation posterior distribution ap phases ﬁrst phase initialization  proximation mean fast low computational com gaussian prior πw second phase iterative  plexity model denote random vectors allows computation Σpost μpost through                                         rn  realizations represent explanatory vectors xi bayesian update equations   respectively               Ω                            irn  relevance class  explanatory vectors xj using em type algorithm jaakkola and  jordan                   Ω        ˜                                                              irrelevance class byx   xj− order ﬁnd variational parameters i each iter            ˜   xj−  respectively sup ation optimal approximation posterior dis                                                        tribution initialization phase chosen  pose ∼ qx ∼ qxwhereq                                                        model Ω Ω respectively absence  chosen distributions associate binary                                                        prior knowledge weights intercept πw  random variable realizations target vari                                                   chosen univariate zero mean large variances  ables si iandforx   associate binary ran                                                        gdon  values μpost components desired  dom variable realizations target variables                                       j−    ir                                                                                ˜          sj jwesets   equal  similarity set estimates pseudometric weights wk  equal  dissimilarity parameters weights intercept parameters posterior distrib                                                        ution approximation computed magnitude given  intercept considered random variables   noted random vector ˜ww  wj−vwe term i close   assume ∼ πwwhereπ   gaussian prior Ω Ω linearly separated quasi separated tends  prior mean μ covariance matrix Σ using bayes’ rule  Ω Ω overlapped  posterior distribution given         analogically classical logistic regression model                                                                                                                                                        term  characteristics i  ws    ˆ                                            ˜        caenen pauwels  terms used                                 ix  xi wqix  xi πw  perform feature selection retrieval method     x∈Ωx∈Ω                                                                                                       training  si  ixi  xi wf − xi each  ∈  using variational approximation jaakkola let consider color image database consists sev  jordan  jensen’s inequality posterior distri eral color image sets each set contains color images  bution approximated follows                     perceptually close each terms object                                                        shapes colors order compute pseudometric  ws                                      weights intercept classical logistic regression                                                                                               Ω                                                 model create relevance class  ir         ws    i     qi    πw          relevance class Ω create Ω draw possible pairs      ≥                                                                         feature vectors representing color images belonging                                                    ijcai                                                    database color image sets each pair com  similarity degrees query color image  pute explanatory vector associate database color images represented  binary target variable equal  similarly create Ωwe sulted array totalscoresuchastotalscorei                                                            n  draw possible pairs feature vectors representing color γlscoreli each ∈  dbwhere  images belonging different database color image sets                                                             γll weightfactors used downweight fea  each pair compute explanatory vector asso                                                                                                      ture low discriminatory power γl   ciate binary target variable equal forthe                                           Ω      Ω         weights computed by classical logis  bayesian logistic regression model create                                      way instead associating binary target tic regression model γl i                                                            weights computed bayesian logistic regres  variable value each explanatory vector Ω Ωwe                                                            sion model  associate binary target variable equal  Ω ex  planatory vectors associate binary target variable  organize database color images order increas  equal  Ω explanatory vectors                 ing resulted similarity degrees array totalscore                                                            negative resulted similarity degrees corre    color image retrieval method                           spond closest target images query image                                                            finally return user closest target color images  querying method phases ﬁrst phase                                                            query color image number denoted  preprocessing phase entire database                                                            ri chosen user  taining db color images second phase querying  phase                                                  used feature vectors    color image database preprocessing               order luminance colors edges  preprocessing phase data color image use luminance histogram weighted                                                        tograms image texture description performed kur  base color images querying general case                                 ×  following steps                                  tosis skewness histograms given pixel lab                                                        color image luminance histogram hl contains number    choose feature vectors comparison          pixels luminance written follows                                       ∈            compute  feature vectors tli                          m− n−      each ith color image database ∈                            −                                                                hl            δ il                   db                                                            feature vectors representing database color      images daubechies wavelets decomposed com each ∈  whereil luminance image                                                                                          pressed coefﬁcients each quantized      δ kronecker symbol  weighted histograms                                                        color histogram constructed edge region elimina    organize decomposed compressed quantized                                    Θl      Θl         tion multispectral gradient module mean histogram      feature  vectors search arrays  −     given        used optimize pseudmetric                                                                 m−  n−                                 computation process ksantini et al                                                                                               −                                                        j−         hk           δ ik  χη λmax      adjustment metric weights w˜ wkk             each featurebase tli   db representing      database color images ∈       given                                                                                         querying algorithm                                             ¯e    hk                                                                          hk                         querying algorithm general case fol                  npk  lowing steps                                         npkc number edge region pixels    given query color image denote feature vectors  deﬁned      representing query image ql              m− n−                                feature vectors representing query image npkc        δiki − cχη∞ λmaxi       daubechies wavelets decomposed compressed                                                                                                                        coefﬁcients each quantized                                                                         similarity degrees ql                                                                  database color image feature vectors tli hk                               n   db  represented arrays m− n−                                                                                     scorel      scorel qltli             δiki − cλmaxi χη∞ λmaxi       each ∈  db arrays are returned                                          procedure retrievalql Θ Θ−                                                     respectively procedure retrieval used optimize each ∈    bwhereλmax represents      querying process ksantini et al      multispectral gradient module ksantini et al  η                                                    ijcai                                                    threshold deﬁned mean multispectral gradient follows truncated poisson distribution greatest  modules computed image pixels ia ib realization best ﬁt analogically make  images chrominances redgreen yellowblue choice  xj− assume                                                                       ˜  respectively χ characteristic function multi random variable realizations positive reals  spectral gradient module mean histogram provides informa follows gaussian mixture distribution  tion overall contrast chrominance     ˜                                                        choice generally carry evaluation  edge region elimination allows avoidance overlappings image retrieval ﬁeld principal issues required  noises color histogram populations caused acquisition ground truth deﬁnition performance  edge pixels lab color image kurtosis skewness criteria ground truth use human observations  histograms given                               fact three external persons participate evalu                  m− n−                             ation concerning performance criteria represent            κ                κ           hk        δik −         evaluation results precisionscope curve pr  fri                                                scope ri number images returned                                                    user each querying performed evaluation experi                  m− n−                             ment each human subject asked goodness score                          −                  each retrieved image goodness score  retrieved           hk           δ ik                                                                                             image similar query retrieved image                                                        fairly similar query  similarity  respectively each ∈    bwhere   κ  κ      κ                                          retrieved image query precision  il ia ib kurtosis images luminance                                                                           computed follows pr sum goodness scores  chrominances respectively il ia                                                                                                  retrieved imagesri curve pr ri  ib skewness images obtained gives precision different values ri lie  local computations kurtosis skewness values   perform querying evaluation  luminance chrominance image pixels linear wang database lie    interpolation used represent kurtosis skewness perform querying evaluation zubud database  values   each used feature vector human subjects perform different queryings  histogram having  components set equal  evaluation experiment compute average precision  following section                                    each value ri construct precisionscope                                                        curve evaluation experiment each color image    experimental results                               wang zubud databases represented                                                                                   ¯e  ¯e  κ   κ  κ                                                          histograms hl ha hb  ha hb hl ha hb  hl  section discuss choices distributions  order validate bayesian logistic regression ha hb order evaluate querying wang  model image retrieval context finally use database each human subject asked formulate query  precision scope deﬁned kherﬁ ziou database execute querying using weights   evaluate querying method using models computed classical logistic regression model  separately choices distributions goodness score each retrieved image  querying evaluation conducted wang reformulate query database execute  zubud color image databases proposed deselaers et al querying using weights computed bayesian logistic   wang database contains db   color   regression model goodness score each  images selected manually form  sets retrieved image each human subject performs querying  africa beach ruins food  images each zurich ﬁfty times choosing new query database  building image database zubud contains training each time repeat experience different orders  db   color images query  color compression ∈   evaluate querying  images training consists  building image sets zubud database each human subject asked follow  each set contains  color images building preceding steps formulating queries  taken different positions feature vector database query wang zubud databases  extractions represent wang zubud database resulted precisionscope curves given figure   color images perceptually uniform lab color space compression orders ∈   figure   each color image zubud wang     illustrates retrieval examples zubud database  databases extract histograms given  comparing performances regression models       respectively each database  each example query located topleft  represented featurebases choices dialog box  separately performed each featurebase each                          ˜  featurebase assume  xj−                                             ˜  independent make assumption   xj− suppose random  vector  xj− random variables realiza  tions positive integers independent each                                                    ijcai                                                    
