    weighted polynomial information gain kernel resolving prepositional                phrase attachment ambiguities support vector machines                               bram vanschoenwinkel bernard manderick          vrije universiteit brussel department science computational modeling lab                                      pleinlaan   brussel belgium                                    bvschoen®  bernardartivubacbe                          abstract                          nagao  program usually rely                                                          kind knowledge       introduce new kernel support vector ma•     problem tackled using memory      chine learning natural language setting based learning like example knearest neighbours       case study incorporate domain knowledge    training examples stored memory classi•      kernel consider problem resolving     fication new example based closest ex•      prepositional phrase attachment ambiguities   ample stored memory needs function       new kernel derived distance function     expresses distance similarity examples       proved succesful memorybased learn• exist dedicated distance functions       ing start simple overlap metric  solve kind natural language problems using memory      derive simple overlap kernel ex•    based learning veenstra et al  zavrel et al        tend information gain weighting finally  daelemans et al        combine polynomial kernel increase   use support vector machine svm tackle       dimensionality feature space clo•  problem pp attachment disambiguation central svm       sure properties kernels guarantee result learning kernel function         kernel kernel achieves high clas• contains examples kernel calculates inner       sification accuracy efficient time product second space feature space product       space usage compare results ob• expresses similar examples       tained memorybased learning meth•      goal combine power svms dis•      ods make clear proposed kernel      tance functions arc wellsuited probem       achieves higher classification accuracy         designed deriving distance kernel                                                          straightforward section  deriving kernel    introduction                                         distance trivial kernels satisfy                                                          extra conditions kernel stronger con•  important issue natural language analysis resolu• dition distance paper   tion structural ambiguity sentence said struc• method shows dedicated distance functions   turally ambiguous assigned used basis designing kernels sequentially   syntactic structure zavrel et al   prepositional used svm learning   phrase pp attachment wants disambiguate use pp attachment problem case study illus•  cases uncertain pp attaches verb trate approach starting point overlap   noun                                        metric succesfully used memorybased learn•  example  consider following sentences        ing problem zavrel et al                                                             section  short overview theory svms      bought shirt pockets                  theorems definitions needed       washed shirt soap                     section  based zavrel et al  section  gives                                                          overview metrics developed memorybased learning     sentence  modifies noun shirt                                                          applied pp attachment problem section  new   pockets pp describes shirt sentence                                                           kernels introduced finally sections     modifies verb washed soap pp describes                                                          experimental results conclusion work   shirt washed ratnaparkhi      type attachment ambiguity easy people  support vector machines   resolve use world knowledge stetina                                                          simplicity explanation consider case      author funded doctoral grant institute advance• binary classification consider input space   ment scientific technological research flanders wt input vectors target space          casebased reasoning                                                                                     goal svm assign classes      decision boundary separates in•   vectors belonging different classes usually arbi•   trary — dimensional manifold input space   dimensional                                                                     properties kernels                                                                      dont know exact form features                                                                  used kernel expresses prior knowledge                                                                  patterns modelled encoded similarity                                                                  measure vectors brown et al                                                                     maps kernels kernel                                                                  related inner product cfr definition                                                                  satisfy conditions arise naturally defini•                                                                 tion inner product given mercers theorem                                                                  map continuous positive definite vapnik                                                                                                                                      paper use following methods construct                                                                  kernels icristianini shawetaylor       svms sidestep difficulties vapnik                                                                   making kernels kernels based fact ker•  overfitting avoided choosing unique maximum mar•                                                                      nels satisfy number closure properties case   gin hyperplane possible hyperplanes sep•                                                                      mercer conditions follow naturally closure   arate data hyperplane maximizes distance                                                                       properties kernels   closest data points                                                                  making kernels features start features                                                                       input vectors obtain kernel working                                                                       inner product feature component in•                                                                      vector case mercer conditions follow                                                                       naturally definition inner product          precise chosen kernel   represent maximal margin hyperplane decision   boundary linear equation       convex quadratic objective function linear constraints   cti prove zero definition   metrics memorybased learning   vectors xi corresponding nonzero called sup•  port vectors sv set consists data points section focus distance functions zavrel   lie closest hyperplane difficult et al  cost salzberg  used memorybased   classify                                                      learning symbolic values look                                                                  simple overlap metric som discuss      order classify new point xnew determines   sign                                                        information gain weighting gw memorybased learning                                                                  class machine learning techniques training in•                                                                 stances stored memory classification new                                                                  instances later based distance similarity be•    sign positive xnew belongs class  negative tween new instance closest training instances   class  zero xnew lies decision boundary note     stored memory wellknown example   restricted summation set sv     memorybased learning knearest neighbours classifica•  support vectors zero          tion literature offers                                                                                                  casebased reasoning  books articles provide comprehensive intro•       kernel natural language settings    duction memorybased learning mitchell     purpose important thing remember    working symbolic values like strings alphabet    characters instances ndimensional vectors         simple overlap metric    basic metric vectors symbolic values    simple overlap metricsom zavrel et         dsom ab distance vectors   represented features distance feature   kncarest neighbour algorithm equipped metric    called ib aha et al  ib algorithm simply   counts number mismatching feature values   vectors reasonable choice information   importance different features   information feature importance add   linguistic bias weight different features    information gain weighting   information gain weighting gw measures fea•  ture separately information contributes   knowledge correct class label information gain   ig feature measured calculating entropy be•  tween cases knowledge value   feature                                                                 dont proof kernel ksok satisfies mer•                                                                cer conditions follows naturally defini•                                                                tion inner product started features                                                                  inner product section  how•                                                                show kernel really corresponds distance                                                                 function given equations   verify dis•                                                                tance formula kernels given section        weights wi used extend equation  weights   equation  knearest neigbour algorithm equipped   metric called bg zavrel et al    corresponding distance called dig       casebased reasoning                                                                                                  extending sok dimensions                                                                      polynomial information gain kernel                                                                 section increase dimensionality fea•                                                                ture space making use polynomial kernel kpoly                                                                 start section giving example cristianini                                                                 shawetaylor    dont proof kernel ksok valid   kernel follows naturally definition   inner product show   kernel corresponds distance function   dsom equation  following show       does impose problems kernel   aiming develop    adding information gain sok                                                                                                 casebased reasoning                                                                          experimental setup                                                                           experiments libsvm cc                                                                           java library svms chihchung chijen                                                                            machine used pentium iii mb ram                                                                           memory running windows xp choose implement                                                                             kernels kig kpig java   closure properties kernels follows naturally            type svm learning used csvm chih  kpig valid kernel calculates in•             chung chijen  parameter controls   ner product vectors transformed feature mapping             model complexity versus model accuracy                                                                           chosen based complexity problem                                                                              experiments section conducted simpli•                                                                          fied version pp attachment problem example                                                                            data consists fourtuples words extracted                                                                           wall street journal treebank marcus et al                                                                              group ibm ratnaparkhi et al                                                                                data set contains  training patterns  test                                                                           patterns independant validation set  patterns                                                                           parameter optimization models described   experimental results                                                  low trained training examples results                                                                           given  test patterns benchmark compari•                                                                          son models literature use results                                                                           parameters optimized valida•                                                                          tion set details concerning data set refer                                                                           zavrel et al                                                                              results      pp attachment disambiguation problem   uncertain sentence preposition attaches   verb noun prepositional phrase   pp attachment problem example sentence  ex•  ample  modifies noun shirt pockets   pp describes shirt contrast sentence  mod•  ifies verb washed soap pp describes   shirt washed ratnaparkhi          fact words   importance pp attachment problem   verbnounprepositionnoun       case sentences reduced quadruples   illustrated example  human performance approx•  imately  ratnaparkhi et al  performance                                                                              fact kernel kig performs worse ibig   rate gives acceptable upper limit maximum                 equipped distance metric   performance unreasonable                 surprising believe   expect algorithm perform better hu•                svms perform linear separation feature space   man show experimental results kernel             decision boundary ibig hand nonlinear   kig achieves classification accuracy  sec•            linearity decision boundary svm   tion  zavrel et al   bg attains         points misclassified number misclassifica  maximum classification accuracy  good              tions controlled parameter choosing larger   indication classification accuracy possi•  ble obtain kernel based distance defined   equation        casebased reasoning                                                                                                                    
