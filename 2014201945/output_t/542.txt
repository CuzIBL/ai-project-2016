                bayesian approach imitation reinforcement learning                                   bob price                                       craig boutilier                    university british columbia                           university toronto                  vancouver bc canada vt                       toronto canada ms                            pricecsubcca                                  ceblycs toronto edu                               abstract                               communication channel sufficiently expressive represen•                                                                 tation language transformation possibly different         multiagent environments forms social learn•                                                                 agent bodies incentive communicate dynamic         ing teaching imitation shown                                                                  competitive domains webbased trading unreal•       aid transfer knowledge experts                                                                  istic expect agents designed compatible rep•        learners reinforcement learning rl re•          resentations altruistic intentions observationbased tech•       cast problem imitation bayesian frame        niques learning agent observes outward        work bayesian imitation model allows               behaviors agent reduce need explicit         learner smoothly pool prior knowledge data ob•       communication implicit communication through passive ob•       tained through interaction environment          servations implemented implicit imitation price        information inferred observations ex•         boutilier   model effects        pert agent behaviors model integrates           agents action choices state environment        recent bayesian exploration techniques          observed internal state agents ac•       readily generalized new settings               tion control signals observable independent explo•                                                                 ration observer used adapt knowledge       introduction                                              implicit observations agents learning agents                                                                  needs unlike classic imitation models learner    reinforcement learning flexible computationally                                                                  required explicitly duplicate behavior agents    challenging paradigm recent results demonstrating un•                                                                   paper recast implicit imitation bayesian    der certain assumptions sample complexity reinforce•                                                                 framework new formulation offers advantages    ment learning polynomial number problem states                                                                  existing models provides principled    kearns singh  tempered sober fact                                                                  elegant approach smooth pooling infor•   number states generally exponential number                                                                  mation agents prior beliefs experience    attributes defining learning problem recent                                                                 observations agents eliminates need   terest building interacting autonomous agents reinforce                                                                 certain ad hoc tuning parameters current imitation mod•  ment learning increasingly applied multiagent tasks                                                                  els second integrates stateoftheart explo•   development adds complexity learning                                                                  ration techniques bayesian exploration finally    littman  hu wellman  paper ex•                                                                 bayesian imitation model extended readily partially   amine multiagent reinforcement learning assump•                                                                 observable domains derivation implementa•   tion agents environment merely ar•                                                                 tion considerably complex reported   bitrary actors actors like agents   similar action capabilities similar objectives   assumption radically changes optimal learning strat•   background   egy information agents like     assume reinforcement learning rl agent learning    learning agent additional information capabili• control markov decision processes mdp     ties capabilities relate objectives finite state action sets sao reward function   number techniques developed exploit in•  dynamics dynamics refers   cluding imitation demiris hayes  mataric     set transition distributions actions   learning watching kuniyoshi et al  teaching     rewards subscripted distinguish   programming demonstration atkeson schaal         agents assume   behavioral cloning sammut et al  inverse rein•    agent knows dynamics mdp   forcement learning ng russell                      adopt automatic programming perspective      learning observation agents intuitive ap•   objective maximizing discounted reward infi•  peal explicit communication action capabil•    nite horizon number rl techniques used   ities agents requires considerable infrastructure   learn optimal policy focus model                                                                                                   multiagent systems based rl methods observer maintains esti•  mated mdp based set experiences             obtained far each stage suitable inter•  vals mdp solved exactly approximately using   techniques prioritized sweeping moore atkeson       known focus learning dynamics      bayesian methods modelbased rl allow agents in•      figure  dependencies model evidence sources   corporate priors explore optimally general em•  ploy prior density possible dynamics update nehaniv dautenhahn  wc as•  each data point letting                   sume observability mentors state space   denote current state history observer    assume observer identify actions taken                  action history use poste•       mentor—it simply observes state transitions   rior update action qvalues                    make additional assumptions regarding men•  used turn select actions formulation dearden     tors dynamics mentor implements stationary pol•  et al  renders update tractable assuming con•    icy induces markov chain    venient prior product local independent densi•    pr  each action taken mentor   ties each transition distribution each den•            exists action distributions   sity dirichlet parameters model                             assump•           require parameter each possible            tion homogeneous action assumption implies     successor state  update dirichlet straightforward    observer duplicate mentors policy con•  given prior data vector sequence treat dynamics   number observed transitions   agents note assume learner knows   posterior given parameters poste•               priori actions duplicates mentors   rior eq  factored posteriors local families given state observer wants duplicate pol•                                                                 icy agents different objectives                                                                 learner observe mentors transitions   subset history composed transitions         actions directly form estimates   state action updates   mentors markov chain estimates mdp   simple dirichlet parameter updates                            transition probabilities reward function price      bayesian approach advantages     boutilier  estimate used augment normal   approaches modelbased rl allows natural in• bellman backup treating observed distribution prs   corporation priors transition reward parameters   model action available observer imitators using   second approximations optimal bayesian exploration     augmented backups based observations mentor   advantage approach specific structural as• learn quickly especially mentors re•  sumptions prior discussed dearden et al  ward function parts policy overlap ob•                                                                 server techniques like interval estimation kaelbling     bayesian imitation                                           used suppress augmented backups value                                                                  low confidence   multiagent settings observations agents      bayesian approach observer incorporates obser•  used addition prior beliefs personal experience   vations mentor directly augmented model   improve agents model environment obser•      environment let hm denote history mentor state tran•  vations enormous impact provide infor•      sitions observed learner repre                                                                 sents observers state action history respectively fig  mation agent parts state space                                                                  ure  illustrates sources information available im  visited information used bias exploration to•   itator constrain beliefs   wards promising regions state space    probabilistic dependence observer knows   reduce exploration costs speed convergence dramatically   action history direct knowledge actions     flexibility bayesian formulation leads ele• taken mentor best weak prior   gant principled mechanism incorporating obser•   knowledge mentors policy learners be•  vations agents model updates following price    liefs updated wrt joint observations   boutilier  assume agents knowledgeable men•  tor naive observer acting simultaneously in•    dependently fixed environment like observer   mentor controlling mdp   underlying state space dynamics ac•                                                                    homogeneous action assumption relaxed price   tion dynamics identical assump•                  boutilier  essentially observer hypothesizes viola  tion agents state space criti• tions repaired using local search short sequence   cal important analogical mapping   actions roughly duplicates short subsequence mentors                                                                  actions repair observer discards mentor       assume agents performing noninteracting tasks influence point state space       multiagent systems                                                                                                                                                                   independent distributions each s—      assume prior pd factored dirichlet    update factored history elements    form described mentor influence learner     state ones relevant computing posterior    maintain posterior factored form updating  difficulty evaluating in•   each component model independently unfor•           tegral models following dearden et al  tackle    tunately complications arise unobservability sampling models estimate quantity specif•   mentors actions show model update     ically sample models factored dirichlet    eq  factored convenient terms         given specific sample      derive factored update model describ•            parameter vector  observed counts  likelihood    ing dynamics state action considering    cases case mentors unknown action    different action case model factor    independent mentors history em•  ploy standard bayesian update eq  regard   mentor case mentor action fact       combine expression expected model fac•  observers action mentor observations   tor probability eq  expression mentor policy   relevant update                                    likelihood eq  obtain tractable algorithm updating                                                                  observers beliefs dynamics model based                                                                    experience observations mentor                                                                    bayesian imitator proceeds follows each                                                                  stage observes state transition men•                                                                 tor using each update density models just de•                                                                 scribed efficient methods used update agents value      let prior parameter vector                                                                   function using updated value function selects suit•       denote counts observer transitions state   action counts mentor transitions         able action executes repeats cycle   state posterior augmented model factor density       like rl agent imitator requires suitable explo•  dirichlet parame•                             ration mechanism bayesian exploration model dear  ters simply update sum                    den et al  uncertainty effects actions   observer mentor counts                             captured dirichlet used estimate distribu                                                                   tion possible qvalues each stateaction pair                                                                 tions value information used approx•                                                                 imate optimal exploration policy method compu•                                                                 tationally demanding total reward including reward cap•     observer does know mentors action   compute expectation wrt cases                tured during training usually better provided                                                                  heuristic techniques bayesian exploration eliminates                                                                  parameter tuning required methods like greedy                                                                  adapts locally instantly evidence facts makes                                                                  good candidate combine imitation      allows factored update usual conjugate form   experiments   mentor counts distributed actions                                                                  section attempt empirically characterize   weighted posterior probability mentors policy                                                                  applicability expected benefits bayesian imitation   chooses action state       mechanism calculate posterior men•   through experiments using domains liter•  tors policy eq  provides complete factored update rule ature unique domains compare bayesian imi•  incorporating evidence observed mentors bayesian     tation nonbayesian imitation price boutilier    modelbased rl agent tackle problem—that      standard modelbased rl nonimitating tech  updating beliefs mentors policy—we        niques including bayesian exploration prioritized sweeping                                                                  complete bellman backups investigate                                                                  bayesian exploration combines imitation                                                                    agents used experiments                                                                  oracle employs fixed policy optimized each domain                                                                       sampling efficient local model needs resam    assume prior mentors policy fac• pled time step   tored way prior models—that        scaling techniques used hmms re•                                                                 quired prevent underflow term eq       assumes observers actions equiv• qvalue distribution changes little each update   alent mentors model generalized het• repaired efficiently using prioritized sweeping fact   erogeneous case additional term required represent bayesian learner cheaper run bellman backup                                                   states                                                                                                    multiagent systems                                                               providing baseline source expert behavior                                                                 observers egbs agent combines greedy ex•                                                                ploration bellman backup sweep                                                                 each time step provides example generic model                                                                based approach learning egps agent modelbased                                                                 rl agent using egreedy exploration prioritized                                                                 sweeping ps egps use fewer backups applies                                                                 predicted good egps does                                                                 fixed backup policy propagate value multiple                                                                 steps state space situations egbs                                                                 agent employs bayesian exploration                                                                 prioritized sweeping backups bebi combines bayesian                                                                 exploration bayesian imitation bi egbi com                                                                bines cgreedy exploration bayesian imitation bl                  figure  flagworld domain                     egnb agent combines egreedy exploration non                                                                bayesian imitation                                                                   each experiment agents begin start state                                                                 agents interact state space agent                                                                 achieves goal reset beginning agents                                                                 continue unaffected each agent fixed number steps                                                                 spread varying numbers runs each                                                                 experiment each domain agents given locally uniform                                                                 priors action equal probability resulting                                                                 local neighbouring states grid world                                                                  neighbours imitators observe expert oracle                                                                 agent concurrently exploration results re•                                                                ported total reward collected  steps                                                                 sliding window integrates rewards obtained agent                                                                 making easier compare performance various agents                                                                 during  steps integration window starts                                                                 causing oracles plot jump zero optimal                                                                  steps bayesian agents use  sampled mdps                                                                 estimating qvalue distributions  samples esti•                                                                mating mentor policy dirichlet distribution ex•                                                                ploration rates egreedy agents tuned each exper•             figure  flag world results  runs             imental domain                                                                   test agents loop chain                                                                 examples designed show benefits bayesian explo•                                                                ration taken dearden et al  experi                                                                ments imitation agents performed identically                                                                 optimal oracle agent separation seen                                                                 imitators                                                                   using challenging flagworld domain dearden                                                                 et al  meaningful differences performance                                                                 agents flagworld shown figure                                                                  agent starts state searches goal state gl                                                                 agent pick three flags visiting states fl                                                                 reaching goal state agent receives                                                                  point each flag collected each action nesw suc•                                                                ceeds probability  corresponding direction                                                                 clear probability  moves agent perpendicu•                                                                lar desired direction figure  shows reward col•                                                                lected preceding  steps each agent or•                                                                acle demonstrates optimal performance bayesian imita•                                                                tor using bayesian exploration bebi achieves quickest                                                                 convergence optimal solution egreedy bayesian           figure  flag world moved goal  runs             imitator egbi able exploit informa•                                                                tion locally bebi nonbayesian imitator                                                                nbi does better unassisted agents early fails       multiagent systems                                                                                                                                                                  optimal policy domain slower ex                                                                 ploration rate decay allow agent opti•                                                                 mal policy hurt early performance                                                                  nonimitating bayesian explorer fares poorly compared                                                                  bayesian imitators outperforms remaining agents                                                                  exploits prior knowledge connectivity do•                                                                 main agents show poor performance                                                                  high exploration rates converge eventu•                                                                 ally conclude bayesian imitation makes best use                                                                  information available agents particularly                                                                  combined bayesian exploration                                                                    altered flag world domain mentor                                                                  learners different objectives goal expert ora•                                                                 cle remained location learners goal loca•                                                                tion figure  figure  shows transfer imita•                                                                tion qualitatively similar case identical rewards                                                                 imitation transfer robust modest differences                                                                  mentor imitator objectives readily explained           figure  tutoring domain results  runs           fact mentors policy provides model information                                                                 states domain employed                                                                 observer achieve goals                                                                    tutoring domain requires agents schedule presen•                                                                tation simple patterns human learners order min•                                                                imize training time simplify experiments                                                                 agents teach simulated student students perfor                                                                mance modeled independent discretely approximated                                                                 exponential forgetting curves each concept agents                                                                 action choice concept present agent re•                                                                ceives reward students forgetting rate                                                                 reduced predefined threshold concepts pre                                                                senting concept lowers forgetting rate leaving unpre                                                                sented increases forgetting rate model simple                                                                 serve realistic cognitive model student provides                                                                 qualitatively different problem tackle note                                                                 action space grows linearly number concepts                                                                 state space exponentially                   figure  nosouth domain                        results presented figure  based presen•                                                                tation  concepts student egbs left                                                                 timeconsuming generally fares poorly                                                                 imitators learn quickly bayesian imita•                                                                tors bebi egbi outperforming egnbi converges                                                                 suboptimal policy generic bayesian agent                                                                 chooses suboptimal solution occurs                                                                 agents priors prevent adequate exploration                                                                 imitation mitigates drawbacks bayesian                                                                 exploration mentor observations used overcome                                                                 misleading priors bayesian imitation                                                                 applied practical problems factored state                                                                 action spaces nongeometric structure                                                                    domain provides insight combina•                                                                tion bayesian imitation bayesian exploration                                                                 grid world figure  agents south                                                                 column domain optimal oracle agent proceeds                                                                 south corner east goal                                                                 bayesian explorer chooses path based prior                                                                 beliefs space completely connected agent              figure  south results  runs                                                                     increasing exploration allows egnbi optimal policy                                                                 depresses short term performance                                                                                                    multiagent systems 
