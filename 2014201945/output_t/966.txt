              theoretical framework ensemble classification                                                    alexander seewald                                 austrian research institute artificial intelligence                                         freyung vi wien austria                                           alexseeoefaiat alexseewaldat                               abstract        ensemble learning schemes adaboost        bagging enhance performance single clas•       sifier combining predictions multiple clas•       sifiers type predictions        ensemble diverse classifiers combined        related ways voting simply se•       lecting best classifier crossvalidation         technique widely used machine learning how•       ensemble scheme best        choice deeper insight structure mean•       ingful approaches combine predictions needed        achieve progress paper offer        operational reformulation common ensemble        learning schemes  voting selection crossvali                                                                figure  illustration stacking dataset three classes        dation xval grading bagging  stacking                                                                 examples tv base classifiers refers        scheme appropriate parameter settings        theoretical point view schemes       probability given classifier class example number        reduced stacking appropriate        combination method result important          class probdist classifier unknown test in•       step general theoretical framework         stance fixed arbitrary order class values base        field ensemble learning                          classifiers assumed classk true class instance                                                                           attribute vector instance num•   introduction                                                ber instances indices zerobased instance                                                                 id satisfies equation  —    apologize space restrictions arc forcing terse   detailed version seewald    explain stacking works order lay founda•  tions functional definitions meta classifiers later   fig  shows stacking hypothetical dataset     during training base classifiers arc evaluated cross mentioned assume ensemble learning   validation original dataset each classifiers output schemes return class probability distributions predictions   class probability distribution example fig needed position maximum class probability   concatenated class probability distributions base  vector  predicted class  easily obtained   classifiers followed class value forms metalevel formula  trivially stacking predictions   training set stackings meta classifier fig simulated variant simply transforming class   training meta classifier base classifiers retrained distributions metadataset predictions  prior ap•  complete training data                                plying meta classifier     testing base classifiers queried class characterize ensemble learning scheme   probability distributions form metaexample  features extracts metadataset during   meta classifier outputs final class prediction     training features define mapping                                                                 metadataset final class probability distribution    definitions                                                  voting   arbitrary training dataset examples classes  voting straightforward extension voting distribution   single test instance given arbitrary base classifiers classifiers simplest case during training features   crossvalidated dataset re• extracted metadataset fact voting does   trained dataset base classifiers output class need internal crossvalidation during testing voting   probability distributions estimated probabilities each determines final class probability distribution follows   class instead deciding single class     refers probability given classifier   class example number during class prob•                                                                   ability distribution classifier instance refers       poster papers                                                                                                       easily seen metaclassifier defined  discussion  conclusion   just computing mean probability distribution base   definition stackingc seewald  mapped   classifiers   simulates voting probability dis• stacking special meta classifier fact avail•  tributions voting predictions mapped similarity  able implementation specialized subclass common   case use instead formula           meta classifier mlr recent variant smm dzeroski   vector                                 zenko  implemented special meta                                                                  classifier amenable kind mapping                                                               adaboost freund schapire                                                                   simulated stacking sequential nature    xval                                                        given formal definitions meta classifiers   xval chooses best base classifier each fold mainly intended enable theoretical work direct   internal tenfold cv determine accuracy    implementation feasible cost penalty sim•  classifier computed directly meta  ulation small training meta classifiers usually   level dataset  derive feature id contributes little total runtime   classifier maximum accuracy value         concluding shown stacking equivalent   bestc corresponds learned model                       common ensemble learning schemes selection                                                                  crossvalidation xval voting class probability dis•                                                             tributions predictions competitive variant grading                                                                 bagging recent variants stackingc seewald                                                               smm dzeroski zenko  arc equiv•                                                                alent conclude approach offers unique                                                                 viewpoint stacking important step                                                                 theoretical framework ensemble learning                                                              grading                                                   acknowledgements research supported austrian   grading seewald furnkranz  case dif•   fonds zur forderung der wissenschafdichen forschung fwf un•  ficult simulate original grading     der grant pinf austrian research institute arti•  simulate competitive variant equivalent accuracy   ficial intelligence supported austrian federal ministry   weighted voting details seewald             education science culture     during training accuracies base classifiers ex•  tracted using formula  accuracies base clas• references   sifiers correspond learned model during testing      breiman  breiman  bagging predictors   build combined class probability distribution similar     machine learning  pp    voting using predictions additional weight   accuracy extracted earlier                     dzeroski zenko  dzeroski zenko                                                                     combining classifiers better selecting best                                                                 proceedings th international confer•                                                                   ence machine learning cml morgan kauf                                                                   mann publishers san francisco                                                                  freund schapire  freund schapire    bagging                                                       experiments new boosting algorithm   bagging breiman  common ensemble tech•         proceedings international conference machine   nique type classifier repeatedly trained   learning icml pages  morgan kaufmann   new datasets generated orig•     seewald furnkranz  seewald ak furnkranz   inal dataset random sampling replacement after•         evaluation grading classifiers hoff•  wards component classifiers combined simple un•     mann et al eds advances intelligent data analy•  weighted voting                                                 sis th international conference ida  proceedings     use metaclassifier voting pre•      springer berlin pp    dictions number base classifiers equal iter•  ation parameter bagging  each base classifier stack•  seewald  seewald ak  make stack•  ing corresponds instantiation base learner     ing better faster taking care un•  bagging order simulate random sampling       known weakness proceedings th international   training set base learners training sets modi•  conference machine learning icml morgan   fied training formula                             kaufmann publishers san francisco                                                               seewald  seewald ak  theoreti•                                                                   cal framework ensemble classification extended ver•  during training formula  used create  each base                                                                    sion technical report austrian research institute   classifier separately  training set size                                                                    artificial intelligence vienna tr   original training set random sampling original   training set exactly bagging training sets  ting witten  ting wittcn     used train base classifiers approach  issues stacked generalization journal artificial intel•  seen probabilistic wrapper each base classifier  ligence research      features extracted metalevel dataset during  wolpert  wolpert  stacked generaliza•  training voting                                         tion neural networks        during testing each base classifier gives prediction   predictions voted yield final prediction   exactly voting predictions  modified     details refer subsection  concluding   shown equivalence bagging stacking                                                                                                        poster papers 
