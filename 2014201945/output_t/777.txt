              document summarization using conditional random fields                    dou shen jiantao sun hua li qiang yang zheng chen                             department science engineering                       hong kong university science technology hong kong                                         dshen qyangcseusthk                             microsoft research asia  zhichun road china                                   jtsun huli zhengcmicrosoftcom                        abstract                          document abstractsummary employ words                                                        phrases appear original document mani      methods including supervised unsuper    summarization task categorized ei      vised algorithms developed extrac ther generic queryoriented queryoriented summary      tive document summarization supervised      presents information relevant given      methods consider summarization task queries generic summary gives overall sense      class classiﬁcation problem classify each sen document’s content goldstein et al  addition      tence individually leveraging relation single document summarization ﬁrst stud      ship sentences unsupervised methods    ied ﬁeld years researchers started work      use heuristic rules select informative multidocument summarization goal generate      sentences summary directly hard summary multiple documents cover similar infor      generalize paper present   mation paper focus generic singledocument      ditional random fields crf based framework      sentence extraction forms basis summa      merits kinds ap  rization tasks hot research topic yeh et al       proaches avoiding disadvantages mihalcea       proposed framework      comes previous methods features seam     past extractive summarizers based      lessly integrate key idea approach scoring sentences source document based set      treat summarization task sequence la predeﬁned features mani bloedorn  fea      beling problem view each document  tures include linguistic features statistical features      sequence sentences summarization pro  location rhetorical structure marcu  presence      cedure labels sentences   label absence certain syntactic features pollock zamora      sentence depends assignment labels  presence proper names statistical measures term      compared proposed approach      prominence luhn  similarity sentences      existing methods open benchmark  measures prominence certain semantic concepts      data set results show approach  lationships gong liu  kinds approaches      improve performance      designed leverage features supervised       best supervised baseline unsu unsupervised supervised approaches kupiec et      pervised baseline respectively terms pop al  yeh et al  summarization seen      ular metrics rouge detailed analysis    class classiﬁcation problem sentences treated      improvement presented          dividually observe individual treatment                                                        sentences advantage relationship                                                        sentences example intuitively neigh    introduction                                       boring sentences similar contents  document summarization attracted attention summary treated individually  original work luhn luhn  formation lost sequential learning systems hid  wideranging applications especially explosion den markov models applied  documents internet main role help fully exploit rich linguistic features mentioned  ing readers catch main points long document assume independence features  effort helpful preprocessing step tractability conroy o’leary   text mining tasks document classiﬁcation shen et al hand unsupervised approaches rely heuristic rules                                                  difﬁcult generalize ideal develop    document summarization categorized machine learning method based training corpus doc  different dimensions abstractbased extractbased uments advantage intersentence  extractsummary consists sentences extracted relationship rich features dependent                                                    ijcai                                                      paper tackle extractive summarization prob ture space small taking special assumptions  lem different manner approaches present open problems mccallum et al  firstly  summarization task sequence labeling problem feature space large features  instead simple classiﬁcation problem individual sen dependent overlapping appearance train  tences approach each document considered ing process intractable approach  sequence sentences objective extractive summa fully exploit potential useful features  rization label sentences sequence  mentioned summarization task com   label  indicates sentence sum putational inefﬁciency secondly approaches set  mary sentence  denotes nonsummary sentence hmm parameters maximize likelihood ob  label sentence expected impact labels servation sequence doing approach fails predict  sentences nearby accomplish task sequence labels given observation sequences  apply conditional random ﬁeld crf lafferty et al  situations inappropriately use generative joint  paper stateoftheart sequence labeling model order solve discriminative conditional problem  method crf provide framework leverag observations given work paper aimed  ing features complex overlap solving problems crf  ping independent fully incorporate unsupervised methods developed doc  knowledge intuition extractive summarization ument summarization exploiting different features  troducing proper features effectively lationships sentences mentioned  framework ensemble outcomes summariza rhetorical structures marcu  lexical chains barzi  tion methods uniﬁed way designing features lay elbadad  hidden topics documents  crfbased approach carries summarization task gong liu  graphs based similarity  discriminative manner conditioning la sentences mihalcea  methods based  bel sequence sentence sequence maximize features require extra resources efforts  likelihood global label sequence max achieve better performances compared methods  imize consistency different labels se shown gong liu  mihalcea   quence result approach overcomes fore review works com  disadvantages previous supervised unsupervised pare approach experiments  approaches experimental results open benchmark gong liu  authors observed hidden  data set duc httpducnistgov show pro topics discovered document projec  posed approach improve performance compared tion each sentence each topic through latent semantic  stateoftheart summarization approaches        analysis deerwester et al  selected sen                                                        tences large projections salient topics                                                        form summary mihalcea’s work mihalcea     related work                                       constructed graph each node sentence  supervised extractive summarization approaches treat weight edge linking nodes simi  summarization task twoclass classiﬁcation problem larity corresponding sentences direction  sentence level summary sentences posi edges decided appearance order  tive samples nonsummary sentences negative sentences constructing graph employed  samples representing each sentence vector fea graphbased ranking algorithms like hits kleinberg   tures classiﬁcation function trained differ pagerank brin page  decide importance  ent manners discriminative way wellknown vertex sentence account global  algorithms support vector machines svm yeh et information recursively computed entire graph  al  classiﬁers effective previous work considered reduce  sume sentences independent classify each dundancy summary typical method based cri  sentence individually leveraging relation teria maximal marginal relevance mmr carbonell et  sentences hidden markov model based methods attempt al  according mmr sentence chosen  break assumption conroy o’leary clusion summary maximally similar  conroy et al’s work kinds states document dissimilar alreadyselected sentences  kind corresponds summary states approach works ad hoc manner tends se  corresponds nonsummary states observations lect long sentences paper redundancy  sentences represented vector three features controlled probabilistic model learned  given training data statetransition probabilities automatically  statespeciﬁc observation probabilities estimated  baumwelch algorithm em algorithm rabiner  crfbased summarization approach   given new document probability sentence  corresponds summary state calculated finally  motivation  trained model used select likely sum intuition comes observations humans  mary sentences approaches handle summarize document posing problem sequence  positional dependence feature dependence fea labeling problem document regarded sequence                                                    ijcai                                                    sentences partitioned segments parameters estimation  each segment relatively coherent content order let Λλkμl set weights crf  generate summary good coverage low redun model Λ usually estimated maximum  likeli  dancy need select representative sentence each hood procedure maximizing conditional log  segment read document likelihood labeled sequences training data Ψ  beginning end judge informativeness each xyxn yn  deﬁned  sentence reading encounter sentence                 informative summary         lΛ        logpΛyjxj            reading sentences encountering better ones            jn  cision previous sentence changed  procedure summarization kind sequence labeling avoid overﬁtting regularization methods em  goal produce label sequence corresponding ployed peng mccallum  common method  sentence sequence label  denoting summary add gaussian prior parameters  sentences  denoting nonsummary sentences                                                                                                                                      λk       μl                                                          lΛ        logpΛyjxj −        −              informativeness easily measured                               σ       σ  directly machines fortunately sentences char jn                          acterized features lengths positions      article terms contain judgment cri σk σl variances gaussian priors  teria learned groundtruth samples generated various methods used optimize lΛ including  people words given sequence sentences rep erative scaling algorithms gis iis lafferty et al  resented certain features goal label sentences  quasinewton method  likelihood label sequence given lbfgs converges signiﬁcantly faster sha pereira  sentence sequence maximized paper use crf  malouf  paper use  tool model sequence labeling problem    bfgs                                                        inference    conditional random fields                        given conditional probability state sequence  random variable data sequences labeled ﬁned crf  parameters Λ probable  random variable corresponding label sequences labeling sequence obtained   conditional random fields crf provide probabilistic           ∗                                                                         argmaxy pΛy              framework calculating probability globally  ditioned lafferty et al  efﬁciently calculated viterbi algo  natural graph structure paper use common rithm rabiner  marginal probability states  specialcase structure linear chain suitable se each position sequence computed dynamic  quence labeling assume onetoone programming inference procedure similar forward  correspondence states labels stateslabels backward procedure hmm lafferty et al  problem summary sentence nonsummary sen deﬁne “forward values” αiyx setting αyx  tence given observation sequence sentence sequence equal probability starting state iterate  xxt   corresponding state se follows                                                                            quence yyt  probability conditioned                                     deﬁned crfs follows               αiyx      αiy xexp Λiy yx             ⎛                                  ⎞                            y                                                                          ⎝                                  ⎠         Λiy yx deﬁned       exp      λkfkyi−yix     μlglyix       zx             ik                  il                                                                                                                                                                                           Λiy yx      λkfkyi  yi   zx normalization constant makes prob                ability state sequences sum fkyi−yix                  arbitrary feature function entire observation se                 μlglyi         quence states positions −  glyix                 feature function state position observation                                                                 zx  equals αt yx “backward values”  sequence λk μl weights learned feature                                                        βiyx deﬁned similarly calculate  functions fk gl reﬂecting conﬁdence feature func  tions feature functions aspect tran marginal probability each sentence summary                                                        sentence given sentence sequence  sition yi− yi yi global character  istics example fk value  yi−                           ∗                                                                                   αi  βi  summary sentence yi summary sentence       yi                                                                                                                  zx  similarity xi− xi larger threshold  gl value  yi summary sentence xi order sentences based yi  uppercase words                                     select ones summary                                                    ijcai                                                      feature space                                    hits  scores shown related work section docu  features designed document summariza ment treated graph applying graph  tion leveraged through crf models paper based ranking algorithm hits pagerank each  use common features widely used sentence gets score reﬂecting importance according  supervised summarization methods features mihalcea  experimental results au  induced unsupervised methods detailed study thority score hits directed backward graph  sophisticated features rhetorical relations effective graphbased methods  sentences left future work            sider authority scores features  basic features                                           experiments results  basic features commonly used features pre                                                        section conduct experiments test crf  vious summarization approaches extracted di                                                        based summarization approach empirically data set  rectly complicated computation yeh et al                                                         open benchmark data set contains  document  given sentence xi  features deﬁned follows                                                        summary pairs document understanding conference  position position xi sentence sequence                                                        duc  httpducnistgov use  document xi appears beginning document                                                        generic singledocument extraction task interested  feature “pos” set  end document                                                        preprocessed denoted duc  “pos”  “pos” set                                                           supervised summarization methods need  length number terms contained xi removing                                                        split data set training data set test data set  words according stopword list                                                        order remove uncertainty data split fold cross  log likelihood log likelihood xi gener                                                        validation procedure applied experiments   ated document logpxid calculated                                                       folds used training fold test                                  wk nw  logpw  nw   num                                                       need split data set unsupervised methods  ber occurrences wk xi pwk estimated apply unsupervised methods test data  nwkd      nwjd                 wj                                     supervised methods convenience comparison  thematic words frequent words use methods evaluate results ﬁrst  document stop words removed sentences precision recall widely used infor  taining thematic words likely summary mation retrieval van rijsbergen  each document  sentences use feature record number manually extracted sentences considered refer  matic words xi                                    ence summary denoted sref  approach compares  indicator words words indicators summary candidate summary denoted scand reference  sentences “in summary” “in conclusion” summary computes precision recall values  feature denote xi contains words  shown equation  report simplicity  upper case words  proper names impor come similar conclusions experiments terms  tant presented through uppercase words three measurements  words authors want emphasize use fea                                                                                                                ture reﬂect xi contains uppercase words     sref  scand      sref  scand        pr                                                                                                similarity neighboring sentences deﬁne features          scand            sref             record similarity sentence neighbors  “sim pre n” “sim n”     record second evaluation method rouge toolkit                                                        based ngram statistics lin hovy   similarity xi previous three sentences three  sentences respectively similarity measurement use tool adopted duc automatic summarization  work cosine similarity                   evaluation highly correlate human                                                                                                      popular features number evaluations according lin hovy    words sentence present title evaluation methods implemented rouge rouge  position sentence paragraph  relatively simple works cases  information title paragraph employ rouge simplicity  available dataset working  baselines  consider features paper                                                        compare proposed method supervised  complex features                                      unsupervised methods supervised methods  lsa  scores decomposing wordsentence matrix  choose support vector machine svm naive bayes  through singular vector decomposition  obtain nb logistic regression lr hidden markov model  hidden topics document projection each hmm svm stateoftheart classiﬁers hmm  sentence each topic gong liu  extends nb considering sequential information  use projections scores rank sentences select lr discriminative version nb time lr  sentences summary paper treat considered linear chain crf model order zero  projections features reﬂect importance crf discriminative version hmm crf  sentences                                            combines merits hmm lr recent literature                                                    ijcai                                                    claimed advantages discriminative models clas ods incorporate complex features results  siﬁcation problems effectiveness sequential infor shown table  compared results based  mation sequence processing sutton mccallum  basic features shown table   detailed comparison methods formance supervised methods improved signiﬁ  make clear crf really hold advantages cantly incorporating complex features crf  summarization problem                            best method improves values rouge    compare approach unsupervised achieved best baselines   methods simplest select sentences randomly  compared best unsupervised method hits  document denoted random approach  crf based kinds features improves  selecting lead sentences taken baseline formance   terms rouge  popularly duc dataset denoted lead sim respectively fact complex features  ilar method select lead sentence each paragraph comes unsupervised methods lsa hits lever  information paragraphs available age complex features through supervised methods  duc include method baseline thought way combining outcomes different  unsupervised methods compare include gong’s al methods order test effectiveness crf combin  gorithm based lsa mihalcea’s algorithm based ing outcomes compared linear combination  graph analysis options mihalcea’s al method used combine results lsa hits crf  gorithm method based authority score hits based basic features tuning weight  directed backward graph best taken each method combination best result obtain  comparison unsupervised methods denoted duc   terms rouge  lsa hits respectively                            respectively improvement signiﬁcant                                                        crf based features conclude    results analysis                             crf provides effective way combine outcomes  performance based basic features               different methods treating outcomes features  ﬁrst experiment compares crfbased method  baselines using basic features tables          nb     lr     svm    hmm     crf   show results methods terms rouge rouge             random worst method                       expected crf best terms evalua  tion metrics hits beats baselines conﬁrms table  results supervised methods features  effectiveness graphbased approaches discovering          nb     lr     svm    hmm     crf  importance sentences lead simply selecting rouge             lead sentences achieves similar performance lsa                                                                              hmm lr improve performance compared nb  advantages leveraging sequential information table  results supervised methods training data  discriminative models lr svm achieve similar  formance summarization problem combining effect size training data  advantages hmm lr crf makes order study impact size training data  improvement   hmm lr supervised methods conduct experiment  terms rouge respectively fact crf change training data test data fold cross  just discriminative version hmm powerful validation procedure fold training  method exploiting dependent features folds test table  shows results based  reason crf outperforms hits   terms basic features complex features  rouge respectively                      performances supervised methods shown                 random      lead    lsa    hits        table  good given table       rouge                       consistent intuition obtain pre                                                        cise parameters models training data                                                                                  observation gap performance          table  results unsupervised methods      crfbased methods supervised methods                                                        clearly larger size training data small                 nb     lr    svm    hmm     crf        reason crf performs better training data     rouge                   hmm  does require features specify                            completely state observation lafferty et al                                                        side hmm generative model spends lot   table  results supervised methods basic features resources modeling generative models                                                        particularly relevant task inferring class labels  incorporation complex features                 bad performance nb lr svm  second experiment test effectiveness com fact tend overﬁtting small  plex features capability supervised meth training data                                                    ijcai                                                    
