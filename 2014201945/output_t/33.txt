                   learning opponents bounded memory                    ∗                            rob powers                                 yoav shoham                science department                  science department                      stanford university                          stanford university                      stanford ca                            stanford ca                powerscsstanfordedu                       shohamcsstanfordedu                        abstract                          algorithm achieves criteria arbitrary                                                        peated games note algorithms      recently number authors proposed cri  makes guarantee payoffs achieved al      teria evaluating learning algorithms multi gorithm nonstationary opponents potentially      agent systems welljustiﬁed each exploited arbitrarily adaptive opponents shown      generally given little attention chang kaelbling       main challenges multiagent setting ca   recent work bowling  addresses vulnerabil      pability agents adapt learn ity adding requirement agent experience zero av      propose extending existing criteria ap erage regret context regret traditionally deﬁned      ply class adaptive opponents bounded maximum payoff achieved play      memory show algorithm prov      ing stationary policy opponent’s entire history                    ǫ      ably achieves best response richer actual moves minus actual payoff agent received      class opponents simultaneously guarantee algorithms proven achieve zero      ing minimum payoff opponent     regret limit hart mascolell  ja      performing selfplay new algorithm  fari et al  examples game theory ai      demonstrates strong performance empirical                                                                                                   tests variety opponents wide range work fudenberg levine  ‘universal      environments                                  consistency’ representative literature points                                                        limitations regret minimization approach                                                        ﬁrst inability noregret strategies capi    introduction                                       talize simple patterns opponent’s play address                                                        limitation proposal stronger concept  recent work multiagent learning forth number                                                        ‘conditional consistency’ new algorithm achieves  proposals judging algorithms bowling veloso                                                         fudenberg levine  second limitation  conitzer sandholm  powers shoham                                                         noregret algorithm guarantees minimum pay  addition arguing merits proposal each                                                        possible opponent ignores possibility  searcher demonstrated algorithm meeting crite                                                        sequence moves played opponent  ria unfortunately algorithms criteria                                                        pendent agent’s moves assumption  selves general applicable limited                                                        quite justiﬁed games large number players  setting particular focus designing al                                                        liability repeated interactions  gorithms behave presence stationary oppo                                                        players aware work dealing  nents dodging complexities arise opponent                                                        explicitly limitation farias megiddo   adapting agent’s past play                                                        address design experts algorithm ra                                                       criteria proposed bowling veloso  tional learning approach kalai lehrer   require agent converge stationary policy principle handle adaptive algorithms arbitrary complexity  class opponents agent play long assigned positive probability prior  best response opponent converges stationary pol  icy criteria satisﬁed players failure consider adaptive opponents  sults guarantee ultimately repeatedly playing nash hurt algorithm’s performance let consider  equilibria stage game propose algorithm peated version prisoner’s dilemma game shown  provably meets criteria player normal form figure  prisoner’s dilemma extensively studied                                                                      games actions player conitzer sandholm axelrod  numerous algorithms proposed al   adopt restatement criteria prove low agents cooperate advantageous cooperation                                                        outcome exploited simplest    ∗this work supported air force grant effective titfortat algorithm titfortat   nsf grant iis                starts cooperating repeats actionthe opponent played note approach game structure payoffs known agents fi  siders stationary opponents play defect nally shall generally refer player  unique best response stationary op opponent mean imply adversarial setting  ponent strategy result noregret instead consider space generalsum games  performance titfortat results payoff   strategy playing cooperate yield      cooperate defect           left right  payoff  clearly noregret policy best response  richer strategy space                        cooperate                            example advantages considering adap                                                          defect                         tive opponents consider playing stackelberg game fig  ure  repeatedly notice strictly dominated strat prisoner’s dilemma stackelberg game  egy regardless opponent chooses row agent  prefer play opponent                                                        figure  example stage games row player’s payoff  learning presumably prompt play left                                                        given ﬁrst column player’s payoff following  resulting payoff  row agent instead played  seemingly suboptimal action opponent  adaptive opponents  learn play right giving row agent higher payoff goal work expand set possible   instance teaching play opponents achieve best response  role achieving desirable outcome learning need limit capabilities way  games successful strategies sider opponents future behavior depend arbitrar  ability cooperate ily entire history play lose ability learn  opponents manipulate opponents appropriate single repeated game    weaknesses reliance assumption given history assume  stationarity previously acknowledged powers limit opponent’s ability condition history                                                      shoham  proposed following three criteria propose directly limiting history available    targeted optimality member target requiring opponents play conditional strategy  set opponents algorithm achieves ǫ ex actions depend recent periods  pected value best response actual opponent past history fi  o− ×  × o−k → ∆ai o−t    compatibility during selfplay algorithm achieves outcome game periods ago assume  ǫ payoff nash equilibrium opponents default past history assume  pareto dominated nash equilibrium  start game note simple model allows                                                        capture methods titfortat current    safety opponent algorithm approaches unable properly handle  ceives ǫ security value game let’s consider apply criteria given    key aspects proposal use param set opponents value best  eterized target class opponents achieve response given conditional strategy welldeﬁned  optimal performance address adap prove unreasonable requirement possi  tive agents previous work provides algorithm ble strategies let’s consider prisoner’s dilemma  stationary opponents work adopt crite game opponent playing grim strategy  ria analyze develop algorithms behave plays cooperate grim strategy oppo  opponents adapt past experience nent initially starts playing cooperate switches play                                                        ing defect indeﬁnitely opponent plays defect    environment                                        conditional strategy history  note possible                                                        learning strategy achieve value best response  paper focus class twoplayer                                                        opponents play defect  peated games average reward setting                                                        distinguish point option  players repeatedly play simultaneous normal form                                                        cooperating grim strategy longer exist  game represented tuple                                                     possible approaches remedying problem  number players  ×  ×                                                     way constrain class opponents consider suf  set actions player  → ℜ reward                                                      ﬁcient requirements conditional strategies  function agent each round agents accumu                                                        opponent condition actions agent  late reward joint outcome observe                                                        past actions policies opponent plays  prior actions agent each agent assumed                                                        sign nonzero probability each action past history  trying maximize average reward example                                                        alternative approach relax best response  normal form games rewards organized table                                                        target instead requiring agent achieve best value  shown figure  purposes assume                                                        possible strategy played start game    three criteria additionally required hold set target highest average value  choice ǫ probability  − δ polynomial period achieved arbitrary initial sequence moves  learning start game                 account need exploration  restrictions target set strategy  stochgodfather  order guarantee ǫbest response high probabil τ time steps  play  strategy  ity require exponential exploration period τ time steps  ﬁnd good outcome agent needs sample      avgv alue  vgodfather − ǫ  exponential number histories opponent considers fur    probability     thermore allow opponent condition       set  strategy     membr  actions number observations required play  strategy  bounded unless add requirement minimum proba   opponentintargetset                                bility playing given action                          τ time  steps   play  membr                                                               opponentintargetset    manipulative algorithm                                    set beststrategy      membr  metastrategy algorithm introduced pow   set  beststrategy      strategy  ers shoham  explicitly designed station strategy  stochgodfather  ary opponents use intuition      avgv alue  vgodfather − ǫ  approach design new algorithm class adaptive set beststrategy      stochgodfather  opponents idea metastrategy start set  beststrategy     membr  teaching strategy initial coordinationexploration phase end game  use payoff opponent’s play determine avgv alue  vsecurity − ǫ  three possible strategies play opponent  play  maximin   strategy  sistent target class adopts best response  achieves target value getting opponent adopt   play  beststrategy  best response teaching strategy continues playing                                                                  figure  manipulator algorithm  selects default strategy algorithm  plays according strategy long exceeds secu  rity level reverting maximin policy payoff drops values plays portion target outcome op    ﬁgure  show implementation general ponent plays action matching action  approach target class membr strategy calcu target outcome agent plays strategy forces  lates best response strategy conditional strategies opponent achieve security value  approach maintains counts opponent’s actions af opponent plays target action purposes  ter each history length uses calculate cy we’ve created stochastic version godfather selects  cle agent actions highest expected reward mixed strategy agent target action oppo  possible unique agent action sequences don’t nent joint strategy gives opponent higher  contain length repeated subsequence given sufﬁcient expected value security value denies  observations lets guarantee achieve ǫbest advantageous deviations necessary want  response member target opponent set godfather algorithm implementable conditional  calculate probability opponent’s play consistent strategy history  constraint need  target set comparing observed distribution make sure opponent can’t achieve net proﬁt deviating  play each history separate times measuring turn playing target action incurring  viation action proﬁles continue use minimax period punishment parameters speciﬁed                                                                                                      ǫ  strategy achieve security value guarantee algorithm function desired values                                                        δ  selfplay guarantee replace bullymixed strategy theoretical guarantees empirical results used                                                        τ         τ               ǫ ǫ      generous stochastic version godfather littman                   stone  godfather motivated folk theorem additional advantage reusing existing framework  repeated games selects outcome game apply proof used metastrategy algo  matrix greater payoff each agent security rithm minor modiﬁcations show theorem                                                         main change require agent play fully    consider conditional strategy plays mixed strategies during beginning game order  defect prisoner’s dilemma opponent played defect sufﬁcient observations test opponent’s  previous plays cooperate δ probability op play consistent target set strategies given  ponent played cooperate played defect plays constraint proof consists mainly long string  cooperate agent played cooperate agent applications hoeffding inequality hoeffding   require number observations proportional δ order  distinguish opponent plays defect                                                       theorem   algorithm manipulator satisﬁes three     metastrategy fictitious play brown  properties stated introduction class condi  note instead using noregret policy achieved                                                        tional strategies bounded memory training pe  stronger payoff guarantee opponents                                                                                                implementation suitable conditional strategies riod depending λ  λ minimum probability  depend agent’s actions general conditional strate opponent assigns action λ   opponents  gies need consider space deterministic conditional condition agent’s actions  strategies ﬁnd best response           manipulator         metastrategy         godfather          hyperq         smoothfp                                                                                                                                                                                                                   alq     hf                                  ato                  bull             fat                                                                lo                                          pul       random                                                              hyper                                       membr            moot                               ani                       stochm                                   wolfph                                                                                                      metastrategy   figure  average value rounds selected games gamut game payoffs range        experimental results                               dove godfather able perform better manipulat  order test performance approach we’ve used ing opponents yielding sending  comprehensive testing environment detailed powers clearer message doesn’t exploration  shoham  nudelman et al  metas waiting longer opponent adapt stub  trategy set opponents includes bully godfather bornness proves undoing games  littman stone  hyperq tesauro  critical adapt opponent disper  local qlearning watkins dayan  smooth ﬁcti sion game metastrategy’s strong performance shapley’s  tious play fudenberg levine  wolfphc game stem default fictitious play strategy  bowling veloso  include random sta exploiting localq wolfphc  tionary strategies random random conditional strategies advantage manipulator metastrategy games like  stochmem membr learns best response prisoner’s dilemma travelers dilemma equi  conditional strategies history                 libria space repeated game strategies pareto    figure  show performance suc dominate equilibria stage game  cessful distinct algorithms variety  mal form games adaptive algorithms fare  discussion  stationary opponents random bully manipulator demonstrates consistent performance  manipulator lesser degree hyperq fare best wide variety games means claim  bounded memory adaptive strategies godfather ing best approach settings  stochmem manipulator godfather ad particular doesn’t fare nearly adver  vantage opponents learn best response sarial games like matchingpennies rochambeau shap  ditional strategies membr manipulator leysgame surprising unable  approaches fall outside target sets ﬁnd deals offer godfather component  metastrategy manipulator metastrategy modelbased assumption opponent conditional  slight advantage mainly ability play strategy offers particular advantage adaptive  pure strategies manipulator constrained explore opponents alternate approach considered adapt  opponent’s strategy space during initial coordination ing modelfree algorithm qlearning watkins  period additional advantage manipulator dayan  multiagent setting following foot  setting ability perform selfplay achieving steps numerous previous researchers attempting ﬁnd ef  highest payoff algorithms tested metastrat fective multiagent learning strategies littman   egy limited space stationary policies misses claus boutilier  tesauro  traditional  complex opportunities cooperation games learning algorithm learns values each action each    let turn performance new algorithm possible state world chooses action  different types games figure  shows relative reward maximizes expected reward propose possible al  achieved successful algorithms selection ternatives dealing explicitly adaptive opponents  games gamut averaged set opponents method incorporate recent history state  manipulator best performance nearly world learn action values each possible recent history  game games like prisoner’s dilemma hawk second approach instead learn values sequences                   manipulator        metastrategy        godfather        hyperq         smoothfp                                                                                       ly                  ar                                                                                llar         ies fort             ical                                                          mma                                        exes                           dove                         os                                         tg      ti                     ile            li  chicken                           pe         mpound      er       sga                    ti  tiong                                                              th               ri      ac                    rs                       le       wog                                                                    mz                               ina                           inimumefn mc                                                    majorityvotingtching randomgame rochambeau velersdilemmaby          rt           rd      ispersiongam grabthedohaw                        sha                                                                     andomgraand                                  ga                     pri                    tra  tw    battle    collaborco                                     figure  percent maximum value rounds averaged opponents selected games  gamut rewards divided maximum reward achieved agent make visual comparisons easier    actions conducted tests algorithms particular need consider handle ﬁnite au  approach conditioned previous tomata multiple ergodic sets states automata  tory demonstrated ability exploit arbitrarily small transition probabilities  adaptive approaches including local qlearning smoothfp  hyperq resulting higher average value zerosum  games previous approaches nei  conclusions future work  ther modelfree methods performed general feel explicitly addressing issue adaptive oppo  sum games dominated manipulator nents critical element learning multiagent systems  each opponent tested set games quality deﬁne difference  approaches lack theoretical guarantees ma tween multiagent setting singleagent  nipulator proﬁt requiring advance knowl algorithm approaches setting combining teaching  edge game payoffs additionally ability iden approach manipulates adaptive opponents playing  tify settings individual approaches particularly ef agent’s advantage cooperativelearning approach  fective lead powerful methods using portfolios adapts best estimate opponent’s strat  algorithms suggested leytonbrown et al  egy algorithm shown achieve ǫoptimal aver    finally pointed model we’ve age reward nontrivial class adaptive opponents  forth modelling opponents bounded capabilities simultaneously guaranteeing minimum payoff  possible ones common approach  opponent performing selfplay high  used literature bounded rationality neyman  probability results translate good empirical  papadimitriou yannakakis  assume agents formance wide variety environments clearly  modelled ﬁnite automata states note work discussed  automata model comprehensive set previous section we’ve far analyzed possi  conditional strategies conditional strategy opponent ble model adaptive opponents bounded memory  bounded memory modelled automata considering best incorporate approaches  ak states allow stochastic outputs exist au achieve better empirical performance existing al  tomata modelled function ﬁnite gorithms additionally approach signiﬁcant  ﬁxed history case automata deterministic tran strictions set environments considers  sitions modify manipulator algorithm handle immediately identify ﬁve limitations current approach  new class implementing version godfather  dfa replacing best response function note  single opponent criteria clearly deﬁned  learning best response opponent modelled games players  known ﬁnite automata equivalent ﬁnding best policy  single state criteria clearly deﬁned                                        unknown pomdp investigated chrisman      repeated games general stochastic games  nikovski nourbakhsh  difﬁcult computa  tion problem able achieve theoreti  average reward criteria deﬁned games  cal properties alternate set opponents given similar agent cares average ag  constraints placed conditional strategies gregated rewards discounted sum
