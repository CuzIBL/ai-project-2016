 informationtheoretic analysis memory bounds distributed resource                                        allocation mechanism                                    ricardo araujo     luis lamb                    instituto informatica´ universidade federal rio grande sul                                       porto alegre brazil                                rmaraujoinfufrgsbr luislambacmorg                        abstract                          smaller analyzing multiple outcomes                                                        produced individual agents importantly cost      multiagent distributed resource allocation requires does depend number agents computation      agents act limited localized information feasible knowledge number      minimum communication overhead order        repeated dispersion games agents learn se      optimize distribution available resources ries past outcomes order improve decisions      requirements constraints dynamic    natural issue large observation window      learning agents needed allow adap  bounds agents’ memory observation capabilities      tation way accomplishing learning   assumed  paucity explanation      observe past outcomes using information  limits established paper tackle      improve future decisions limits agents’  problem specialized version dispersion game known      memory observation capabilities assumed   minority game mg  mg ﬁnite number      decide large obser    agents decide ﬁnite set actions      vation window investigate     choose action chosen smallest number agents      cision inﬂuences agents’ system’s   rewarded game repeated agents chance      formance context special class dis learn previous rounds improve performance      tributed resource allocation problems dis considering ﬁnite window past outcomes      persion games show  numerical experi      ﬁnite memory order try learn temporal patterns      ments speciﬁc dispersion game minority  contribution twofold systematically simu      game scenario agent’s performance late cases mg composed agents different mem      nonmonotonically correlated memory   ory sizes compare performances ﬁxed class      size agents kept unchanged agents ﬁxed memory sizes using different      provide informationtheoretic explanation learning algorithms results show clear nonmonotonic      observed behaviors showing  relation access information agent’s perfor      ward causation effect takes place                mance unveiling cases information translates                                                        higher gains cases harmful    introduction                                       relate ﬁndings system’s global efﬁciency state  important class natural problems involving distributed second provide informationtheoretic explanation  resource allocation mechanisms modeled disper observed behaviors show size gen  sion games  problems class anticoordination erated outcome patterns does match system’s  nature agents prefer actions taken memory size allowing agents larger memory sizes  agents stock exchange load balancing networks exploit information  instances problems  iterated versions remainder paper organized follow start  games learning important component decision discussing relevant related work present mg  making process each agent try learn individ model terminology methodology  ual behaviors agents complex task especially results experimental studies using different  total number agents unknown learning algorithms finally analyze results point  constant large possible solution consider directions future research  agents’ actions indistinguishable environ  ment  idea socalled level agents   related work  scenarios yields best results  cases previous works focused problems relating ho  joint actions agents considered mogeneous memory size global performance little  represented single global outcome computa effort understanding relation  tional cost analyzing single global outcome expected individual memory size individual agent’s performance                                                    ijcai                                                     homogeneous agents involved selfplay typically agents modeled using simple inductive learning algo  sumed emphasis dynamics indi rithm make decisions corresponding simpliﬁed model  vidual agents argued  appropriate human decisions complex environments actu  agenda multiagent learning “asks best learning ally  algorithm widely used  strategy ﬁxed class agents game” studied applications make use  allows construction effective agents fol straightforward way refer traditional  low methodology context mg little learning algorithm algorithm each agent ac  work far directed concerned indi cess outcomes game given  vidual agent’s performance exception  set hypotheses randomly selected ﬁxed hy  genetic algorithmicbased agent shown outper potheses space hypothesis maps possible patterns  form traditional ones little investigation condi past outcomes decision group  tions happens learning algorithms choose round known memory size    applied mg    homogeneous game hypothesis effectivelym represented bi  agents assumed focus global properties   nary string length   learning al    heterogeneous memory sizes studied  gorithm keeps measure fijt effectiveness each  evolutionary setup used search best memory size hypothesis held agent time updated  game ﬁx class agents al fij tfijt −   aisignat −  measure  lowing agent evolve independently similar setup called virtual points agent uses hypothesis  used  concern average memory size maximum number virtual points argmaxjfij  shall provide alternative explanation results commit decision round game ties   stated agents larger memories perform broken coin toss algorithm simple  better agents smaller ones nonetheless provides adaptation learning agents  sider varying number agents endowed ex able better explore strategies outside initial set  tra bit memory show having larger memories der overcome limitations algorithm allow  lead better performance                   comparisons different learning strategy shall                                                        introduce new evolutionarybased learning algorithm    multiagent market scenario                                                           methodology experiments  minority game mg initially proposed  text econophysics formalization el farol bar nature model consider  problem  models main type agents different hypotheses space sizes different                                                        memory sizes order observe effects dif  information structure created multiple interacting bounded                      rational agents mg groups ferences agents’ performance  split mg  odd number agents each round agents  parts environment consists traditional mg set  choose concurrently communication agents described homogeneous agents  groups each round control group group agents sample hy                                                     potheses different space agents  cisions global outcome calculated                                                                                    environment differing size  hypotheses    ai  ai decision agent round space size setup deﬁned each agent’s mem  ait ∈ − agents’ wealth distributed ac                                                        ory size shall denote environment’s  cording witwit −   aitsignat −       ·     −               −                   control group’s mg  sign       according   positive   work assume   investigates larger  negative functions used initial values                                                      groups similar fashion experiments  set zero encodes group minority memory size systematically modiﬁed  agents group rewarded effects changes group sizes observe  majority penalized equilibrium point performance resulting control agent hy  agents proﬁt expectation majority                                                        potheses space size mg varies relation  frustrated anticoordination nature want verify performance deﬁned average  game believe particular group minority wealth  changes compare environ  actually end majority spite                                                                      ment’s performance deﬁned average wealth  simplicity mg offers rich complex behavior random agent environment order mea  shown represent scenarios decentral                                                        sure control agent’s gain wgwe results averages  ized resource allocation ﬁnancial markets trafﬁc  randomly initialized games sta  commuters scenarios   model generalized tistically tested signiﬁcance using standard ttest  shown belong broader class dispersion games  conﬁdence interval each game run     main concerns mg game’s rounds used standard values  global efﬁciency number agents winning game literature parameters     viewpoint interested  maximizing number winners resources dealing homogeneous agents hypotheses drawn  better distributed mg originally introduced hypotheses space                                                    ijcai                                                                                                           figure  control agent gain solid line                                                        dashed line agents using traditional learn  figure  control agent gains combinations ing algorithm  mg lighter shades denote higher gains     comparisons easily  shall  experiments using traditional evolutionary  learning algorithms    traditional learning algorithm  start traditional learning algorithm  control agent agents environment fig   show control agent’s gain combination   ∈   mg ∈   ranges  somewhat limited adequate purposes ex  ponential increase hypotheses space large  number agents makes experimentation larger figure  control agent gain varying    memories intractable resulting topography ob                                         serve interesting behaviors ≤  higher  unit gains obtained mg dow harmful agent environment  hand values immediately observe small memory sizes observe phenomena plot                                                                                                      mg wg falls unit showing control agent’s gain varies set mg   target agent does worse having larger hypotheses shown fig  clear  space larger wg unit nonmonotonic relation memory size agent’s  mg  gain expected exactly unitary performance observe gains high small values    let observe happens speciﬁc smaller unit reaching mini  points observed regions sectioning topography mum later increasing ﬁnally converge value                                                        slightly unit having worse performance access  fig  ﬁrst case   control agent beneﬁts larger hypotheses ing information counterintuitive  space interestingly having smaller spaces cause argue larger hypotheses space                                                        makes ﬁnding good hypothesis harder  harm agent performs mg   observe transition cases explanation does account                                                        observe nonlinear relationship gains  quite sharp increases mg provide ad  ditional gains instead logarithmic decrease wealth hypotheses space size shall investigate  observed increases  highest gain                                  mg                      evolutionary learning  occurs precisely mg   spike  observed second case behavior argued observed behavior pe   observe logarithmic drop wg mg culiar effect learning algorithm used main lim  mg small decrease gains observed itation inability explore hypotheses space during  region mg does better unit game agent use hypotheses given  ingly having access larger input window harmful start run order address concern  agent larger values losses larger peated experiments using evolutionarybased learning  mg having access larger information win algorithm allow agents explore hypothe                                                        ses space different evolutionarybased algorithms    known larger does change qualitative behav mg proposed    chose adapta  ior                                  tion proposed  simplicity good                                                    ijcai                                                       algorithm evollearning       ←        ← random hypotheses ∈       foreach ∈          ﬁtnesss ←        end       end game          obs ← window size past outcomes          ← arg maxs ﬁtnesss          decision ← decision using observation obs          commitdecision          decision  outcome ←          foreach ∈             decision using obs  outcome                 ﬁtnesss ← ﬁtnesss               figure  agents using evolutionarybased algorithm                             ﬁtnesss ← ﬁtnesss               end          end          probability pr             worst hypothesis ← best hypothesis             probability pm ﬂip bits          end       end    end         figure  evolutionary learning algorithm    performance applied elaborated  algorithm used similar games  chosen  create algorithm closely related traditional  presenting better learning characteristics doing  able better understand compare results ob figure  control agent gains combinations  tained using algorithms                         mg lighter shades denote higher gains    algorithm depicted fig  each agent starts  hypotheses round probability pr traditional learning algorithm mg  discards worst performing hypothesis replaces considerable losses gains observed  copy best performing each bit  copy ﬂipped probability pm allows agents  analyzing results  search better hypotheses continually introducing new  ones game interesting note differently  dynamics game efﬁciency regions  evolutionary learning algorithms  stated main concerns mg  does require global ordering agents based global efﬁciency agents winning game  performances agents retain autonomous characteristics during run typical way measuring temporal efﬁ  relying central authority decide hypothe ciency resource distribution means statistical  ses agents replaced preserving variance σ  larger variance larger  distributed nature game figure  shows waste resources making efﬁcient  typical run agents using new proposed learning variance σ function number agents  algorithm pr  pm  clearly learn game homogeneous memory size   ing taking place using traditional algorithm number agents ﬁxed control group uni                                                                           decrease oscillations observed long runs tary consider σ function memory  experiments using algorithm consider size agents environment figure  shows σ  sults  rounds order observe “steady” state function agents use traditional learning  figure  shows control agent’s gain algorithm plot observed mg three  combination ∈   mg ∈   gions efﬁciency observed small high vari  agents use evolutionarybased learning algorithm ance observed low efﬁciency characterize  observe different behavior compared pre large precisely variance expected  vious case regions larger memories agents deciding randomly random case  beneﬁcial each extra bit environment’s memory game  rcg intermediate values correlated  size harmful target agent logarithmic decrease smaller better random variances case  mg observed mg tested values main subject mg indicates  fig  behavior values agents able selforganize improve efﬁciency                                                    ijcai                                                                                                                      figure  target agent’s gain versus mg solid figure  σ function traditional solid line  line dashed line using evolutionary learn evolutionary algorithm dashed line  ing algorithm                                                        measured information conditional probability    comparing fig  plotted gain using traditional μ   probability having “” following binary  algorithm fig  observe higher unit gains                                                        sequence μk length shown inefﬁ  associated regions high variances gains unit cient region actually informationally efﬁcient sense  associated regions small variance increases                                                        μk exactly  μk agents  gains follow increases variance indicates cor memory size hand efﬁcient  relation system’s global efﬁciency exploitation region asymmetry probability distribution  possibilities individual agents larger memories                                                        μk offer predictive power informa  beneﬁcial behaving inefﬁciently worse tion analysis  focused cases   random efﬁcient having larger mem measurement information available agents  ories harmful behavior observed memory size peers wish access  evolutionary setup observe fig  formation available  represents  variance curve quite different using tradi target agent different memory sizes order  tional algorithm  smooth transition deﬁne precise information measure  low variance expected rcg measure asymmetry distribution predictive  inefﬁcient region expect following analogous rea power each hypothesis use concept mutual  soning agent mg perform better                                                        formation  string μk outcome comput  agents environment actually ob ing average information contained each string ik                                                                                                       served fig  results interesting        pμk                 pμk                                                                pμ log            pμ log           late efﬁciency individual ex μk puμkp             puμkp  ploitation possibility means larger memories indi probabilities “” “” occur  cate existence stability points system’s efﬁciency ring respectively pμk pμk probabilities  regions instance evolutionary version “” “” immediately occurring string μk ∈  game agents start small memory spectively puμkpμkpμk equation  allowed increase decrease memory size during ik zero probabilities observing “”  run expect initial incentive “” μk highest value ik  larger memories leading higher highest average asymmetry probabilities indicating  efﬁciency points incentive stop existence hypotheses predictive power mea  reaches efﬁcient point larger memories sured information executing runs  rounds  harmful efﬁcient point stabil recording each outcome resulting binary string  ity point experiment conducted  initial  symbols mutual information measured  memory growth observed halting predicted mem string averaged  independent runs  ory size behavior attributed simple nature position measure information avail  game propose detailed distinctive expla able hypotheses different lengths agents  nation relating individual agent gain efﬁciency region using traditional learning algorithm each value  case downward causation let assume values equal  game dynamics initially ﬁxed agents dynam figure  shows results val  ics end ﬁxing possible proﬁtable agents’ behaviors ues detailed previous sections observe ik                                                         remains close zero ≤  information    mutual information                                                                                                                          available memory sizes   homogeneous memory sizes traditional learn explains agents smaller memories  ing algorithm used show different informa present lower gains fig   observe  tion available agents different memory sizes substantial increase available information                                                    ijcai                                                     
