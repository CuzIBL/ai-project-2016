     scalable kernelbased algorithm semisupervised metric learning∗                                   dityan yeung hong chang guang dai                             department science engineering                             hong kong university science technology                                  clear water bay kowloon hong kong                            dyyeunghongchdaiguangcseusthk                        abstract                          formulation approach solve problem semi                                                        supervised learning literature large comprehen      recent years metric learning semi     sive review interested readers referred good      supervised setting aroused lot research surveys zhu       terests type semisupervised metric learn  way categorize semi      ing utilizes supervisory information form  supervised learning methods consider type su      pairwise similarity dissimilarity constraints pervisory information available learning unlike unsu      methods proposed far  pervised learning tasks supervisory information available      limited linear metric learning unable scale semisupervised learning tasks information      data set size paper form weaker available typical su      propose nonlinear metric learning method based  pervised learning tasks type weak supervisory      kernel approach applying lowrank ap  formation assumes usually limited      proximation kernel matrix method  training data labeled scenario commonly en      handle signiﬁcantly larger data sets countered realworld applications example      lowrank approximation scheme naturally lead  automatic classiﬁcation web pages semantic cate      outofsample generalization experiments gories labeling web pages laborintensive      formed artiﬁcial realworld data show costly unlabeled web pages far plentiful      promising results                           web desirable classiﬁcation algorithm                                                        advantage unlabeled data increasing                                                        classiﬁcation accuracy semisupervised classiﬁcation    introduction                                       methods zhu  belong category    semisupervised learning                           type supervisory information weaker  supervised learning given training sample assumes existence pairwise  form inputoutput pairs learning task ﬁnd func straints indicating similarity dissimilarity relationships                                                        tween training examples video indexing applications  tional relationship maps input output  disagreement future inputoutput observations mini example temporal continuity data naturally used  mized classiﬁcation regression problems impose pairwise similarity constraints successive                                                        frames video sequences example proteomic  common supervised learning problems discretevalued  continuousvalued outputs respectively unsupervised analysis proteinprotein interactions naturally  learning given training sample objects represented pairwise constraints study proteins                                                        encoded genes database interacting pro  output values aim extracting structure  obtain concise representation gain teins dip httpdipdoembiuclaedu  derstanding process generated data clustering example antispam problem recent studies                                                        show half email internet today  density estimation novelty detection problems com  mon unsupervised learning problems                   spam unsolicited commercial email recent ap                                                        proach spam detection based trustworthiness    past decade growing inter                                                        social networks boykin roychowdhury  est exploring new learning problems super                                                        social networks naturally represented graphs  vised unsupervised learning extremes methods                                                        edges nodes representing pairwise relationships  generally referred semisupervised learning meth                                                        social networks  ods exist large variations problem                                                          supervisory information form limited la    ∗this research supported competitive earmarked beled data transformed pairwise similarity  research grant  research grants council dissimilarity constraints inverse transformation general  hong kong special administrative region china       possible special case twoclass problems                                                    ijcai                                                    sense second type supervisory information  organization paper  weaker form corresponding learning problem section  propose simple efﬁcient kernelbased  difﬁcult solve focus paper metric learning method based pairwise similarity  category semisupervised learning problems        straints like kernel methods limitation                                                        method does scale sample size                                                        section  address scalability issue applying    semisupervised metric learning based         lowrank approximation kernel matrix extended       pairwise constraints                             method naturally rise outofsample gener                                                        alization addressed section  present  semisupervised learning methods assume experimental results section  demonstrate effec  existence limited labeled data zhu  tiveness metric learning algorithm finally section   fewer methods work pairwise constraints concludes paper  survey representative methods subsection    pairwise constraints simply state  kernelbased metric learning  examples belong class different classes  problem setup  wagstaff cardie  ﬁrst used pairwise infor                                                                                                 mation semisupervised clustering tasks modifying let set data points input space   standard kmeans clustering algorithm account suppose mercer kernel kˆ induces nonlin  pairwise similarity dissimilarity constraints exten ear feature map φˆ reproducing kernel hilbert  sions modelbased clustering based space sch¨olkopf smola  corresponding set  expectationmaximization em algorithm gaus                                                                                 feature vectors φˆxii kernel matrix  sian mixture models shental et al  lu leen                                                        kˆ kˆxi xjn×n φˆxi φˆxj n×n choices kˆ   methods explicitly learn dis                                                        include gaussian rbf kernel polynomial kernel  tance function seek satisfy constraints typically                                                        apply centering transform feature vectors  clustering tasks referred                                                        zero mean resulting kernel matrix  constraintbased clustering methods                                                        computed kxi xj n×n φxiφxj n×n     different category methods proposed hkhˆ  centering matrix  − nt  learn mahalanobis metric distance function × identity matrix  ×  vector  based pairwise constraints xing et al  formulated ones  convex optimization problem inequality constraints consider type semisupervised metric learning  learn mahalanobis metric demonstrated performance supervisory information given form  improvement subsequent clustering task solving simi pairwise similarity constraints speciﬁcally given  lar problem learn mahalanobis metric relevant com set point pairs subset x×xass   ponent analysis rca algorithm barhillel et al                                                         xi xj  xi xj belong class goal         proposed simpler efﬁcient algorithm make use learn better metric through modifying                           xing et al   rca make use kernel performance subsequent task                                            similarity constraints hertz et al  proposed clustering classiﬁcation based metric improved af  distance function learning method called distboost ter kernel learning  guarantee distance function learned  metric bilenko et al  explored possibility  kernel learning  integrating constraintbased clustering semisupervised                                                        kernel matrix symmetric positive semi  clustering based distance function learning distance                                                                                             deﬁnite express     λrvrvr    function learning methods reviewed learn ma p                                                          λrkr λ ≥  ··· ≥ λp    ≤  halanobis metric corresponds linear transformation                  learn distance function metric positive eigenvalues   corresponding                                                                                     vt  previous work chang yeung  proposed met normalized eigenvectors    ric learning method corresponds nonlinear transfor consider restricted form kernel matrix learning  mation method powerful linear modifying through changing λr’s  methods optimization problem nonconvex ing kr’s ﬁxed ensure eigenvalues                                                        nonnegative rewrite as kβ kβxi xjn×n   complicated solve                                                                                                              φβxiφβxjn×n         βr kr represents    paper focus distance function learn                                                                               family kernel matrices parameterized β    ing approach distance functions central                                                             ββp   learning models algorithms makes easier  achieve outofsample generalization focus later formulation optimization prob  learning metrics allows formulate met lem kernel learning constraints “soft” constraints  ric learning problem based kernel approach sch¨olkopf “hard” constraints preferred enforced  smola  provides disciplined computa makes easy handle noisy constraints erroneous super  tionally appealing approach nonlinear metric learning visory information wish                                                    ijcai                                                      perform kernel learning mean squared eu  lowrank approximation  clidean distance induced kβ feature vectors apply lowrank approximation approximate                             corresponding point pairs reduced crite × symmetric positive semideﬁnite matrix k   rion function optimization                                                                                                                                                                 k  wlw                      js β                                                     ∈ rn×m       ×            ∈ rm×m                                                                        matrix                                        −                 × symmetric positive semideﬁnite matrix                 βii  βjj    βij                        ∈s                                                       ⎡j                              ⎤        different ways construct lowrank ap          p                                           proximation consider way constructs             β ⎣           −   −   ⎦                                                        ing subset data points refer points                xixj ∈s                          landmarks silva tenenbaum  weinberger et al                                                                                      βt β                                        silva et al   loss generality                                                  sume points ordered way ﬁrst                                                                                                                                             points form set landmarks en  bi ith column × identity matrix                                                        sure points involved chosen landmarks  ds × diagonal matrix diagonal entries                                                        landmarks randomly sampled data                                                       points similar previous section obtained                          −    −       rr                        applying centering transform lˆasl  hlhˆ                      ∈s                                                    lˆ   kˆ xi xj m×m    φˆ xi  φˆ xj  m×m                                                                                                                                                  ×             kˆ                           bi − bj vr ≥       upperleft     submatrix                                                       apply eigendecomposition express                     xixj ∈s                                                                        q                                                                         μ α  αt      vt     prevent β degenerating zero vector                     α  μ  α            eliminate scaling factor minimize convex func                                               tion js β subject linear constraint  β  μ ≥···≥μq   ≤ positive eigenvalues  constant simple convex optimization prob ααq corresponding normalized eigenvec                                                                     μ μ          α  α  lem quadratic objective function linear equality tors μ  diag  qand α      sub                                         ρ                                                            constraint introduce lagrange multiplier minimize stituting   rewrite  μrkr  following lagrangian                                                                                                            kr wαrwαr                                                       js βρjs βρc  −   β             kernel learning                                                        apply lowrank approximation devise scalable kernel    optimization problem solved easily learning algorithm seen extension  optimal value β following closedform solution                                                        algorithm described section  use k approximate                                                                                 cd−                            deﬁne the following parameterized family kernel                     β                                    k       βk                  β                         d−                        matrices β      note                                                      ×  vector ×  vector            −                                            optimal value βhas form   note  exists long diagonal entries                      √                                                      constant set μr ds ×                                                                                ds positive usually true set constant     p    √                                            diagonal matrix diagonal entries      λr                                                                                                                                        −  wv     ≥                                                               rr                       scalable kernel learning                                           xixj ∈s                                                          computing embedding weights  kernel learning method described requires                                               forming eigendecomposition case large question remains answered obtain  large matrix operations eigende lowrank approximation use method similar  composition computationally demanding locally linear embedding lle roweis saul   section apply lowrank approximation extend ker saul roweis  differences  nel learning method scales use ﬁrst lle obtain weights locally linear                                                        ﬁtting second perform locally linear ﬁtting kernel                                                        induced feature space input space     indicator variable “overloaded” later refer                                                                                     let  wij n×m wi wiwim ifxi  column identity matrix size clear context                                                       landmark  ≤ ≤ mthen     case ds really singular rare case common               way make invertible add term i ds                                                                                       wij                               small positive constant                                                                                                          ijcai                                                                                                                                                 xi landmark minimize following func nonlandmark points xiim  tion obtain wi                                    outofsample point embedded                                                                                                 way          ewi φxi  −         wij φxj           let uuq orthonormal basis each ur ∈                                                                                                                          φxj ∈ni                        unit vector direction th prin                                                        cipal component deﬁne   uuqthen  ni set nearest landmarks φxi each landmark xi embedded                                                                                                                            wij     wi                                            φ    subject constraints φxj ∈ni              adimensional vector                                                                                      φ   ∈n                             ur     √        αjrφ xj                yi    yi   ij     rewrite               μr          express                                                               −                                                                dμ    vαlbi    dμ  vαbi let ym  yymso    ewi                 wij wikφxi − φxj  ·                                                            dvt                 φx φx ∈n                                 μ    α                                                                                                                                     the nonlandmark points im  use                           φxi − φxk                                                                      φ xi              wij φ xj              φ xi     yi                                                             φxj ∈ni      approximate                wi giwi                                                                                                             denote embedding φxi                                                                                                                                          yi  φxiymw      bi  dμ  vαw    bi    gi kxi xikxj xk − kxi xj − kxi xkk×k    similarly outofsample example xweuse                                                                                                                   φ              wjφ  xj               φ     y                                                               φxj ∈ni     approximate    local gram matrix xi                                                                                               denote embedding φx embedding weights    prevent wi degenerating  minimize ewi                                                                   wwm    determined section   subject constraints     wij    wi                          φxj ∈ni                                                                φ    ∈n                                similar nonlandmark points im ob   ij      kernel learn tain  ing problem solve convex optimization problem                                                                                               y  dμ  vαw                  quadratic objective function linear equality constraint                                                          based   squared euclidean distance  lagrangian lagrange multiplier α follows                                                        tween yi y kernel learning expressed                                              lwiαwi   giwi   α −  wi                                                                                                                                               xi yi   − y   closedform solution optimization problem                                                −       −      −                             bi    −  vαdμvαw      bi −  given wi gi   gi   gi exists                                                                                        instead performing matrix inversion efﬁcient         bi    −  lw   bi −       way ﬁnding solution solve linear squared euclidean distance kernel learning  equations giwˆ   wˆ compute wi wi                                                                                dβxi xbi   −  vαdβvαw     bi −   wˆ wˆ ensure equality constraint  wi                                                                                  satisﬁed                                          dβ diagβ βq     assume neighborhood relationships  tween points local gram matrices remain ﬁxed  experimental results  during kernel learning process simple extension section present experiments  basic algorithm repeat procedure iteratively formed based artiﬁcial realworld data  using learned kernel each iteration basic al  gorithm just special case iterative extension  experimental setup  number iterations equal             compare versions kernelbased metric learning                                                        method described sections   rca barhillel et    outofsample generalization                       al   promising linear metric learning  exact form outofsample generalization depends method usually performs equally computa                                                        tionally demanding methods baseline comparison  operation want perform example given                                                        include metrics metric learning  points ﬁrst clustered ≥  classes kernel learn  ing new data point classiﬁed euclidean metric input space euclidean                                                        metric feature space induced gaussian rbf ker  classes interested case clus                                                      nel  tering points classiﬁcation new points                                        based euclidean metric                each data set randomly generate  different sets                                                        small data sets learn kβ lowrank ap    key idea outofsample generalization scheme                                                        proximation large data sets addition data points  rests observation kernel principal component anal                                                        involved randomly select points  ysis kpca sch¨olkopf et al  performed                                                                                            kβ  xii obtain embedding qdimensional subspace landmarks learning  use iterative extension                                                        scalable kernel learning algorithm number         similar ds add i make sure gi iterations equal  measure change metric  invertible                                           learning performance number landmarks increases                                                    ijcai                                                                                                                                                                                                                                                                                                                                                                                                                                          −                       −                      −                                                                     −                         −                      −                     −  −               −   −              −   −                                                                                                                                                                                                                                                                                                                 −                   −                    −                        −                   −                    −                     − −    − −      − −                                                                           figure  xor illustration input data similarity constraints new data points rca rbf kβ       kβ                                                                                                                                           db    nb          xi − xj     illustrative example                                   yiyj        mean                                                                          figure  uses xor data set compare performance class distance number point pairs                                                                              dw     nw         xi − xj   different metrics metric learning figure different class labels   yiyj    shows  data points input space points mean withinclass distance nw number  color mark belong class ran point pairs class label note closely  domly generated point pairs corresponding similarity lated optimization criterion js  js  constraints shown solid lines figure shows deﬁned labeled data data involved pairwise   new data points input space results obtained constraints data assuming exis  rca rbf kernel versions method shown tence true class labels kernel methods use φxi                                                             figure c–f rca performs metric learning directly φxi place xi apply kernel trick compute  input space generalized new data using mean distances larger value corre  learned linear transformation kernel methods sponds better metric higher class separability  apply kpca using learned kernel matrix embed ﬁrst perform experiments larger xor  data points dimensional space shown fig data set  data points randomly select  sim  ure d–f new data points embedded using ilarity constraints form measure metric learning  generalization method described section  expected performance terms value increasing number  rca does perform satisfactorily xor data set landmarks table  shows results different metrics  perform linear transformation metric learning methods rca method  hand kernelbased metric learning method group show each trial mean upper standard devia  points according class membership result tion lower  random runs corresponding different  scalable kernel learning method lowrank approximation sets results method outper  based  landmarks good basic forms methods signiﬁcantly using  algorithm based  data points result landmarks generally gives better results  embedding new data points veriﬁes effectiveness perform experiments realworld data  outofsample generalization method              sets isolet data set uci machine                                                        learning repository contains  isolated spoken    quantitative performance comparison              english letters belonging  classes each letter repre          let yii set true class labels data points sented dimensional vector data sets hand  deﬁne following performance measure           written digits mnist database digits                                                        database sizenormalized centered ×                            db                                                                               dw                             httpyannlecuncomexdbmnist                                                    ijcai                                                    
