              combining learning constraints numerical regression                                          dorian  sucˇ  ivan bratko                national ict australia sydney laboratory unsw nsw  australia                   faculty information science university ljubljana                                    trˇzaˇska   ljubljana slovenia                                   doriansuc ivanbratkofriuniljsi                        abstract                            qualitatively faithful quantitative learning called                                                         learning short proposed sucˇ et al  rectify      usual numerical learning methods primarily    qualitative problems numerical learning  learning      concerned ﬁnding good numerical ﬁt data combines qualitative numerical learning numer      make predictions correspond ical predictions ﬁt data consistent      qualitative laws domain modelling                                                       induced qualitative model qualitative consis      expert intuition contrast idea learn tency beneﬁcial terms explanation phenomena      ing induce qualitative constraints training modelled domain quite surprisingly case study       data use constraints guide numerical learning shows induced qualitative constraints      gression resulting numerical predictions improve numerical accuracy paper extends previous      consistent learned qualitative model work directions      beneﬁcial terms explanation phenomena   contribution paper learning scheme      modelled domain improve nu  combines learning monotonic qualitative constraints      merical accuracy paper proposes method arbitrary numerical learner enables study accu      combining learning qualitative constraints racy beneﬁts induced constraints qualitative learning      arbitrary numerical learner explores  previously used number applications      accuracy explanation beneﬁts learning  mainly tied dynamic systems control applica      monotonic qualitative constraints number                                                       tions advantages terms explanation terms      domains show  learning correct     control performance induced qualitative models      errors caused bias learning algo observed models deﬁne constraints      rithm discuss potentials similar hierar class variable direct assessment accuracy beneﬁts      chical learning schemes                          previously possible second contribution                                                        empirical evaluation number domains demon                                                        stration regression guided induced qualitative    introduction                                       constraints increases numerical accuracy  learning understandable models main goals analyze reasons accuracy improvements  machine learning recently overshadowed show learning corrects errors caused  methods mainly concentrate classiﬁcation regres bias learner respect learning similar  sion accuracy effort devoted improving ensembles classiﬁers particular approaches com  explanation strength machine learning methods bine classiﬁers constructed different learning algorithms  duced models complex overly detailed combining instance modelbased learning quin  provide understandable explanation phenomena lan  stacking variations wolpert   modelled domain particularly notable methods gama brazdil  todorovski dˇzeroski    achieve excellent accuracy constructing ensembles important distinction learning qualitative  classiﬁers overview dietterich  problem consistency models different levels abstractionwe  illustrated discussed sucˇ et al  stateof discuss advantages similar hierarchical learning schemes  theart numerical machine learning methods make pre demonstrate explanation improvements  dictions knowledgeable user ﬁnds obviously incorrect necessarily come price lower accuracy  – numerical qualitative terms section  learning scheme proposed  qualitative errors numerical predictors undesirable par paper elements details learning  ticularly make numerical results difﬁcult described section  experimental results  terpret underlying mechanism domain usually various domains study accuracy improvements using  best explained qualitative terms obscured biasvariance decomposition section  discusses results  qualitative errors numerical predictions       beneﬁts learning gives directions future work   learning quin qﬁlter                                                                                                                                idea learning combine qualitative numer                                                                                         ical learning ﬁnd regression function ﬁts data              consistent induced qualitative model  paper learning consists stages                                                        figure  qualitative tree induced examples                        program quin   described section  induces function  sinΦ  ≤ Φ ≤ π      qualitative tree numerical training examples qual right leaf applies Φ π says mono      itative trees similar decision trees mono tonically decreasing Φ monotonically increasing      tonic qualitative constraints leaves    algorithm qﬁlter described section  uses   data points                                                                 points satisfying  ma      duced qualitative tree plus training examples plus class       predictions arbitrary numerical learner alter      predictions respect induced qualitative tree qﬁl       ter optimization procedure ﬁnds minimal      quadratic changes class values achieve consis       tency qualitative tree experiments        numerical learners called baselearners regres class   ma       sion trees model trees locally weighted regression                                                                                             attribute    paper learner consists qualitative                                                                                                   straints learner numerical baselearner             noted quallearner baselearner abbreviate figure  achieving consistency mqc   common case qquin baselearner simply qbase                                                        class values ci denoted circles changed  di  learner learning scheme particulary interesting denoted crosses minimizing sum squared  cause baselearner’s predictions changed optimally                                                        changes di arrows denote class changes di  sense squared error differences  baselearner’s predictions predictions come just  induced qualitative constraints                      tree note simple qualitative tree rel                                                        atively complicated nonlinear function qualitative trees    elements   learning                          induced numerical data learning program quin                                                        sucˇ     monotonic qualitative constraints                 quin constructs qualitative tree topdown greedy       qualitative trees quin                       fashion similar decision regression tree learning algo  monotonic qualitative constraints mqcs kind rithms split selection criterion based minimum  monotonicity constraints widely used ﬁeld description length principle takes account encod  qualitative reasoning generalization mono ing complexity subtrees consistency mqcs  tonic function constraint used qsim kuipers subtrees corresponding data “ambi  simple example mqc  says    guity” mqcs respect data unam  monotonically increasing dependance biguous qualitative predictions mqc make bet  increases increases general ter  mqcs argument example         −                                              qﬁlter algorithm        says monotonically increasing  monotonically decreasing  qﬁlter handles each leaf qualitative tree separately  increase according constraint increase ﬁrst splits examples according qualitative tree  decrease stay unchanged case mqc changes class values achieve consistency mqcs  make unambiguous prediction qualitative change corresponding leaves                                                       let ﬁrst observe simple example figure     qualitative trees similar decision trees examples aici    class values ci  monotonic qualitative constraints leaves figure  gives attribute values aii examples                                                                                         example simple qualitative tree qualitative tree sistent given mqc  mqc                                       sinΦ          requires ci ci violated   qualitative model function                                           ≤ Φ ≤ π describes qualitatively achieve consistency  class values  pends attributes Φ tree partitions attribute changed ci  di unknown param  space regions correspond leaves eter di denotes change ith class value class                                                        changes di constrained mqcimposed inequalities    quin  graphical user interface qﬁlter lo ci  di ci  di     inequal  cally weighted regression baselearner available ities formulated matrix notation ad   httpaifriuniljsidorianqquinhtm           vector unknown parameters di vector elements bi  ci − cii matrix elements aii  −  aii zeros general depend  mqcimposed inequalities turn depend  mqc ordering attributes’ values    ﬁnding minimal quadratic changes class val  ues achieve consistency given mqc posed  quadratic programming optimization problem ﬁnd                                         vector minimizes criterion function hdsuch figure  planar twolink twojoint robot arm ﬁrst  ad   formulation matrix identity link extendible length ranging    matrix general changed differently penalize  changes class values described section     criterion function diagonal matrix ples baselearner used predict class values test ex  convex function linear constraints ad  amples training examples used quin  deﬁne convex hull local minimum criterion func induce qualitative tree qualitative tree training  tion globally optimal solution elaborate descrip test examples baselearner’s class predictions  tion qﬁlter deﬁning appropriate ordering class used qﬁlter predictions consis  values attributes used mqc tent qualitative tree procedure repeated  given sucˇ bratko  previous work qﬁlter example times tenfold crossvalidation  used qualitative trees derived manually domain different training test examples                                                          experiments compare root relative squared er  knowledge use different challeng                                      ing context qualitative trees induced data rors rres short baselearner learner                                                        rre root mean squared error normalized                       qﬁlter  learning                           root mean squared error average class value base  qﬁlter supplied qualitative tree training exam learners study regression model trees breiman  ples class values test examples base et al  quinlan  locally weighted regres  learner’s class predictions qﬁlter adjusts class val sion atkeson et al  ﬁrst baselearners  ues training test examples achieve consis chosen wellestablished numerical learners  tency qualitative tree                      provide symbolical model locally weighted regres    improvement qﬁlter use baselearner’s sion lwr short does provide model explaining  conﬁdence estimates predictions case qﬁlter studied domain gives accurate predictions                                                        model regression trees baselearners  makes smaller adjustments class values higher                     conﬁdences expense larger changes class val explanation beneﬁts learning obvious  ues lower conﬁdences achieved chang experiments used implementations base  ing quadratic programming criterion function learners regression model trees use costcomplexity  matrix changed identity diagonal matrix pruning breiman et al  smoothing lwr uses  elements hii  wi weight wi computed gaussian weighting function local optimization set  baselearner’s conﬁdence estimate ith class value kernel size each prediction point rres  course computation weight wi depends type model trees quinlan  weka implementation                                                        called mprime witten wang  show  scale conﬁdence estimates generally larger         numerical predictor conﬁdent ith class comparing baselearners perform poorly                                                        learners default values parameters used  prediction                                                                             based idea used heuristic weighting function analyze reasons accuracy improvements                                                               −ci−µ                     using biasvariance decomposition geman et al   wi signµ−ci−exp            ci denotes                               σ                       draw interesting conclusions  baselearner’s conﬁdence class prediction ith  test example µ σ denote mean standard  robot arm domain  deviation baselearner’s conﬁdences test examples  weight test example zero experiments domain planar  weight training examples set link twojoint robot arm depicted figure  angle  experiments locally weighted regression conﬁdence es shoulder joint denoted Φ angle elbow                                                                         Φ          Φ  timates ci set sizes conﬁdence intervals model joint denoted  angle  zero π  regression trees provide similar conﬁdence esti Φ −π π ﬁrst link  mates reason conﬁdence estimates test exam link shoulder elbow joint extendible  ples set                                 length ranging   second link ﬁxed                                                        length  ycoordinates ﬁrst second link ends    empirical evaluation                               denoted respectively experimented                                                        learning problems differ class variable     experimental details                             attributes used learning learning problems  evaluate accuracy beneﬁts learning var deﬁned pose increasingly difﬁcult problems  ious numerical baselearners given set training exam learningfigure  comparing rres baselearners  learners learning problems    noise                                                          general baselearners accurate  table  rres lwr model mt regression trees rt                                                       mprime learning problems  corresponding learners noise interested comparison different baselearners  column gives rres mprime                         good know improving base                                        lwr     lwr   mt      mt    rt     rt   mpr     learners perform poorly                     signiﬁcance accuracy improvements each                                                                       learning problem tested using resampled paired test                                       ×                  ×                   learning problems three baselearners three noise                                                        levels gives  comparisons rre baselearner                                                        corresponding learner  signiﬁcance level                                                        learners signiﬁcantly better  comparisons    easiest learning problem called problem pre three comparisons comparisons  dict ycoordinate ﬁrst link end given length                          Φ                          differences rre signiﬁcant correspond model  angle learning   learning problems trees learning problem three noise levels  require predicting ycoordinate second link end  ing different attributes learning problems biasvariance decomposition                                     Φ     Φ   Φ  helped learners derived attribute sum   understand reasons accuracy improvements  deﬂection second link horizontal                                                           Φ  Φ   Φ               used biasvariance decomposition geman et al   problem learn    sum   domingos  proved useful  problem used attribute problem tool understanding machine learning algorithms bias  quires learning  fl Φ Φ problems pose                                                       variance decomposition regression states expected  increasingly difﬁcult problems learning eas squared error learner test example sum  iest problem correct qualitative model irreducible noise nx bias bx variance  simple qualitative tree given figure  learning prob                                                       bias bx learner example squared  lem difﬁcult  correct qualitative difference true value mean prediction  model expressed qualitative tree                                              bx                                                         possible training sets deﬁned bias    compare accuracy different baselearners literature called squared bias use notation  learning generated examples angles Φ  Φ domingos  bias variance  link length randomly generated uniform expected value squared difference true  distribution experimented different percentages value mean prediction bias measures  gaussian noise class variable noise percentage systematic error incurred learner variance mea  means standard deviation noise × dc sures error incurred ﬂuctuations central  dc denotes difference maximal min tendency response different training sets irreducible  imal class value used  training examples mea noise error optimal bayes model gen  sured accuracy separate test sets  examples eral difﬁcult estimate using  noise results averages  randomly generated artiﬁcial learning problem measure bias  training test sets                               variance directly simulating deﬁnitions    table  gives rres lwr model regression trees used  training sets size  generated  learning uses baselearners compar previous experiment measured average bias aver  ison rres mprime model trees given results age variance test set  equidistant data points  zero   noise given figure  refer averages different training sets differ  learning problems noise levels  improves ent test examples simply bias variance comparing  average rres baselearners improvements baselearners noticed regression trees highest  accuracy depend baselearner learning prob variance accordance complexity –  lem generally improvements greatest usually  leaves model trees smaller  gression trees smallest model trees notable smallest variance noise increastable  description data sets average fold crossvalidation root relative squared errors rres baselearners  corresponding learners column gives rres available mprime     data set   cases   attributes  lwr    qlwr     modtr  qmodtr  regtr qregtr      autompg                                                 autoprice                                              housing                                             machinecpu                                                 servo                                                craneskill                                             craneskill                                              antisway                                                                                                                     vgain   servo qualitative tree   ing noise variance increases notably comparing                    learners corresponding baselearners noticed      notably reduces bias baselearners mpgain     mpgainvgain  happens learning problems three base  learners different noise levels example problem      peakrpm  autoprice qualitative tree   noise reduces bias variance lwr                                                                                      respectively        tably decreases variance regression trees     height  losses    times increases variance lwr model trees  baselearners used bias linear models  restrictive monotonicity constraints losses  engsize  wheelbase    reduces bias does considerably increase vari  ance                                                                                    learning combines hypotheses different learning                                                         mctympgstrokepeakrpm    mcurbwghtwidth   algorithms qualitative learner numerical learner mlosses       mcurbwghtctympg  respect similar ensembles classiﬁers mwheelbase      mcompratio  particular approaches combine classiﬁers constructed mctympgborecurbwght  different learning algorithms example stacking  variations methods improve accuracy mainly reduc figure  qualitative trees induced data sets servo  ing error bias learner conse autoprice mqcs leaves each qualitative tree  quence combining hypotheses make uncorrelated er monotonic constraints class variable  rors dietterich  believe uncorrelated  errors lead bias reductions case learning  noted predictions consis data sets craneskill craneskill logged data  tent induced qualitative model combine experienced human operators controlling crane simula  baselearner’s predictions qualitative model tor control traces typically used reconstruct                                                        underlying operator’s control skill learning task    uci dynamic domains                          predict velocity crane trolley given position  explore potentials learning similar constraints trolley rope angle velocity data set antisway  wider range learning problems experiments used reverseengineering industrial gantry crane  data sets ﬁrst ﬁve smallest regres troller socalled antisway crane used metallurgi  sion data sets uci repository blake merz cal companies reduce swing load increase   majority continuous attributes reason productivity transportation slabs learning task  choosing data sets quinlan  gives learn control force applied trolley given  results regression methods desired current trolley velocity position  data sets enables better comparison velocity load relative trolley  methods data sets autompg autoprice housing experiments qualitative trees considerably sim  machinecpu servo                                 pler induced models figure  gives examples    three data sets dynamic domains qualitative trees induced data sets servo autoprice  quin typically applied far sucˇ  sucˇ qualitative trees considerably simpler model  bratko  noted domains trees servo mprime induces model tree  primary objective explain underlying control skill leaves attributes appearing linear  use induced qualitative models control dy models autoprice mprime induces model tree  namic possible measure leaves attributes each linear model  numerical accuracy compare learning methods tenfold crossvalidation results given table 
