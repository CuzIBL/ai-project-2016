        empirical study noise impact costsensitive learning                xingquan zhu  xindong wu taghi khoshgoftaar yong shi      dept science  eng florida atlantic university boca raton fl  usa          department science university vermont burlington vt  usa               graduate university chinese academy sciences beijing  china                  xqzhu taghicsefauedu xwucemsuvmedu yshigucasaccn                      abstract                      rithm behaves noisy environments handle                                                  data errors supporting effective cs learning paper     paper perform empirical study report empirical study impact noise cs     impact noise costsensitive cs learning learning hoped observations beneficial     through observations cs learner reacts                                                   realworld applications cost concerns     mislabeled training examples terms mis     classification cost classification accuracy      empirical results theoretical analysis indicate  related work     mislabeled training examples raise problem learning noisy data focus     concerns costsensitive classification especially attention data mining quinlan al     misclassifying classes ex gorithms mechanism handle noise training     tremely expensive compared general inductive data example pruning decision tree designed     learning problem noise handling data reduce chance tree overfitting noise     cleansing crucial carefully training data quinlan  realworld applications     investigated ensure success cs learning learning algorithms rely data cleaning model data                                                  enhancement brodley  friedl  problem   introduction                                  error handling supervised learning stud  recently body work attempted address ied research area mainly focused general  costrelated inductive learning issues techniques inductive learning minimization zeroone loss  known costsensitive learning tan  turney  paz error rate reality applications charac  zani et al  “cost” interpreted mis terized error rate various types costs tur  classification cost training cost test cost turney ney misclassification cost test cost  different types costs misclassification cost different types costs misclassification  popular general misclassification cost cost popular assume examples   described cost matrix ci indicating cost dataset drawn independently distribution  predicting example belongs class fact domain xyc input space classifier  belongs class type cost objective output space    importance mis  cs learner form generalization average                                                  classification cost associated misclassifying ex  cost previously unobserved instances minimized ob                                                  ample goal costsensitive cs learning  viously minimal cost determined impor  tant factors  inductive bias underlying cs misclassification cost perspective learn classifier  learner  quality training data existing minimizes expected cost zadrozny   research significant progress exploring efficient  ed yc ihx    cs learning algorithms bradford et al  zuberk  diet                                                    important issues determine minimization  terich  tan  geibel  wysotzki  brefeld et al                                                   expected cost total number misclassifications  domingos  chan  stolfo  zadrozny  abe    zadrozny  assumptions input data cost each single misclassification minimize overall  noisefree noise datasets significant data costs compromise sacrificing cheap ex  driven application domains potential problems amples enhancing accuracy classes containing  unreliable data acquisition sources faulty sensors expensive instances distinct general inductive  data collection errors make data vulnerable errors learning biased larger classes  important understand cs learning algo likely cheap examples cs scenario enhance                                                   cs learner trained noisy data environments zhu                                                  wu  proposed classification filter data      support national science foundation china cleansing study cs learners noisy environ  der grant acknowledged          ments lacks indepth empirical theoretical analysis                                                 ijcai                                                experiment setting                            table  class distributions benchmark datasets                                                           dataset     class distribution separability   empirical study needs control noise levels                                                                                   credit screening credit      data observe behaviors wisconsin diagnostic   lying cs learners purpose implement breast cancer wdbc     manual corruption mechanisms total random corruption thyroid disease sick     trc proportional random corruption prc table  major symbols used paper  trc users specify intended corruption level symbol         description   randomly introduce noise classes                                                         training set test set  noise corrupted  instance label  chance mis training set  noise cleansed training set  labeled random class excluding class trc csx average misclassification cost dataset  changes class distribution dataset accsx average classification accuracy cs  noise corruption raises big concern cs learning acnmx normal classifier dataset respectively   modifying class distribution change ht vs ht cs classifier vs noncs classifier                                                              input instance class label cost   average cost considerably propose prc     associated mislabeling   keeps class distribution constant during noise corrup                                                          cost ratio minor class major   tion given dataset classes assume original                class                                                               number instances test set   class distribution         percentage common classes  cost predicting example belongs                                                      class fact belongs class  respectively user specified corruption level say                                                          distribution major class vs minor class  proportionally introduce random noise differ classification error rate test set  ent classes instances class having        major class examples minor class examples   chance corrupted random class            respectively                                                             cost classifying major class example   use common class baseline propor cs                                                        min              minor class   tionally introduce noise different classes obvious                                                      csavg  average misclassification cost test set   actual noise level prc csupperbound upper bound cs classifier  intended corruption level                dxyc   distribution noisy training set    each experiment perform  times fold cross    distribution new set constructed sam                                                    xyc              pling  validation each run dataset divided training                                                      edxyc expected average misclassification cost  set test set introduce certain level noise cihx instances drawn distribution  generate noisy dataset  assuming  noise cleansing technique able remove noisy  noise impact  stances  build cleansed dataset   observe cs learners trained    misclassification costs   study noise impact assessed using test set figs   report noise impact cost cs   empirical study observation three bench learners trained benchmark datasets figs   mark twoclass datasets credit screen dataset credit figs represent results   wisconsin breast cancer dataset wdbc thyroid different noise corruption models wdbc   disease dataset sick blake  merz  class sick dataset respectively fig reports results   distributions varying extremely biased credit dataset class distributions   shown table  use twoclass datasets test credit dataset report results    bed twoclass problem explicitly reveal trc model figures second rows   impacts noise costratio cs learning xaxis represent intended actual noise levels   multipleclass problems issues costs classes dataset yaxis indicates average cost cs   class distributions errors each class make classifiers cse  cse  represent average cost    difficult draw comprehensive conclusions use cs classifier trained   noise   cs classification tree quinlan  experiments cleaning    assign misclassification cost matrix values ci ij figs   noise corruption   adopt proportional cost pc mechanism models average cost cs classifier proportionally   classes ij check class distribution increases value dataset noise   ij means class relatively rarer free does surprise fix cost  cj ci equals  ci  set cost cj  raising  ci cj  costratio value surely increase average cost  defined eq                               noise introduced dataset average cost                            evitably increase regardless noise corruption model                                                   value hand removing noisy                                                  ijcai                                                stances average costs  significantly larger increase    noisefree dataset indicates data cleansing  obvious given   effective way reduce impact noise cs noise dataset noise does change class   learning                                       distribution dataset large costratio tends     noise increases cost cs classifier error prone receive large misclas   gardless costratio comparing results sification cost observations indicate   three values patterns dataset large costratio value existence noise  crease different shown fig fig fatal small portion noise corrupt   fig small increasing noise levels results significantly sensitivity minor data  gradually raise average cost impacts noise errors dataset large costratio experience  significant low noise level data hand difficulties data cleansing overall goal mini  large values introducing small portion noise mize misclassification cost  elevate average cost greatly increasing noise interesting finding figs    level does show significant extra impacts applying cs learners noisy dataset  trc fig fig fig trained cs classifiers saturation point reacting   changed class distribution dataset clear actual noise dataset regardless errors  given noise dataset change class distribution noise level  responsible increase cost change point continuously impact cs classi  class distribution large costratio turn fier noise level point classi  fig fig answers class distri fier insensitive noise following analysis  bution constant during noise corruption      reveal saturation point determined    shown figs depending bias important factors class distribution cost  class distribution dataset actual noise level ratio given twoclass dataset assuming class distri  dataset lower lower intended cor bution     denoting distribution  ruption level example fig intended major class minor class respectively cost   noise level  actual noise level database predicting minor class instance major class   just  refer section  tab  csmin csmin indicating opposite cost   reason noise level cost increase  major class minor class assuming built    increase  cs classifier dataset test set    argue increase examples classification error rates classifier   incurred increasing classratio raising                                                   test set major class    make minority class expensive                                                   respectively error rate minor class   assume classification accuracy remains number incorrectly classified minor class examples  average increase                                                                                                cse                                cse                                                                                             cse                               cse                                                                                             cse                              cse                                                                                             cse                               cse                                                       avg costs avg           avg costs avg                                                                                             cse                               cse                                                                                               cse                                                                                   cse                                                                         noise level                                noise level                wdbc dataset total random noise trc wdbc dataset proportional random noise prc                      fig  impacts class noise average cost cs classification algorithm                              cse                    cse                   cse                                                                                                                               cse                   cse                    cse                                                                                                                                 cse                      cse                     cse                                                                                                           avg costs avg                                                                   avg costs avg                                    avg costs avg                       cse                    cse                  cse                                                                                                                        cse                     cse                    cse                                                                                                                                        noise level     cse    noise level     cse    noise level    cse      sick dataset total random noise trc sick dataset proportional random noise prc  credit dataset total random noise trc                      fig  impacts class noise average cost cs classification algorithm                                                  ijcai                                                divided total number minor class instances section  tab  know    hown eq  average misclassification cost         csavg denoted eq                 csmin wdbc dataset according eq                                                   cs        consistent results                                       upperbound                                                  fig interesting results come results                                                                                     fig clearly poses saturation point                                  cs avg        cs min        cs min       misclassification cost according table  eq                                             know sick dataset                cs min              cs min   ratio   far larger value                                                   result csupperbound cs                                                                                     min      average error rate  increases normally raises corresponding second item eq  unfortu                                                   nately results fig  indicate real csup  average misclassification cost csavg value  bounded upper bound given eq       perbound got  actually equals value                                                   having instances classified minor class                      cs                                    min                    item eq  reality understandable                                            cs upper  bound                               underlying cs learners reluctant stick major                                         cs min          class minimal misclassification cost especially                                                 noise changed apriori class distributions signifi    actually upper bound eq  average cost cantly instead cs learners tend stick mi   predicting instances belonging minor class classes high uncertainty environments result       predicting instances belonging major results seriously biased dataset consistent   class    optimal solution cs conclusion drawn item eq     classifier low confidence cost higher based observations know given   upper bound necessary understandable datasets noise level different costratio    csavg csupperbound holds time eventu value larger cost ratio smaller value   ally produces inequality denoted eq   dataset likely approach satura                                          tion point misclassification cost appears reach   cs  cs                                   avg upperbound               otherwis  maximum class noise does continu                                            ously make impact cs classifier time                                                      noise reaches certain level cs classifier                                                                  simply stick minor class result impact                                                     noise constant given dataset                                                                         pretty common reality higher    class problems   eq                                                       costratio likely classifier tends                                                                   situations    conclusions likely hold                            inequality  finally analyze rela  classification accuracies  tionship class distribution   cost ratio observations conclude dataset  error rate  reveal following facts higher costratio sensitive data errors learners built    given test set average misclassification error data likely cost intensive conclu      rate  cs classifier trained training sion does solve concerns like        set equivalent bounded critical value  learner built noisy data large costratio cost inten                                                   sive  given noisy dataset users able      fined eq    higher value                                                 build normal classifier cs classifier       costratio lower  likely                                                   trustworthy identifying noisy examples      classifier tends approach saturation point                                                   let’s refer theoretical proof empirical com    theory                                                    parisons classification accuracies normal      approximated  means classifier cs classifier answers      larger number minor class instances larger                                                   following analyses based existing folk theo      higher  likely classifier tends                                                   rem zadrozny et al  states      approach saturation point conclu                                                   examples drawn distribution      sion crucially relies fact underlying cs      learner “optimal” knows sticking      major class lead lowest cost  called “folk theorem” literature authors      case reality analyze cluded result appears known pub                                                   lished                                                   ijcai                                                                                              ht built biased distribution  sampled            dx          dx     err err ht                                                                                                            need ht exist directly use ht  dxyc represents distribution original normal classification eq   dataset optimal error rate classifiers built statement theorem  true  newly constructed distribution optimal cost mini                                                                  err   err ht     mizers data drawn theorem states                    sample original distribution dataset let’s turn experimental results fig   constructing distribution  efforts try evaluate empirical results support  learn optimal error rate classifier  actually leads theoretical analysis meaning each curve fig   classifier optimizes misclassification cost denoted fig  fig  accse acnme  indi                                                   cate classification accuracies normal cs clas  theorem  dataset distribution assum sifier trained  respectively shown fig    ing able use optimal learning theory build costratio small differences  cs ht normal classifier ht respectively accuracies normal cs classifiers trivial  ht inferior ht terms average classification accuracy cs classifier slightly worse  accuracy errdh errdh errx  represents derstandable cs classifier likely sacrifices   classification error rate classifier accuracy minimal costs costratio gets larger  proof                                           higher accuracy cs classifier  assume cs classifier ht built significantly worse normal classifier compare  cs learning formula eq  expected cost ht fig fig fig fig  denoted eq                      actually reason dataset large                                                  ed ycc ihx dx yc ihx costratio willing approach saturation                        yc                     point situation small portion noise make   folk theorem eq  know  cs classifier ignore classification accuracy stick                              sam “optimal” class                                                theorem  results fig  clearly indi   pled distribution eq  cates general situations cs classifier inferior                                                   normal classifier terms classification accuracy      hx yci hx     yc        yc              cause removing mislabeled training examples shown                                                                              lead better cs learner shown figs   class                           hx                         yc    noise handing effective cs learning crucially   transform eq  follows           pend accurate noise identification mechanisms                                                   sult noisy environments want adopt noise                              ed xycihx ed xycc ihx identification mechanism data cleansing shall trust                      ed xycc             normal classifier cs classifier                                                 higher accuracy determining                              ihx                            xyc              right class instance                      ed xycc                                                           accse acnme accse                                                             becausee    arg max yc  eq                                                                                 xyc                                  accuracy                                                                                                                                                                          acnme rnoise level accse acnme                                                  ed  xycihx ed  xycihx        fig  meaning each curve fig       errd    ht     errd ht                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       accuracy         accuracy                                                                           accuracy                                                   accuracy                                                                                                                                                                                                                                                                                                                       noise level             noise level            noise level           noise level        wdbc dataset trc  wdbc dataset prc sick dataset trc sick dataset prc               fig  impacts class noise classification accuracy normal cs classification algorithms                                                  ijcai                                               
