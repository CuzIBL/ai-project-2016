duallayer crfs based joint decoding method cascaded segmentation                                             labeling tasks                          yanxin shi                                 mengqiu wang               language technologies institute              language technologies institute                 school science                   school science                  carnegie mellon university                   carnegie mellon university                     yanxinscscmuedu                           mengqiucscmuedu                        abstract                          diction earlier tasks sutton mccallum asev                                                        eral new techniques proposed recently address      problems nlp require solving cas                                                        problems sutton et al  introduced dynamic      cade subtasks traditional pipeline approaches                                                        conditional random fields dcrfs perform joint train      yield error propagation prohibit joint train ingdecoding subtasks disadvantage model      ingdecoding subtasks existing solutions                                                        exact inference generally intractable      problem guarantee nonviolation                                                        come prohibitively expensive large datasets sutton      hardconstraints imposed subtasks  mccallum  presented alternative model decou      rise inconsistent results especially cases                                                        pling joint training performing joint decoding      segmentation task precedes labeling task                                                        kudo  et al  presented conditional random      present method performs joint decod    field crf lafferty et al  based model      ing separately trained conditional random field                                                        forms japanese word segmentation pos tagging jointly      crf models guarding violations                                                        popular approach joint decoding cascaded      hardconstraints evaluated chinese word seg  tasks combine multiple predicting tasks single      mentation partofspeech pos tagging tasks                                                        tagging labeling task luo  ng low       proposed method achieved stateoftheart                                                        yi palmer  miller et al       formance penn chinese treebank      sighan bakeoff datasets segmen      aforementioned approaches work      tation pos tagging tasks proposed method cases segmentation task comes labeling      consistently improves baseline methods  task cases segmentation task      perform joint decoding                    imposes hardconstraints violated succes                                                        sive tasks example chinese pos tagger assigns                                                        different pos labels characters word    introduction                                       deﬁned word segmenter word single  exists class problems involves solving cas consistent pos labels similarly constituent constraints  cade segmentation labeling subtasks natural lan imposed syntactic parsing npchunking tasks disallow  guage processing nlp computational biology argument overlaps semantic role labeling pradhan et al  stance semantic role labeling srl relies heavily  graphical modeling’s perspective mod  syntactic parsing nounphrase chunking npchunking els assign nodes smallest units base segmen  segment group words constituents based tation task case chinese word segmentation  constituent structure identiﬁes semantic argu smallest unit chinese character result  ments assigns labels xue palmer  models ensure consistency segmentation  pradhan et al  asian languages labeling tasks instance ng low   japanese chinese delimit words space evaluate pos tagging results percharacter basis instead  solving word segmentation problem prerequisite solv perword basis hindered problem kudo  ing partofspeech pos labeling problem exam et al  considered words predeﬁned lexicon  ple computational biology ﬁeld dna coding constructing possible japanese word segmentation paths  region detection task followed sequence similarity based puts limit generality model  gene function annotation burge karlin       tackle problem propose duallayer crfs    previous approaches treat cascaded tasks processes based method exploits joint decoding cascaded se  chained pipeline common shortcoming ap quence segmentation labeling tasks guards  proaches errors introduced upstream tasks prop violations hardconstraints imposed segmentation  agate through pipeline easily recovered task method model segmentation labeling  pipeline structure prohibits use predic tasks duallayer crfs decoding time ﬁrst perform  tions tasks later chain help making better pre individual decoding each layer individual                                                    ijcai                                                    codings probabilistic framework constructed order note joint probability tc factorized  ﬁnd probable joint decodings subtasks terms twcs sc ﬁrst term repre  training time trained cascade individual crfs sents probability pos tagging sequence built  subtasks given application’s scale joint training segmentation sentence second term  expensive sutton mccallum aevalu represents probability segmentation sequence  ated chinese word segmentation partofspeech pos maximizing product terms viewed  tagging tasks proposed method achieved stateoftheart reranking process particular sentence maintain  performance penn chinese treebank xue et al list possible segmentations sentence sorted   sighan bakeoff datasets sproat emer probability sc each segmentation  son  segmentation pos tagging tasks list ﬁnd pos tagging sequence  proposed method consistently improves baseline meth maximizes probability twcs using product  ods perform joint decoding particular probabilities rerank segmenta  port best published performance open track tion sequences segmentation sequence reranked  sighan bakeoff dataset best average list possible segmentations  performance open tracks                  ﬁnal segmentation sequence output probable    facilitate discussion later sections use pos tagging sequence segmentation ﬁ  chinese segmentation pos tagging working exam nal pos tagging output ﬁnal pair maximizes  ple illustrate proposed approach joint probability tc  clear model applicable cascaded sequence intuitively given segmentation sentence  labeling problem                                     maximum probabilities pos tagging sequences built                                                        segmentation small signal    joint decoding cascaded sequence               tells high chance segmentation incorrect     segmentation labeling tasks                    case able ﬁnd segmentation                                                        does probability high ﬁrst  section using chinese sentence segmentation pos best pos tagging sequence built segmentation  tagging example present joint decoding method reasonable probability joint proba  applicable cascaded segmentation labeling tasks bility tc increased    uniﬁed framework combine chinese              nbest list approximation decoding       sentence segmentation pos tagging            ﬁnd probable segmentation pos tagging se                                                        quence pair exact inference enumerating possi  let    cc  cn denote observed chinese                         th                             ble segmentation generally intractable number  sentence ci chinese character sen possible segmentations sentence exponential  tence  ss  sn denote segmentation sequence                  ∈                                   number characters sentence  si     bi   represents segmentation tags  overcome problem propose nbest list ap  begin inside word  tt  tm                                      ≤            ∈    proximation method instead exhaustively computing  note pos tagging sequence    tj       list possible segmentations restrict rerank  thesetofpossibleposlabels goal ﬁnd seg                                                        ing targets nbest list   snwhere  mentation sequence pos tagging sequence max                                                         sn ranked probability sc                                     ˆ     ˆ  imize joint probability lets denote approximated solution maximizes joint probabil  likely segmentation pos tagging sequences ity tc formally described  given chinese sentence respectively applying chain      ˆ    ˆ                                                   sˆ tˆ  argmaxp   tc  rule obtained follows                                  s∈st         sˆ tˆ  argmaxp   tc                                  argmaxp    twcsp sc                          st                                                   s∈st                 argmaxp     ts cp sc          comparing similar work uses nbest lists                       st                              svm   reranking daume marcu  asa                 argmaxp     twcsp sc      hara et al  perform rulebased postprocessing                       st                              error correction xue shen  gao et al                                                         ng low  method unique advantage  equation  rewritten equation  given se                                                      outputs just best segmentation pos sequence  quence characters  cc  cn segmenta joint probability estimate probability esti  tion sentence interpreted sequence mate allows natural integration higher level nlp  words wcs                                   wm                       applications based probabilistic models                                                        reserves room joint inference    note segmentationpos tagging sequence pair mean  ingful pos tagging sequence labeled basis  duallayer conditional random fields  segmentation result pair consistent  proposed method joint probabilities tc inconsis section  factorized joint probabil  tent pairs deﬁned              ity terms sc twcs notice                                                    ijcai                                                    terms probabilities label sequence given  observed sequences use conditional ran           tj    tj          tj  dom fields crfs lafferty et al  deﬁne       cd       jj          nn  probability terms    crfs deﬁne conditional probability zx markov        wj    wj      wj  random ﬁelds case chinese segmentation pos                                        second                                                                   si    si      si    si  tagging markov random ﬁelds crfs chain struc                                         layer                                                               bbbi  ture sequence characters words layer  segmentation tags characters used indicate                                                                   ci    ci     ci     ci  word boundaries pos labels words nn vv jj  conditional probability deﬁned                           t  k                       figure  duallayer crfs tc joint probability                           zx       exp       λ xt      segmentation sequence pos tagging sequence                  nx                                                            given sentence modeled duallayer crfs                                                        ﬁrst layer crf observed nodes characters  nx  normalization term guarantee sentence hidden nodes segmentation tags  summation probability label sequences characters second layer crf given segmentation                 th  fkz xt localfeaturefunction sequence po results ﬁrst layer characters combine form ”su                                                    sition maps pair index   pernodes” words words observed variables  λ  λk  weight vector learned training set pos tagging labels hidden variables    model separately probability terms deﬁned                           model wcs using duallayer                                                                                             scaling algorithm iis della pietra et al  maxi  crfs figure  probability want mize loglikelihood ﬁrst layer crf learn  maximize written                                        λ                                                                    parameters  λ  λkt using iis maximize     tcp    twcsp sc               loglikelihood second layer crf detailed deriva                                                      tion learning algorithm each learning step                              ×                        lafferty et al                    nt wcs    nsc                         m  kt                           features crfs                   × exp       λkfkt wcsj                                                features word segmentation                                                        features used word segmentation listed                         n  ks                   ×                                    half table  feature  basic segmenta                     exp       μkgks ci        tion features adopted ng low                                                                                  lbeginc lendc lmidc represent maxi  number words characters mum length words lexicon contain cur  sentence respectively nt wcs nsc rent character ﬁrst middle character  malizing terms ensure sum probabilities spectively  singlec indicates current  possible λk μk parameters character single word lexicon  ﬁrst second layer crfs respectively fk adopted basic features mentioned  gk localfeaturefunctions ﬁrst sec experimented additional semantic features table  ond layer crfs respectively properties functions   features semc refers se  common crfs described         mantic class current character semc− semc    nbest list segmentation sequences represent semantic class characters position  value corresponding probabilities scs ∈s left right current character respectively ob  obtained using modiﬁed viterbi algorithm tained character’s semantic class hownet dong  search schwartz chow  ﬁrst layer crf dong  characters multiple semantic  given particular sentence segmentation proba classes deﬁned hownet nontrivial task choose  ble pos tagging sequence probability twcs different semantic classes performed contex  inferred viterbi algorithm lafferty et al  tual disambiguation characters’ semantic classes calcu  second layer crf having nbest list segmen lating semantic class similarities example let assume  tation sequences corresponding probable pos current character lookread word context  tagging sequences use joint decoding method pro read newspaper character look seman  posed section  ﬁnd optimal pair segmentation tic classes hownet readanddoctor  pos tagging deﬁned equation              determine class appropriate check ex    divide learning process learning ample words illustrating meanings semantic  ﬁrst layer segmentation crf learning sec classes given hownet read example word  ond layer pos tagging crf learn parameters μ read book doctor example word                 μ  μks using algorithm based improved iterative doctor calculated semantic class                                                    ijcai                                                             segmentation features                        second best segmentation probability             cnn∈ −             cncnn∈ −                          foreign ownedcompanyproduction            c−c                                      situationsituationrelativelygood                                pu                                   difference ﬁrst sequence             c−t c−t ct ct                                                          relatively good segmented words            lbeginc lendc            singlec                              relativelygood despite lower probability            semc                                second segmentation appropriate char            semcnsemcnn∈−                acters compose word relatively good carry           pos tagging features                         meanings individual words            wnn∈ −                             traditional methods stopped use            wnwnn∈ −                       ﬁrst segmentation ﬁnal output actually            w−w                                  incorrect according goldstandard joint decoding                              ∈ −             wn−wnwnn                          method performs pos tagging based each                      ∈ −             cn                              segmentation sequences pos tagging sequence                                 len                                 highest probability  assigned ﬁrst segmentation            morphological features                         table  feature templates list            nn    nn  nn   nn   nn                                                            vv   pu   similarity scores newspaperandbook   nn represents noun vv represents verb   newspaperandillness using hownet’s builtin sim pu represents punctuation second segmentation  ilarity measure function newspaperandbook  assigned following pos label sequence highest  semantic class document maximum    probability   similarity score  maximum similarity score   newspaperandillness     nn    nn  nn   nn   nn    semcsemcreaddocument similarly             ad   va  pu   ﬁgure semc−semcforsemcwe        ad represents adverb va represents predicative ad  simply picked frequent semantic classes ranked jective  hownet used ”none” absent values           best pos sequence arising second segmen                                                        tation discriminative best sequence based  features pos tagging                              ﬁrst segmentation indicates second seg  half table  summarizes feature templates mentation informative pos tagging joint  employed pos tagging denotes current word probability second segmentation pos tagging se  w−n  wn refer words positions left                                                   th   quence  higher joint probability ﬁrst  right current word respectively cnw  method reranks second  character current word number characters best output according goldstandard sec  word  use ”none” absent characters ond segmentation pos tagging sequences  lenw  number characters current word correct sequences  used group binary features each word  used represent morphological properties current  word current word punctuation number  results  foreign                                    evaluate model using penn chinese treebank                                                        ctb xue et al  open tracks    illustrating example joint               ternational sighan chinese word segmentation bakeoff     decoding method                                    datasets sproat emerson   section use illustrating example motivate using linearcascade crfs set fea  proposed method example real output tures listed table  baseline compared perfor  gives suggestive evidences pos tagging helps mance proposed method accuracies word  predicting right segmentation right segmentation segmentation pos tagging measured recall                                                                                               rp  likely better pos tagging sequence precision fmeasure equals rp  seg  showing snippet sentence space limit mentation recall percentage correct words                                                        produced segmenter precision percentage     production sales situation foreign owned automatically segmented words correct pos tag               companies relatively good            ging recall percentage goldstandard words                                                        correctly segmented labeled pre  segmentation highest probability                                                         cision percentage words returned     foreign ownedcompanyproduction               correctly segmented labeled chose value       situationsituationrelatively good           using nbest list based crossvalidation results                                                    ijcai                                                      results segmentation                          average runs bolded en  segmentation ﬁrst evaluated joint decoding try indicates performs better average  method ctb corpus fold crossvalidation site results shown row table  formed corpus results summarized table  ofﬁcial runs team achieved best results                                                        open track achieved best runs open                                                  aso track score   higher     baseline             second best peng et al  comparing peng et     joint decoding       al  crfs based chinese segmenter                                    average        evaluated open tracks achieved higher perfor     baseline                 mance three tracks average score     joint decoding           tracks   higher peng                                                        et al’s comparing sites using average  table  comparison fold crossvalidation segmenta measures rightmost columns outperformed  tion results ctb corpus each column represents seven sites sites higher  fold crossvalidation results column average performance did signiﬁcantly better  average result  folds                     ctb open ctbo track ofﬁcial results showed                                                        systems obtained worst performance    seen table  results  ctbo track inconsistent segmentation style  fold tests improved joint decoding method training testing sets sproat emerson   conducted pairwise ttest joint decoding method  statistically signiﬁcantly better baseline                              −                                        aso  ctbo  hko  pko  savg oavg  method conﬁdence level  pvalues                                      evaluated proposed method open tracks                           sighan bakeoff datasets datasets designed             evaluation segmentation results pos tag                         ging information provided training corpus                          learning pos tagging model seg                   mentation model decoupled use separate training                    corpus learn second layer pos tagging crf model             able advantage proposed joint decoding peng et al ’        method results comparing baseline method           summarized table                                                         table  comparisons systems table                                    ctb              adopted peng et al                                        baseline              joint decoding         results pos tagging                        pk              hk              bakeoff competition does provide goldstandard                                          pos tagging outputs used ctb corpus compare     baseline             pos tagging results joint decoding method     joint decoding       baseline method performed fold crossvalidation                                                        ctb corpus results summarized table   table  overall results sighan bakeoff open  tracks stands precision stands recall stands                                  measure                                      baseline                                                                    joint decoding          comparison results previous work                                average  systems summarize results open baseline           tracks table  adopted table used peng et al joint decoding        consistency ease comparison   participating teams sites ofﬁcial runs table  comparison fold crossvalidation pos tagging  ternational sighan bakeoff show  teams results ctb corpus each column represents  participated open tracks each fold crossvalidation results column  row represents site each cell gives score average result  folds  site open tracks second ﬁfth columns  contain results  open tracks bold entry table  joint decoding method  dicates best performance track column savg higher accuracy each  fold tests  contains average performance site tracks baseline method pairwise ttest showed method  participated bold entry indicates site av signiﬁcantly better baseline method  erage performs better column oavg der signiﬁcance level − pvalue improve                                                    ijcai                                                    
