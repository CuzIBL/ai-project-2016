             monte carlo theory explanation bagging boosting                         roberto esposito                                         lorenza saitta                     universita di torino                            universita del piemonte orientale              cso svizzera  torino italy                     spalto marengo  alessandria italy                     cspositodiunitoit                                    saittamfnunipmnit                               abstract                               monte carlo theory section  core                                                                  paper introduces link monte carlo theory        paper propose framework                                                                  ensemble learning algorithms analyzes        monte carlo algorithms useful ana•                                                                 algorithms ideal situation having access        lyze ensemble learning particular                                                                  example hypothesis spaces section  relaxes        framework allows guess bagging                                                                  hypotheses section  particular focuses        useful explains increasing mar•                                                                 adaboosts case offering intuitive accounting        gin improves performances suggests new                                                                  generalization performances algorithm finally        way performing ensemble learning error                                                                  section  concludes paper proposes future work        estimation                                                                   bagging adaboost    introduction                                                                  bagging breiman  studied   ensemble learning aims combining learned hypothe•           ensemble learning algorithms takes input learn•  ses classification tasks instead choosing   ing set lo integer number possibly weak   proved valuable learning approach ac•       induction algorithm each iteration step algo•  tively investigated machine learning community          rithm creates bootstrap replicate efron  tibshirani   freund  freund  schapire  breiman             original learning set uses conjunc•    wolpert                                                tion form new hypothesis ht hy•                                                                 potheses learned combined using   addition advantages pointed dietterich        simple majority voting rule    really attracts great ensemble   learning amazing effectiveness robustness        breiman  significant increase classifi•  overfitting fact ensemble classifiers specifically   cation accuracy hypotheses bagged dif•  adaboost show drop generalization error far        ferent datascts uci repository regres•  point training error reaches zero     sion classification tasks pa•                                                                 theoretical analysis carried notion   paper wc concentrate approaches         order correct classifier introduced shown   bagging breiman  boosting implemented          order correct bagging expected   adaboost algorithm freund  schapire            behave optimally behaves like bayes   propose analysis approaches          classifier paper claimed bag•  exploiting links monte carlo theory parallel        ging stable classifiers good idea stable   allows deeper understanding basic process un•         classifier qualitatively defined does   derlying hypothesis combination particular                show large variations learned hypotheses      • suggests conditions bagging         training set undergoes small variations experimental          likely better single hypothesis learning    analysis baggings performances provided bauer                                                                  kohavi  quinlan  dietterich      • explains reducing margin schapire et ai                                                                             increases performances      • allows reasons robustness overfitting   simplest version adaboost freund  schapire          conjectured                                          deals binary classification problems   article organized follows section            refer adaboost maintains weight dis•  briefly review bagging adaboost algorithms         tribution training examples adjusts   section  introduce basic terminology results           each iteration adjusted weights chosen       learning                                                                                                               higher values associated previously misclassified                 definition   monte carlo algorithm pcorrect   examples main effect weighting scheme forces                  probability gives correct answer   focus difficult examples ensuring                   independently specific problem instance   examples sooner later taken               considered   account weak learner hypothesis combination                                                                            great monte carlo algorithms resides   rule weighted majority vote puts weight                                                                            amplification ability given problem instance   successful hypotheses hypotheses incur low                                                                            monte carlo algorithm ac mc run multiple   weighted training error                                                                            times majority answer taken result                                                                            mild assumptions probability correct   despite apparent simplicity adaboost solves diffi•                                                                           answer exponentially increases conditions arc   cult problem choosing example hypothesis weights                                                                            mc consistent greater   way ensure exponential decrease                                                                            random guess note reduces   training error committed boosted committee                                                                            mapping instead   algorithm   choice knowledge                                                                            consistent let denote algorithm ob•  assumption weak learner                                                                            tained combining majority vote answers   weak learner able outperform random                                                                            given multiple runs monte carlo mc   guessing contrast bagging works be•  nign weak learners used hand   adaboost sensitive noise bag•                    complete information case   ging quinlan                                                     section provide theoretical account                                                                            links monte carlo theory bagging boost•  mentioned earlier adaboost shows characteristic                  ing case complete information   robust respect ovcrfittiig schapire et                learningclassification problem precisely let     al  drucker  cortes  quinlan  bre                   finite set examples target concept binary clas•  iman  freund  schapire  con•                      sification weak learner   vincing account behavior imputes adaboost                     probability distribution let   ability successful increasing final                power set intensional concept description be•  classifiers margin training examples schapire                 longing given language mapped   etal                                                              element i»   set examples classified                                                                            description belonging way   effort understanding andor explaining                         reduce finite set extensional hypothe•  adaboosfs behavior boosting related                   ses let subset prob•  existing theories game theory schapire                  ability distribution associated elements   logistic regression friedman et al  collins et al                 set thought collection    optimization ratsch et al  brownian                  hypotheses generated learning algorithm applied   motion freund                                                    number training sets set hypotheses pro•                                                                           vided oracle learning simulated    monte carlo algorithms                                                 extracting replacement elements according                                                                            each extraction corresponds run   section recall basic notions monte                                                                            training set   carlo algorithms theory literature   monte carlo denotes generic stochastic algo•                                                                           let represent described situation means ta•  rithm paper refer restrictive                                                                            ble  let probability hy•  definition given brassard bratley  let                                                                            pothesis correctly classifies examplebe   class problems finite set answers                                                                            average probabilities ac•  problems function maps                                                                            curacy hypothesis   each problem set correct answers                                                                            using given probability distributions   definition   stochastic algorithm mapping                    compute average accuracy table   said monte carlo algorithm applied   instance terminates providing                                                                                                                                           answer  occasionally incorrect     properties monte carlo algorithms turn   relevant     definition   monte carlo algorithm consistent   outputs distinct correct answers run   times problem instance                                             finiteness essential assumed                                                                                sake simplicity                                                                                   assumption implies bayes error zero                                                                                                                                    learning                    table   theoretical setting                         condition  true long bayes error zero                                                                            condition  true depending                                                                            set                                                                              let set examples ac•                                                                           cording monte carlo theory examples                                                                            amplified correctly classified                                                                            limit  application                                                                            rows yields asymptotic accuracy                                                                                                                                                                                                                      denoting pt expected monte carlo accuracy   given subset measure wrt                                                                            finite number extractions performed                                                                              obtain                                                                                                                                           analogously  define                                             let set hypotheses accuracy                                                                            greater value                                                                                                                                           clearly measures reduce probability hypothesis better bag•          respectively uniform distributions             ging obtained single extraction infi•                                                                           nite number trials hypothesis occur    monte carlo bagging                                              frequency clear limit bagging                                                                            surely convenient   establish link monte carlo algo•                                                                           case single hypothesis better bag•  rithms bagging let         training set                                                                           ging better hypothesis surely ap•  integer let generic bootstrap replicate                                                                                      pear soon later interesting case occurs   case set different hypotheses                                                                            finite number extractions performed   obtained applying  replicates                                                                            approaching case let make consid•  frequencies each generated determine                                                                            erations described far     look table  bagging proceeds say                                                                            let notice ensemble learning claims am•  columns extracting columns using                                                                            plification occurs        accuracy each   obtain majority vote each considering                                                                                                                                      combined hypotheses greater  condition   global average accuracy exact distribution                                                                            columns actually case    easy say                                                                          really required each example     correctly classified majority voting procedure                                                                            learned hypotheses correct condition   perform bagging rewarding                                                                            rows monte carlo theory suggests bag•  generally said breiman  bagging useful                                                                            ging successful possible   unstable show later notion                                                                            subset  looking   instability precise                                                                            details relationship rjs                                                                                     let consider extreme cases   let consider monte carlo interpretation ta•  ble  contrary bagging monte carlo algo•                                                                           let weak learner returns hy•  rithm  proceeds rows eachproblem                                                                            pothesis regardless training set lucky   solved extracts hypotheses classifies                                                                            situation single hypothesis quite accu•   majority voting process repeated each                                                                          rate training set docs imply   row  procedure necessary compo•                                                                           occurrence monte carlo amplification fact   nents monte carlo voting independent                                                                            equal  examples arc cor•  entries each column monte carlo theory                                                                            rectly classified  words   tells correctness amplified                                                                            accuracy amplification oc•                                                                             curs example perfectly stable learner       trials independent       mc consistent                                                   let consider situation global average                                                                          accuracy little  value                                                                            time average column   condition  true construction process ex•                                                                           row table  according  sake il•  traction replacement hypotheses each row                                                                            lustration let uniform distributions let                                                                           consider following three extreme situations          let set apart moment computational issues       learning                                                                                                                                 tailed analysis interesting cases pro•        amplification possible   vided esposito                                      means learning procedure able    column distributed            make useful selection inside hypothesis space         way half examples val•            contrary like weak learner         ues close half examples      nice property focusing best hypotheses                 values close zero rjs row     previous section showed basic monte         contrary value          carlo approach implemented bagging fail          hypotheses show per•           number reasons happens instance         formances cover exam•           cases section  explicitly         ples                                                    stated underling problem weak learner    column                suited kind procedure           value  rjs       question arc trying investigate         row half values close     situation occurs particular         half close zero hypotheses       want modify weak learner pos•        good bad coverage     sible convert case case said         uniformly distributed examples               words possible convert learning problem    distributions uniformly valued         monte carlo assumptions amplification         hypotheses uncertain examples      hold     case half examples arc amplifiable bag•     solution problem thinking   ging useful single hypothesis classi•      role weak learner monte carlo   fies correctly half examples    process mentioned earlier learner duty   gained bagging process                                 modify distribution hypotheses   case number amplifiable examples depends        drawn qj columns way   actually values little         fctbctter hypotheses likely drawn moreo•   best case  bag•                         ver actually change behavior weak   ging convenient fact half single      learner through input modifying training   hypotheses   extractions      set particular force weak learner   average perfect hypothesis extracted                extract hypotheses make minimum value   case values columns arc       pxij grow larger  order satisfy monte   little greater  case              carlo conditions amplifiability examples   bagging convenient single hy•            weak learner influenced training set   pothesis accuracy close  actually case    solution modify training set way   ideal case bagging amplification       examples low values arc represented   obtained minimum effort bad hy•            attempt make weak learner try harder   potheses obtain large amplification                 successful obtain reasoning                                                                  adaboostlike algorithm words   following theorem summarizes consid•             weak learner unable satisfy monte carlo conditions   erations establishes link bagging            amplification adaboost forces satisfy   monte carlo amplification proof provided esposito       weighting examples precisely adabost tries                                                          increase minimum unconscious goal                                                                  extending  fact weak learner satisfies   theorem   amplified accuracy                                                                  relatively mild conditions shown schapire   greater average accuracy                                                                  et al  adaboost actually effective in•  bagged hypotheses                                                                  creasing minimal margin   bagging single hypotheses accuracy          definition   particular example classifi•   possible obtain higher accuracies    cation margin  difference   monte carlo amplification order reach       weight assigned correct label maximal     value  case   weight assigned single incorrect label   combined hypotheses allowed                                                                  case binary classification margin dif•  accuracy  hindering amplification                                                                  ference weight assigned correct label   provided does                                                                   weight assigned incorrect                                                                  theorem   case binary classification fol•   monte carlo adaboost                                   lowing relation holds margin monte   let notice set hypotheses coincides         carlo   power set     possible hypotheses con•                                                                                                                      sidered situation                                                                                                                  learning proof theorem  derives immediately                      unbiased estimate bernoulli probability px   definition margin  formula  explains                      hypotheses generated used     increasing margin improves performances                       classify discarded hinders   boosting fact increasing minimum margin lets                recording du•  cardinality set increase augmenting number                   plication columns table  consider jr   examples monte carlo conditions                     repeat procedure running new     amplifiability hold                                                    replicates classifying record                                                                           new generated hypotheses update probability dis•   nonasymptotic case                                                 tribution case repetitions finish exam•  let consider finite integer let denote prob•                 ining filled table corre•  ability hypothesis better bagging ob•                  sponding test set allows estimates   tained extractions order compare bagging               examples test set computed each   hypothesis selection let consider classification accu•            based experimental values number   racy given table  given number close                       runs necessary estimate generalization accuracy   let minimum number components                          using leaveoneout method number train•  bagged hypothesis                                             ing examples                                                                          interesting during learning learn   probability  zero                                        convenient exploit bagging                                                                           classifying future unseen examples turns ex•  analogously let set hypotheses rj    ra minimum number hypothesis extrac•                    perimentally lower number runs   tions                                                         tm sufficient obtain stable estimate                                                                           sought probabilities                                                                                                                                           adaboosts generalization behavior   probability  zero                                                                           section discuss adaboosts generalization ca•  draw graph ordinate vs abscissa                   pabilities interpretation monte carlo   parameterized graph problem dependent                         framework having related margin example   say general points graph                  average probability correctly classified   diagonal correspond situation se•                hypotheses learned weak learner allows   lection better bagging accuracy ex               transfer results provided schapire et al    bagging requires trials points diagonal                monte carlo framework interestingly   correspond situation selection worse               link used new intuitive interpre•  bagging accuracy bagging requires                    tation margin explanation adaboosts generali•  trials points diagonal correspond situa•                zation capabilities nice explanation   tions indifference                                                  test error observed decrease                                                                           training error reaches zero suggested    problems partial information                                                                           understand generalization error inter•  setting discussed section  useful under•                 preted monte carlo framework necessary   standing functioning ensemble learning                   deepen little discussion link en•  need extend case real learning problems             semble learning monte carlo algorithms let   concrete situation set example                     choose randomization procedure   space available priori training               choice weak learner deter•  set test set subsets                         mines distribution  words   learning algorithm generating hypotheses learn                major parameters ensemble algo•  hypotheses possibly consistent estimate              rithm fixed think theoretical   generalization error                                                 property examples                                                                           ensemble process starts compute esti•  consider problem monte carlo per•                    mations accuracy estimates increases   spective table  think pro•                number iterations performed   ceed follows beginning rows                 final rule classify correctly example   training test examples columns                 matter things value underlying exact   let consider example let     run replicates let compute                                                                                     datasets used experimental results                                                                               url         paper consider use crossvalidation                                                                                    httpwwwdi unitoitcspositomcandboost        leavconcout       learning                                                                                                                                
