                  learning identify unexpected instances test set                   xiaoli li                          bing liu                        seekiong ng             institute infocomm            department             institute infocomm                 research            science university illinois chicago   research            heng mui keng terrace            south morgan street             heng mui keng terrace              singapore                 chicago il                singapore              xlliirastaredusg                liubcsuicedu                 skngirastaredusg                        abstract                          formed later stage disease evolves                                                        mutations cancercausing agents phenomenon      traditional classification involves building classi                                                        uncommon seemingly simpler application      fier using labeled training examples set                                                        domains example document classification topics      predefined classes applying classifier                                                        heterogeneous new topics evolve time      classify test instances set classes                                                        document classifier built classifying say sci      practice paradigm problematic ence papers face similar problems cancer      test data contain instances                                                        classifier described science      long previously defined classes                                                        heterogeneous increasingly crossdisciplinary      tecting unexpected instances test set                                                        main rapidly evolving new topics      important issue practice problem                                                        created time      formulated learning positive unlabeled                                                          classifier created based notion fixed set      examples pu  learning current pu      predefined classes bound inadequate com      learning algorithms require large proportion                                                        plex dynamic realworld long run requiring      negative instances unlabeled set effec                                                        user manually through classification results      tive paper proposes novel technique solve                                                        unexpected instances practice competent clas      problem text classification domain                                                        sifier learn identify unexpected instances test      technique generates single artificial negative                                                        set automatically set unclassifiable instances      document anthesetsp used    apart applications important     build naïve bayesian classifier experiment                                                        example cancer example detection unex     results show method significantly better                                                        pected instances alert scientists new medi     existing techniques                                                        cal discovery new cancer subtype occurred                                                          recent years researchers studied problem    introduction                                       learning positive unlabeled examples pu  classification wellstudied problem machine learn learning given positive set unlabelled set ua  ing traditionally build classifier user collects pu learning algorithm learns classifier identify  set training examples labeled predefined hidden positive documents unlabeled set uour  known classes classification algorithm applied problem identifying unexpected instances test set  training data build classifier subsequently modeled pu learning problem treating  employed assign predefined classes instances training data positive set test set  test set evaluation future instances practice labeled set classifier learned using pu    paradigm problematic practice learning algorithms classify test set identify  test future instances belong unexpected negative instances applying tradi  predefined classes original training set test set tional classifier classify remaining instances  contain additional unknown subclasses new sub original predefined classes  classes arise underlying domain evolves time current pu techniques operate trying  example cancer classification training set consists identify adequate set reliable negative data  data currently known cancer subtypes unlabeled set learn require large propor  cancer complex heterogeneous disease tion unexpected instances unlabeled set  perplexing todate likely test data contain effective practice number unexpected instances  cancer subtypes medically classified test data small likely  covered training data training arising emerging class means  data contain current cancer subtypes new subtypes classifiers built existing pu learning techniques                                                      ijcai                                                    perform poorly small number unexpected  stances test set belong training  negative instances                            classes ci subsection section     paper propose novel technique called lgn baseline algorithm directly applies pu learning tech  pu learning generating negative examples niques identify unexpected instances section  study problem using text classification lgn uses en  present proposed lgn algorithm  tropybased method generate single artificial negative  document based information uinwhich  baseline algorithms pu learning  features’ frequency distributions correspond recapitulate problem identifying unexpected  grees “negativeness” terms respective entropy stances test set formulated pu learning  values accurate classifier use naïve bayesian problem follows training instances classes  method built identify unexpected instances combined form positive set test set  help artificial negative document experimental forms unlabeled set contains positive  results benchmark  newsgroup data showed stances belonging training classes ci nega  lgn outperforms existing methods dramatically        tiveunexpected instances belonging                                                        training class ci pu learning techniques     related work                                      employed build classifier classify unlabeled set  pu learning investigated researchers recent test set identify negative instances unexpected  years study pac learning positive unlabeled instances figure  gives detailed framework gener  examples statistical query model given ating baseline algorithms based pu learning techniques  denis  liu et al  reported sample complexity  results showed problem solved       ue      subsequently number practical algorithms liu et al   training examples classes treated positive   yu et al  li liu  proposed   test set ignore class labels present  conformed theoretical results liu et al   run existing pu learning algorithm build                                                            classifier  following twostep strategy  identifying set reliable          ∈  negative documents unlabeled set  building  each instance di  classifier using em svm iteratively specific differ  use classifier classify di                                                             di classified negative  ences steps follows sem proposed liu           ∪  et al  based naïve bayesian classification  ue  ue di  em  algorithm dempster  main idea  output ue  use spying technique identify reliable negative figure  directly applying existing pu learning techniques  documents unlabeled set run em  build final classifier pebl yu et al  uses dif inthebaselinealgorithmweuseasetue store  ferent method dnf identify reliable negative examples negative unexpected instances identified step  initializes  runs svm iteratively build classifier  ue set steps  initialize positive set    recently li liu  reported technique unlabeled set described step  run  called rocsvm technique reliable negative docu existing pu learning algorithm various pu learning tech  ments extracted using information retrieval tech niques applied build different classifiers  nique rocchio rocchio  svm used struct classifier employ classifier clas  second step fung et al  method called  sify test instances steps   instances  pnsvm   proposed deal situation classified negative class added ue  positive set small existing methods require unexpected instances iterated through  unlabeled set large number hidden negative test instances step  outputs unexpected set ue  instances paper deal opposite problem  proposed technique lgn  number hidden negative instances small    line related work learning positive traditional classification training test instances  data scholkopf  oneclass svm proposed drawn independently according fixed distribution  studied manevitz yousef    × ywherex denotes set possible documents  crammer  oneclass svm builds classifier text classification application  cnde  treating training data positive set instances notes known classes theoretically each class ci  test set classified negative classifier training test instances follow distribution  regarded unexpected instances experi classifier learned training instances used  ments show results poorer pu learning classify test instances known classes  indicates unlabeled data helps classification     problem training set tr instances                                                        classes  cn drawn distribution     proposed algorithm                            test set consists subsets tp called                                                        positive instances tandtn called unexpected  negative  given training set cii …n multiple classes  target automatically identify unexpected instances instances tp independently drawn                                                      ijcai                                                    instances tn drawn unknown laplacian smoothing                                                                                 different distribution du objective identify                        Ρ                                                                             ∑   nwt   rc                                                                 Ρrw           instances drawn unknown distribution orin                                                                                      ∑∑nw        Ρrc    words identify hidden instances tn                                                                                   nwtdi count number times    let  formally reformulate problem                                       ∈  twoclass classification problem labeled negative word wt occurs document di prcjdi  depend  training examples rename training set tr ing class label document  positive set changing class label ci ∈ “” finally assuming probabilities words  positive class rename test set independent given class obtain nb classifier                                                                                      unlabeled set comprises hidden positive                 Ρrc ∏  Ρrw                                                                  Ρ                   di                                                                rc  di   stances hidden unexpected instances unexpected                         di                                                                           ∑   Ρrc ∏   Ρrw          instances called negative instances                    di  class label “−” bear mind hidden naive bayesian classifier class highest  positive instances learning algorithm select prcjdi assigned class document  function class functions →  −tobe  used classifier identify unexpected nega generating negative data  tive instances problem subsection present algorithm generate  labeled negative examples learning negative data given naïve bayesian framework  problem learning positive unlabeled examples conditional probabilities prwt equation  computed  pu learning discussed previous section based accumulative frequencies documents  problem studied researchers recent years negative class single artificial negative instance  existing pu techniques performed poorly number work equally bayesian learning  negative unexpected instances small words need generate negative document                                                                             −  address propose technique generate artifi awaytoensureprw prw   positive feature                                                                      −  cial negative documents based given data      prw prw   negative feature wwe     let analyze problem probabilistic point use entropybased method estimate feature wi  view text classification problem documents com significantly different conditional probabilities                                                        prwi prwi entropy equation  monly represented frequencies words  wv  appear document collection called vo             −                                                                 entropywi  ∑prwi  logprwi                                                                               c∈−  cabularyletw represent positive word feature char  acterizes instances let represent negative entropy values show relative discriminatory  feature characterizes negative unexpected instances power word features bigger feature’s entropy  uifu contains large proportion positive instances likely similar distributions                                                        discriminatory means negative  feature similar distribution  negative feature  probability distribu feature entropy entropywissmallasprw  tions set different strategy mainly occurring significantly larger prw  exploit difference generate effective set artificial entropyw large prw prw simi  negative documents used lar entropy conditional probabilities  positive set classifier training identify negative indicate feature belongs positive  unexpected documents accurately               negative class generate features based     given use naïve bayesian framework entropy information weighted follows  work going introduce naïve baye                        entropyw                                                                     qw   −                          sian classifier text classification                                                                                                         max jv  entropyw     naÏve bayesian classification                           qwi  means wi uniformly occurs  naïve bayesian nb classification shown generate wi anifqwi  effective technique text classification lewis  certain wi negative feature  mccallum nigam  given set training docu generate based distribution uinthisway  ments each document considered ordered list features deemed discriminatory    words use wdik denote word position generated frequently features  document di each word vocabulary  qwi extremes frequencies   wv  vocabulary set words generated proportionally  consider classification set predefined generate artificial negative document fol  classes ccc order perform classifica lows given positive set unlabeled set uwe  tion need compute posterior probability prcjdi compute each word feature’s entropy value feature’s  cj class di document based baye frequency negative document randomly  sian probability multinomial model   generated following gaussian distribution according                           Ρ                                                            ∈                         ∑   rc  di               qwientropywimaxentropywj wj detailed                 Ρrc                                                   algorithm shown figure                                                       ijcai                                                                                                  summarizes unlabelled data set fea    training documents classes treated positive tures indicative positive class dramatically reduced    test set ignore class labels present                                                        building final nb classifier   each feature ∈                                                      finally build nb classifier      compute frequency each document freqw                                                   positive set generated single negative document        dk dk ∈                                                        identify unexpected document instances detailed                     ∑  freqwi dk       let mean     ∈            dw set                    dk dw                              μ                                    algorithm shown figure                  wi                         dw                                                        ue         documents containing wi                                                         build naïve bayesian classifier anusing      let variance                                          σ            ∑ freqw  − μ        equations                    wi       −            wi                       dw   ∈d                            wi                       each document di ∈                    ∈   each feature wi                                  using classify di using equation       compute prwi prwi  using equation  assuming  prdiprdi        documents negative             ue  ue di     let           −                                output ue           entropywi  ∑prwi  clogprwi                        c∈−                          figure  building final nb classifier   let maxentropywj   each feature wi ∈                             ue  stores set unexpected documents identified                 entropyw                             test set initialized set step  step      qw  −                                                            use equations   build nb classifier comput      dwiqwi                           ing prior probabilities pr pr conditional        generate frequency fnewwi using gaussian probabilities prwi prwi clearly prwi                              x−μ                              −   wi                     prw  computed based positive set           distribution       σ                                                        wi                    μ    π                             single negative document respectively regarded                      wi            fneww                      average document set virtual negative docu                                                ments problem compute prior   output                                                        probabilities pr pr turns  figure  generating negative document         major issue  simply assume generated     algorithm step  initializes negative docu negative document set number docu                                                        ments number documents positive set pwe  ment consists set featurefrequency pairs  set steps  step  initialize posi report experimental results support  tive set unlabeled set ufromsteptostepfor section building nb classifier use clas                                                        sify each test document steps  final output  each feature wi appeared compute fre  quency each document calculate frequency ue set stored identified unexpected documents                                                         mean variance documents dwi contain wi  information used generate later step   step  compute entropy wi using prwi  empirical evaluation  prwi computed using equation  section evaluate proposed technique lgn  assuming documents negative compare oneclass svm osvm use  obtaining maximal entropy value step  gener libsvm httpwwwcsientuedutwcjlinlibsvm  ate negative document steps   particular existing pu learning methods sem liu et al   step  computes qwi shows “negative”  pebl yu  et al  rocsvm li liu   feature wi terms different wi’s distributions sem rocsvm publicly availableweimple  bigger difference higher mented pebl available authors  frequency generate feature steps      inner loop dwiqwi decides number  datasets  times generate frequency word thusifqw                                                    evaluation used benchmark  newsgroup col  small means wi occurred  similar probabilities generate fewer lection consists  documents  differ                                                      ent usenet discussion groups  groups  wi quite likely negative feature generate  distribution similar uineachiteration categorized  main categories “computer” “recrea                                                        tion” “science” “talk” perform following  step  uses gaussian distribution corresponding      σ                                               sets experiments   wi generate frequency fnewwiforwistep  places pair wi fnewwi negative document classes set experiments simulates case  finally step  outputs generated negative set note training data classes positive set  frequency each feature wi contains classes classes data chosen  integer value generated gaussian distribution                                                             essentially randomly generated aggregated document httpwwwcsuiceduliublpulpudownloadhtml                                                      ijcai                                                    main categories “computer” “science” table  experimental results α   “computer” group subgroups  “science” group subgroups subgroup data set         osvm   sem rocsvm  pebl lgn  sists  documents                              graphiccrypt                     each data set training testing constructed graphicelectro           follows positive documents training graphicmed                   testing consist documents subgroup class graphicspace             “computer” subgroup class “science” oscrypt                     gives  data sets each class subgroup parti oselectronics          tioned documents standard subsets  osmed                         training  testing each positive set osspace                training contains  documents classes each machardwarecrypt           test set contains  positive documents machardwareelectro        classes add negative unexpected documents machardwaremed            randomly selected remaining  groups machardwarespace             order create different experimental settings vary ibmhardwarecrypt       number unexpected documents controlled ibmhardwareelectro           parameter α percentage number ibmhardwaremed            unexpected documents added α ×            ibmhardwarespace              classes set experiments simulates case windowscrypt                 training data three different classes windowselectro          positive set contains three classes data used windowsmed                  data sets formed added class windowsspace                each added class randomly average                      selected remaining  groups each data set figure  shows macroaverage results α values  unexpected documents randomly se                                                          techniques classes  lected remaining  newsgroups settings                                                        experiments method lgn outperformed sig  classes case                                                        nificantly α ≤  α increased     experimental results                              rocsvm achieved slightly better results lgn                                                        observe osvm sem rocsvm  outper  classes performed experiments using possible formed pebl able extract reliable  combinations  data sets each technique negatives dnf method used pebl pebl  namelyosvmsemrocsvmpeblandlgnwe                needed higher α  achieve similar good results  performed  random runs obtain average results                                                             each run training test document sets  unexpected document instances    classes selected randomly varied α    table  shows classification results various                                                                                                    lgn                                                 α                                                 sem  techniques terms fscore negative class                                        rocsvm                                                                                               pebl   column table  lists  different com fscore                              osvm  binations columns   show results   techniques osvm sem rocsvm   pebl  spectively column  gives corresponding results   technique lgn                                                                     observe table  lgn produces best                 unexpected documents  sults consistently data sets achieving fscore figure  comparison results different percentages   average        unexpected documents classes experiments   higher fscores existing techniques  osvm sem rocsvm pebl respectively abso  classes figure  shows classes results lgn  lute terms lgn highly consistent performed better methods pro  different data sets fact checked step portion unexpected documents small α ≤   three existing pu learning techniques comparably sem rocsvm proportion  extracted negative documents wrong larger osvm’s results worse sem  result respective second steps svm em rocsvm lgn α larger showing pu  unable build accurate classifiers noisy nega learning better oneclass svm problem  tive data sem algorithm parameter pebl required larger proportion unex  tried different values results similar pected documents produce comparable results                                                      ijcai                                                    
