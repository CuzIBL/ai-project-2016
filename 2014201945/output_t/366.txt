           probabilistic model redundancy information extraction                            doug downey oren etzioni stephen soderland                             department science engineering                                         university washington                                          seattle wa                               ddowneyetzionisoderlancswashingtonedu                        abstract                          obtained documents “support”                                                        extraction large independently authored      unsupervised information extraction uie                                                        conﬁdence extraction increases dramatically      task extracting knowledge text                                                        number supporting documents      ing handtagged training examples fundamen                                                        precisely quantify conﬁdence extraction given      tal problem uie supervised                                                        available textual evidence      sessing probability extracted information                                                          paper introduces combinatorial model enables      correct massive corpora web                                                        determine probability observed extraction      extraction repeatedly differ                                                        correct validate performance model empiri      ent documents does redundancy impact                                                        cally task extracting information web using      probability correctness                                                        knowitall      paper introduces combinatorial “ballsand   contributions follows      urns” model computes impact sample      size redundancy corroboration multi      formal model unlike previous work explicitly      ple distinct extraction rules probability models impact sample size redundancy dif      extraction correct methods         ferent extraction rules probability extrac      estimating model’s parameters practice     tion correct analyze conditions      demonstrate experimentally uie       model applicable provide intuitions      model’s log likelihoods  times better av   behavior practice      erage obtained pointwise mutual   methods estimating model’s parameters      formation pmi noisyor model used        uie supervised tasks      previous work supervised model’s      formance comparable support vector     experiments demonstrate model’s improved      machines logistic regression                    formance techniques used assess extraction                                                            probability previous work uie model                                                            factor  closer correct log likelihood    introduction                                           noisyor model used previous work model   information extraction task automatically ex times closer knowitall’s pointwise mutual  tracting knowledge text unsupervised uie formation pmi method etzioni et al   absence handtagged training data uie sys based turney’s pmiir algorithm turney   tems require human intervention recursively supervised model achieves  improve  discover new relations attributes instances rapid ment average log likelihood noisyor model  scalable manner knowitall etzioni et al     marginally better svms logistic                                                      gression    fundamental problem supervised uie  remainder paper organized follows section  assessing probability extracted information cor  introduces abstract probabilistic model section   rect explained section  previous work used describes implementation practice section  reports  variety techniques address problem experimental results domains section  contrasts  provide adequate formal model impact model previous work paper concludes  redundancy—repeatedly obtaining extraction discussion future work  different documents—on probability correctness  massive corpora web redundancy  main sources conﬁdence extractions                urns model    extraction obtained multiple distinct doc probabilistic model takes form classic “balls  uments likely bona ﬁde extraction andurns” model combinatorics ﬁrst consider thesingle urn case simplicity generalize note expressions include prior information  multiple urns model used experiments refer label – example ∈ prior probability  model simply urns                                 string target label numx  rx ∈    think abstractly generative process maps represents probability target label repeated  text extractions extractions repeat distinct docu balls urn general integrating prior information  ments yield extraction example web valuable systems analysis  page containing “scenic towns yakima” experiments follow make simplifying assumption  web page containing “washington towns yakima” uniform priors yielding following simpliﬁed form  lead believe yakima correct extraction proposition   relation cityx    each extraction modeled labeled ball urn ∈ cx appears times draws   label represents instance target relation                                                                                           n−k  error information extraction process modeled                    r∈numc   −   repeated draws urn replacement                               n−k                                                                              r∈numc∪e    −    example balls drawn urn each                                      label “yakima” labels instances relation  uniform special case  cityx each label appear different number illustration consider simple case labels  balls urn finally balls urn repeated number balls  error labels “california” representing cases numc   ∈ assume  process generated extraction member                                                                       numei    ei ∈ assumptions  target relation                               unrealistic fact use zipf distribution numb    formally parameters characterize urn experiments reasonable approximation    • – set unique target labels number majority labels lie ﬂat tail zipf      unique target labels urn                  curve    • – set unique error labels number deﬁne precision extraction process      unique error labels urn                   probability given draw comes target                                                        relation uniform case    • numb – function giving number balls la                                                                                cr      beled ∈ ∪ numb multiset                              giving number balls each label ∈                      ere   crc    course systems access param probability particular element appears  eters directly goal discern given draw   similarly  −p   labels extracts fact elements based                            repeated draws urn central question using poisson model approximate binomial  investigating given particular label ex proposition   tracted times set draws urn ∈ cx appears times draws ≈  probability ∈                                                                                               deriving probability formally assume                                                                                                                                access multisets numc nume giv                               npc −pe                                                                                        ing number times labels appear balls                            urn experiments provide methods es practice extraction process noisy informative  timate multisets unsupervised supervised pc  pe notice true equation   settings express probability element ex shows odds ∈ increase exponentially  tracted times target relation follows number times extracted decrease                                    exponentially sample size                                                          numerical examples illustrate behavior    appears times drawsx ∈             equation examples assume precision                                                                                  ×       n  r k    n−k                          let                  means        e—                     −       numx   rx ∈     target balls times common urn error balls                                                                               ∈   total number balls urn sum  small number repetitions  taken possible repetition rates               yield high conﬁdence extraction    express desired quantity using bayes sample size increases    pa  rule                                                 rameters unchanged ∈ drops                                                         hand balls repeat frequently    ∈ cx appears times draws             balls say rc   × set                                                         remains unchanged ∈ rises      appears times drawsx ∈ cp ∈                                                       examples enable illustrate advantages             appears times draws           urns noisyor model used previous work linet al  agichtein gravano  noisyor  model assumes each extraction independent asser  tion correct fraction time extracted label  “true” noisyor model assigns following probability  extractions                                                    pnoisy−orx ∈ cx appears times   −  −  noisyor model assign  probability— —in three examples  explained  correct case     rc   × ex  amples show different sample sizes repetition rates  noisyor model highly inaccurate surpris  ing given noisyor model ignores sample size  repetition rates section  quantiﬁes improvements figure  schematic illustration number distinct  obtained urns practice                         labels sets repetition rate “con                                                        fusion region” shaded    applicability urns model  conditions does redundancy model provide example consider  particular numc   accurate probability estimates labels target nume learned unsupervised setting film  set repeated balls urn labels relation experiments sample size     set figure  shaded region figure  used experiments expected number true positives  represents “confusion region” – labels  expected precision  close  region classiﬁed incorrectly ideal classi actual observed true positives  precision  ﬁer inﬁnite data labels simply  increase sample size   isn’t information decide belong expect true positives increase   model effective confusion precision  urns equations  gion relatively small secondly small confusion enable intelligently choose sample size  regions sample size large approxi depending precision recall requirements resource  mate distributions shown figure  constraints absence tagged training data  probabilities output model inaccurate    attractive feature urns enables esti  multiple urns  mate expected recall precision function sample generalize model encompass multiple urns  size distributions figure  cross dotted line information extracted using multiple distinct mech  shown given sufﬁciently large sample size expected anisms – example employ  recall fraction area curve lying patterns extracting city names “cities including x”  right dotted line                      “x towns” case different pat    given sample size deﬁne τn number terns different modes failure extractions appearing  appearances extraction likely multiple patterns generally likely true  set set given distributions figure appearing single pattern model   τn computed using proposition  situation introducing multiple urns each urn repre                                                                                             et ruep ositives                                 sents different extraction mechanism                                                          instead total extractions sample size                         τn−                              r k    n−k      nm  each urn ∈ extraction appearing           −                          −                                                  km times let ax     km     nm denote                                          r∈numc                               event let amx event label ap  deﬁne “true positives” number ex pears times draws urn assuming  tracted labels ci ∈ model assigns probability draws each urn independent  ci ∈                                       proposition     expected number false positives similarly                                                          ∈ cax     km     nm     ef alsep ositives                                                                                                                                            amci km nm                         τ −                                               ci∈c  m∈m                        xn n  r k    n−k                                       −                          −                               x∈c∪e   m∈m  amx km nm                                                        r∈nume                                                          multiple urns distributions labels balls    expected precision approxi                                                        urns represented multisets num  mated                                                                                                                et ruep ositives               lump mechanisms single urn tend  ep recision ≈                 ef alsep ositives  et ruep ositives behave similarlynumme expressing correlation nummx   expressions frequency rank extraction  numm important modeling decision multiple assumed urns qc qe  urns especially beneﬁcial repetition rates ele normalizing constants  ments strongly correlated different urns                                                                         −zc         −ze  elements e—that nummx            qc         qei        numm tend closer each ∈     ci∈c          ei∈e  ∈ fortunately turns case prac  tice section  describes method modeling multiurn nonsystematic error present urn  correlation                                          amx km nm  km    substi                                                        tuting expressions amx km nm propo    implementation urns model                   sition  gives ﬁnal form urns model  section describes implement urns uie  efﬁcient computation  supervised identiﬁes assumptions  each case                                            feature implementation allows efﬁ    order compute probabilities extractions need cient computation probabilities general computing                                                                                                        method estimating numc nume pur sum proposition  potentially large  pose estimating sets tagged untagged data sets require signiﬁcant computation each extrac                                                                                                   numc  assume numc nume  zipf distributed tion given ﬁxed number urns                                                        nume  zipf distributed integral approximation  meaning ci ith frequently repeated label                                −zc                     sum proposition  using poisson place bi  numci proportional  char  acterize numc nume sets ﬁve parameters nomial equation  solved closed form terms                                                        incomplete gamma functions closed form expres  set sizes shape parameters zc ze  extraction precision                       sion evaluated quickly probabilities ex    model multiple urns consider different extraction tractions obtained efﬁciently solution leverages                                                        assumptions size shape parameters identical  precisions pm each urn make simplifying assump  tion size shape parameters urns relative frequencies perfectly cor  urns mentioned section  expect repetition rate related finding efﬁcient techniques computing proba  correlation urns higher elements set bilities stringent assumptions item future  set model correlation follows ﬁrst work  elements set assumed come  parameter estimation  location zipf curve urns relative  frequencies perfectly correlated elements event large sample handtagged training ex  set similar relative frequency amples available each target relation  urns – systematic errors rest directly estimate each parameters urns  set nonsystematic errors meaning use populationbased stochastic optimization technique  appear kind extraction mechanism exam identify parameter settings maximize conditional log                                                                                   ple “eastman kodak” extracted instance film likelihood training data parameters set  phrases involving word “ﬁlm” model yields probability each extraction given  involving word “movie” formally nonsystematic er number times km appears each urn number  rors labels present urns draws nm each urn  each type nonsystematic error makes fraction argued etzioni et al  systems rely  set fractions parameters cor handtagged training examples scale ex  relation model assuming simple correlation model tracting information arbitrary relations speci  identical size shape parameters urns restric ﬁed advance implementing urns uie requires  tive general— differences extraction mechanisms lution challenging problem estimating numc  complex assumptions allow nume using untagged data let multiset consist  compute probabilities efﬁciently described ing number times each unique label extracted  appear hurt performance signiﬁcantly practice number unique labels encountered sam                                                                      correlation model label element ple size  u∈u  systematic error present urns terms order learn numc nume untagged  proposition  probability label appears km times data make following assumptions  nm draws                                  • number different possible errors nearly                     n                                                                                                                 km          nm−km         unbounded assume error set large  amx km nm       fmx  −fmx                       km                                                        speciﬁcally use differential evolution routine built  frequency extraction mathematica                                                                                                                 −zc                             experiments set    sensitivity analysis         fmci     pmqc    ci ∈               showed changing order magnitude direc                                −ze         fmei      − pmqei    ei ∈          tion resulted small changes results  • assume numc nume zipf dis methods experimentally lastly compare urns sev      tributed ze parameter set       eral baseline methods supervised setting    • experience knowitall     evaluated  algorithms extraction sets      different extraction rules differing precision relations cityx filmx countryx      each rule’s precision stable different relations mayorofxy taken experiments performed et      etzioni et al  urns takes precision zioni et al  sample size  city      input demonstrate urns overly sensitive  film  country       parameter chose ﬁxed value  used mayorof extraction patterns partitioned                                                       urns based employed target      precision pm urns experiments                                                        lation “country” “nation”    use expectation maximization em lefthanded “countries including x” righthanded  der arrive appropriate values zc “x countries” each combination rela  quantities uniquely determine numc given assump tion handedness treated separate urn  tions em algorithm proceeds follows         sulting urns each cityx filmx                                                                                                      initialize zc starting values         countryx urns mayorofx   each    repeat convergence                         relation tagged sample  extracted labels using                                                        external knowledge bases tipster gazetteer cities       estep assign probabilities each element internet movie database ﬁlms manually tagging          using proposition                         instances knowledge base uie       mstep set zc using probabil experiments evaluate algorithms  exam          ities assigned estep details ples supervised experiments perform   obtain zc mstep ﬁrst estimating fold cross validation  rankfrequency distribution labels untagged  data untagged data probabilities  uie experiments  estep obtain expected number la                                                      compare urns  methods unsuper                           bels extracted times round vised information extraction noisyor model  fractional expected counts discrete rankfrequency dis used previous work extraction appearing times  tribution number elements equal expected signed probability  −  − ex                                                                   m∈m                tal number labels untagged data  traction precision urn second method            obtain ﬁtting zipf curve rankfrequency  distribution linear regression loglog scale lastly               set   ec  unseen estimate pointwise mutual information  number unseen labels set using goodturing esti                                                        previous work knowitall used pointwise mutual  mation gale sampson  speciﬁcally choose                                                        information pmi obtain probability estimates extrac  unseen probability mass unseen labels                                                        tions etzioni et al  speciﬁcally pmi  equal expected fraction draws ex                                                        extraction set automatically generated discrimi  tracted labels seen                                                        nator phrases “movies x” computed    unsupervised learning strategy proved effective                                                        web search engine hit counts pmi scores used  target relations different sizes example number                                                        features naive bayes classiﬁer nbc produce  elements country relation nonnegligible                                                        probability estimate extraction nbc trained  extraction probability orders magnitude                                                        using set automatically bootstrapped seed instances  smaller film city relations                                                        positive seed instances taken having high    clearly unsupervised learning relies strong                                                        est pmi discriminator phrases bootstrapping  sumptions sensitivity analysis shown                                                        process negative seeds taken positive seeds  model’s performance robust future                                                        relations work lin et al   work plan perform comprehensive sensitivity                                                                                                    analysis model investigate performance pmi shown etzioni et al  order  semisupervised setting                            extractions fairly signiﬁcant shortcomings                                                        obtaining hit counts needed compute pmi    experimental results                               scores expensive requires large number queries    section describes experimental results set draws urns intended represent independent ex  tings unsupervised supervised begin describ tractions sentence duplicated multi  ing unsupervised methods used previous work ple different web documents experiments consider  noisyor model pmi compare urns each unique sentence containing extraction draw                                                        urns experiments possibilities including counting    sensitivity analysis showed choosing substantially number unique documents producing each extraction sim  higher  lower  value pm resulted urns ply counting occurrence each extraction  outperforming noisyor model factor  pmi formance differences various approaches negligi  factor  experiments described section  ble task
